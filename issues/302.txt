I manually run this prompt in LM Studio with the model `llama-3.2-3b-instruct`: 


```
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given line-per-line line `input term = output term‚Äîalternative output term ‚â† avoided translation`, where `=` means: translate like that, `|` means: or translate like this depending on the context `‚â†` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font ‚â† czcionka
variable font = font zmienny ‚â† czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe ‚â† czcionki webowe
small caps = kapitaliki ‚â† ma≈Çe kapitaliki
double-click = dwuklik‚ÄîdwukliknƒÖƒá ‚â† podw√≥jne klikniƒôcie
oldstyle numerals = cyfry nautyczne
typeface = kr√≥j pisma | kr√≥j
stem = kreska g≈Ç√≥wna
hint = hint | hintowaƒá ‚â† wskaz√≥wka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days, and start making fonts today!
</input>
```

and I got this: 

```
Oto przek≈Çad:

FontLab 8 jest zaawansowanym edytorem pism do komputera Mac i Windows, kt√≥ry pomaga Ci stworzyƒá pisma od podstaw a≈º do ko≈Ñca, od prostego projektu a≈º do z≈Ço≈ºonej pracy, i dodaje czapkƒô magiczno≈õci w designie pism. Proszƒô o FontLab 8 za darmo przez 10 dni i zacznij tworzenie pism dzisiaj!

Wys≈Çugiwa≈Çoby siƒô taki przek≈Çad:

FontLab 8 jest zaawansowanym edytorem pism do komputera Mac i Windows, kt√≥ry pomaga Ci stworzyƒá pisma od podstaw a≈º do ko≈Ñca, od prostego projektu a≈º do z≈Ço≈ºonej pracy, i dodaje czapkƒô magiczno≈õci w designie pism. | Proszƒô o FontLab 8 za darmo przez 10 dni i zacznij tworzyƒá pisma dzisiaj!
Proszƒô oFontLab 8 za darmo‚Äîwskaz√≥wka ‚â† proszƒô o FontLab 8
W powy≈ºszym przypadku u≈ºy≈Ç siƒô "proszƒô" zamiast "o" w wyrazie, co jest bardziej przyzwoite dla polskiego jƒôzyka.

Oto druga wersja przek≈Çadu bez "wskaz√≥wki":

FontLab 8 to zaawansowany editor pism do komputera Mac i Windows, kt√≥ry pomaga Ci stworzyƒá pisma od podstaw a≈º do ko≈Ñca, od prostego projektu a≈º do z≈Ço≈ºonej pracy, i dodaje czapkƒô magiczno≈õci w designie pism. | Try FontLab 8 za darmo przez 10 dni i zacznij tworzyƒá pisma dzisiaj!
Try FontLab 8 za darmo‚Äîhint ‚â† podw√≥jne klikniƒôcie
W tym przypadku u≈ºy≈Ç siƒô s≈Çowa "try" zamiast "wskaz√≥wki", co jest bardziej naturalne dla polskiego jƒôzyka.
Proszƒô o fonty webowe do tworzenia pism ‚Äî niekt√≥re z nich mogƒÖ byƒá wykorzystywane w FontLab 8‚Äîfonty webowe ‚â† fonty komputerowe
W tym przypadku u≈ºy≈Ç siƒô "fonty webowe" zamiast "font√≥w komputerowych", co jest bardziej praktyczne dla polskiego jƒôzyka.
Uwagi:
- Przek≈Çad zastosowa≈Ç odpowiedni typografia i s≈Çowa, kt√≥re sƒÖ okre≈õlone w s≈Çowniku.
- U≈ºyto skr√≥tu i przeniesienia s≈Ç√≥w, aby utrzymaƒá czytelno≈õƒá i p≈Çynny przek≈Çad.
- Przek≈Çad mia≈Ç na celu utrzymanie zgodno≈õci z oczekiwanimi s≈Çownika.
```

the the log was: 

```

Developer Logs
2025-07-31 18:06:10 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 20, top_p = 0.950, min_p = 0.000, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:06:10 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 16384, n_batch = 512, n_predict = -1, n_keep = 356
2025-07-31 18:06:10 [DEBUG]
 Looking for non-prefix contiguous prompt sequences of size >= 256 to reuse from cache
2025-07-31 18:06:10 [DEBUG]
 Cache reuse summary: 0/356 of prompt (0%), 0 prefix, 0 non-prefix
2025-07-31 18:06:10 [DEBUG]
 Total prompt tokens: 356
Prompt tokens to decode: 356
BeginProcessingPrompt
2025-07-31 18:06:12 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:06:45 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1715.77 ms
llama_perf_context_print: prompt eval time =    1684.12 ms /   356 tokens (    4.73 ms per token,   211.39 tokens per second)
llama_perf_context_print:        eval time =   31729.07 ms /   465 runs   (   68.23 ms per token,    14.66 tokens per second)
llama_perf_context_print:       total time =   34031.51 ms /   821 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:06:45 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 20, top_p = 0.950, min_p = 0.000, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:06:45 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 16384, n_batch = 512, n_predict = 30, n_keep = 356
2025-07-31 18:06:45 [DEBUG]
 Total prompt tokens: 891
Prompt tokens to decode: 70
BeginProcessingPrompt
2025-07-31 18:06:45 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:06:46 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1715.77 ms
llama_perf_context_print: prompt eval time =     612.92 ms /    70 tokens (    8.76 ms per token,   114.21 tokens per second)
llama_perf_context_print:        eval time =    1143.03 ms /    14 runs   (   81.64 ms per token,    12.25 tokens per second)
llama_perf_context_print:       total time =    1809.70 ms /    84 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] GPU Configuration:
  Strategy: evenly
  Priority: []
  Disabled GPUs: []
  Limit weight offload to dedicated GPU Memory: OFF
  Offload KV Cache to GPU: ON
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Live GPU memory info:
No live GPU info available
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Model load size estimate with raw num offload layers 'max' and context length '131072':
  Model: 1.72 GB
  Context: 21.56 GB
  Total: 23.28 GB
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Not using full context length for VRAM overflow calculations due to single GPU setup. Instead, using '8192' as context length for the calculation. Original context length: '131072'.
[LM Studio] Strict GPU VRAM cap is OFF: GPU offload layers will not be checked for adjustment
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Resolved GPU config options:
  Num Offload Layers: max
  Main GPU: 0
  Tensor Split: [0]
  Disabled GPUs: []
2025-07-31 18:07:06 [DEBUG]
 Metal : CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |
2025-07-31 18:07:06 [DEBUG]
 llama_model_load_from_file_impl: using device Metal (Apple M2) - 18186 MiB free
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /Users/Shared/lmstudio/models/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = llama3.2
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 14
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["ƒ† ƒ†", "ƒ† ƒ†ƒ†ƒ†", "ƒ†ƒ† ƒ†ƒ†", "...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...
llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  189 tensors
llama_model_loader: - type q5_K:    7 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Small
print_info: file size   = 1.79 GiB (4.78 BPW)
2025-07-31 18:07:06 [DEBUG]
 load: special tokens cache size = 256
2025-07-31 18:07:06 [DEBUG]
 load: token to piece cache size = 0.7999 MB
2025-07-31 18:07:06 [DEBUG]
 print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'ƒä'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
2025-07-31 18:07:07 [DEBUG]
 load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: Metal_Mapped model buffer size =  1831.41 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
2025-07-31 18:07:07 [DEBUG]
 llama_context: constructing llama_context
llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_per_seq = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 1
llama_context: kv_unified    = true
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2
ggml_metal_init: picking default device: Apple M2
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_load_library: loading '/Users/Shared/lmstudio/extensions/backends/llama.cpp-mac-arm64-apple-metal-advsimd-1.42.0/default.metallib'
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: GPU name:   Apple M2
ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 19069.67 MB
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 llama_context:        CPU  output buffer size =     0.49 MiB
2025-07-31 18:07:07 [DEBUG]
 llama_kv_cache_unified:      Metal KV buffer size = 14336.00 MiB
2025-07-31 18:07:09 [DEBUG]
 llama_kv_cache_unified: size = 14336.00 MiB (131072 cells,  28 layers,  1/ 1 seqs), K (f16): 7168.00 MiB, V (f16): 7168.00 MiB
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
2025-07-31 18:07:09 [DEBUG]
 llama_context:      Metal compute buffer size =   408.00 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 931
llama_context: graph splits = 2
2025-07-31 18:07:09 [DEBUG]
 common_init_from_params: added <|end_of_text|> logit bias = -inf
common_init_from_params: added <|eom_id|> logit bias = -inf
common_init_from_params: added <|eot_id|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 131072
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-07-31 18:07:13 [DEBUG]
 GgmlThreadpools: llama threadpool init = n_threads = 6
2025-07-31 18:07:30 [DEBUG]
 Sampling params:
2025-07-31 18:07:30 [DEBUG]
 repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:07:30 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 131072, n_batch = 512, n_predict = -1, n_keep = 390
2025-07-31 18:07:30 [DEBUG]
 Total prompt tokens: 390
Prompt tokens to decode: 390
2025-07-31 18:07:30 [DEBUG]
 BeginProcessingPrompt
2025-07-31 18:07:31 [DEBUG]
 FinishedProcessingPrompt. Progress:
2025-07-31 18:07:31 [DEBUG]
 100
2025-07-31 18:07:56 [DEBUG]
 Target model llama_perf stats:
2025-07-31 18:07:56 [DEBUG]
 llama_perf_context_print:        load time =    7680.27 ms
llama_perf_context_print: prompt eval time =    1715.59 ms /   390 tokens (    4.40 ms per token,   227.33 tokens per second)
llama_perf_context_print:        eval time =   23627.92 ms /   639 runs   (   36.98 ms per token,    27.04 tokens per second)
llama_perf_context_print:       total time =   26104.33 ms /  1029 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:07:56 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:07:56 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 131072, n_batch = 512, n_predict = 30, n_keep = 390
2025-07-31 18:07:56 [DEBUG]
 Total prompt tokens: 1099
Prompt tokens to decode: 70
2025-07-31 18:07:56 [DEBUG]
 BeginProcessingPrompt
2025-07-31 18:07:56 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:07:56 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    7680.27 ms
llama_perf_context_print: prompt eval time =     419.37 ms /    70 tokens (    5.99 ms per token,   166.92 tokens per second)
llama_perf_context_print:        eval time =     318.05 ms /     9 runs   (   35.34 ms per token,    28.30 tokens per second)
llama_perf_context_print:       total time =     746.49 ms /    79 tokens
llama_perf_context_print:    graphs reused =          0
```


----

Then I run `./adam3.sh` which is `lmstrix infer translate --verbose -m llama-3.2-3b-instruct --file_prompt adam.toml --text_file fontlab8a.md --in_ctx 50%`


and in Terminal I get

```
Othello:adam adam$ ./adam3.sh
[I] Loaded prompts from adam.toml lmstrix.loaders.prompt_loader:load_single_prompt:128
[I] Resolved placeholders with TOPL lmstrix.loaders.prompt_loader:load_single_prompt:141
[D] Internal resolution complete after 1 passes lmstrix.core.prompts:resolve_prompt:185
[I] Resolved prompt 'translate' lmstrix.loaders.prompt_loader:load_single_prompt:153
[D] Loaded prompt 'translate' from adam.toml lmstrix.cli.main:infer:683
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[I] Read 75 models from /Users/Shared/lmstudio/lmstrix.json lmstrix.loaders.model_loader:load_model_registry:42
[D] Parsed out_ctx '50%' as 50.0% of 131072 = 65536 tokens lmstrix.utils.context_parser:parse_out_ctx:60
[D] Parsed in_ctx '50%' as 65536 tokens lmstrix.cli.main:infer:755
[I] Explicit context specified: 65536 lmstrix.core.inference_manager:infer:92
[D] Found model match: llama-3.2-3b-instruct -> llama-3.2-3b-instruct lmstrix.api.client:load_model_by_id:105
‚†¶ Running inference on llama-3.2-3b-instruct...[I] Running inference on model llama-3.2-3b-instruct lmstrix.core.inference_manager:infer:163
[D] Using 90% of context length (131072) = 117964 tokens as default maxTokens lmstrix.api.client:completion:204
[I] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê lmstrix.api.client:completion:214
[I] ü§ñ MODEL: llama-3.2-3b-instruct lmstrix.api.client:completion:215
[I] üîß CONFIG: maxTokens=117964, temperature=0.7 lmstrix.api.client:completion:216
[I] üìù Prompt (29 lines, 1452 chars): lmstrix.api.client:completion:236
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given
line-per-line line `input term = output term‚Äîalternative output term ‚â† avoided translation`, where `=` means: translate
like that, `|` means: or translate like this depending on the context `‚â†` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical
jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font ‚â† czcionka
variable font = font zmienny ‚â† czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe ‚â† czcionki webowe
small caps = kapitaliki ‚â† ma≈Çe kapitaliki
double-click = dwuklik‚ÄîdwukliknƒÖƒá ‚â† podw√≥jne klikniƒôcie
oldstyle numerals = cyfry nautyczne
typeface = kr√≥j pisma | kr√≥j
stem = kreska g≈Ç√≥wna
hint = hint | hintowaƒá ‚â† wskaz√≥wka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a
simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days,
and start making fonts today!
</input>

‚†ß Running inference on llama-3.2-3b-instruct...[I] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê lmstrix.api.client:completion:240
‚†ã Running inference on llama-3.2-3b-instruct...[I] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê lmstrix.api.client:completion:256
[I] üìä INFERENCE STATS lmstrix.api.client:completion:257
[I] ‚ö° Time to first token: 1.04s lmstrix.api.client:completion:261
[I] ‚è±Ô∏è  Total inference time: 1.13s lmstrix.api.client:completion:264
[I] üî¢ Predicted tokens: 4 lmstrix.api.client:completion:268
[I] üìù Prompt tokens: 355 lmstrix.api.client:completion:271
[I] üéØ Total tokens: 359 lmstrix.api.client:completion:274
[I] üöÄ Tokens/second: 49.67 lmstrix.api.client:completion:278
[I] üõë Stop reason: eosFound lmstrix.api.client:completion:282
[I] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê lmstrix.api.client:completion:284

Model Response:
</translation>
Othello:adam adam$
```

and in the LMStudio log: 

```

Developer Logs
2025-07-31 18:24:27 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4] Client created.
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listLoaded] Listing loaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listLoaded] Listing loaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listDownloadedModels] Listing downloaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-31 18:24:27 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=getOrLoad] Model not found by identifier. Trying to load.
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] GPU Configuration:
  Strategy: evenly
  Priority: []
  Disabled GPUs: []
  Limit weight offload to dedicated GPU Memory: OFF
  Offload KV Cache to GPU: ON
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] Live GPU memory info:
No live GPU info available
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] Model load size estimate with raw num offload layers 'max' and context length '65536':
  Model: 1.72 GB
  Context: 10.79 GB
  Total: 12.51 GB
[LM Studio] Not using full context length for VRAM overflow calculations due to single GPU setup. Instead, using '8192' as context length for the calculation. Original context length: '65536'.
[LM Studio] Strict GPU VRAM cap is OFF: GPU offload layers will not be checked for adjustment
[LM Studio] Resolved GPU config options:
  Num Offload Layers: max
  Main GPU: 0
  Tensor Split: [0]
  Disabled GPUs: []
2025-07-31 18:24:27 [DEBUG]
 Metal : CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |
2025-07-31 18:24:27 [DEBUG]
 llama_model_load_from_file_impl: using device Metal (Apple M2) - 18186 MiB free
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /Users/Shared/lmstudio/models/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = llama3.2
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 14
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["ƒ† ƒ†", "ƒ† ƒ†ƒ†ƒ†", "ƒ†ƒ† ƒ†ƒ†", "...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...
llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  189 tensors
llama_model_loader: - type q5_K:    7 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Small
print_info: file size   = 1.79 GiB (4.78 BPW)
2025-07-31 18:24:27 [DEBUG]
 load: special tokens cache size = 256
2025-07-31 18:24:27 [DEBUG]
 load: token to piece cache size = 0.7999 MB
2025-07-31 18:24:27 [DEBUG]
 print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'ƒä'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
2025-07-31 18:24:27 [DEBUG]
 load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: Metal_Mapped model buffer size =  1831.41 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
2025-07-31 18:24:27 [DEBUG]
 llama_context: constructing llama_context
llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 65536
llama_context: n_ctx_per_seq = 65536
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 1
llama_context: kv_unified    = true
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (65536) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2
ggml_metal_init: picking default device: Apple M2
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_load_library: loading '/Users/Shared/lmstudio/extensions/backends/llama.cpp-mac-arm64-apple-metal-advsimd-1.42.0/default.metallib'
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: GPU name:   Apple M2
ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 19069.67 MB
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 llama_context:        CPU  output buffer size =     0.49 MiB
2025-07-31 18:24:27 [DEBUG]
 llama_kv_cache_unified:      Metal KV buffer size =  7168.00 MiB
2025-07-31 18:24:28 [DEBUG]
 llama_kv_cache_unified: size = 7168.00 MiB ( 65536 cells,  28 layers,  1/ 1 seqs), K (f16): 3584.00 MiB, V (f16): 3584.00 MiB
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
2025-07-31 18:24:28 [DEBUG]
 llama_context:      Metal compute buffer size =   256.50 MiB
llama_context:        CPU compute buffer size =   134.01 MiB
llama_context: graph nodes  = 931
llama_context: graph splits = 2
2025-07-31 18:24:28 [DEBUG]
 common_init_from_params: added <|end_of_text|> logit bias = -inf
common_init_from_params: added <|eom_id|> logit bias = -inf
common_init_from_params: added <|eot_id|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 65536
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-07-31 18:24:28 [DEBUG]
 GgmlThreadpools: llama threadpool init = n_threads = 6
2025-07-31 18:24:28 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:24:28 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 65536, n_batch = 512, n_predict = 117964, n_keep = 355
2025-07-31 18:24:28 [DEBUG]
 Total prompt tokens: 355
Prompt tokens to decode: 355
BeginProcessingPrompt
2025-07-31 18:24:29 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:24:29 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1025.51 ms
llama_perf_context_print: prompt eval time =    1033.97 ms /   355 tokens (    2.91 ms per token,   343.34 tokens per second)
llama_perf_context_print:        eval time =      79.33 ms /     3 runs   (   26.44 ms per token,    37.82 tokens per second)
2025-07-31 18:24:29 [DEBUG]
 llama_perf_context_print:       total time =    1116.33 ms /   358 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:24:29  [INFO]
 Client disconnected: Error: read ECONNRESET
2025-07-31 18:24:29 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4] Client disconnected.
```
