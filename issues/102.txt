Note: `@` is used as an indicator that what follows is a path. `@` is never part of the actual path!

Our @lmstrix.json may have an entry like this for an MLX model:

```json
"google/gemma-3n-e4b": {
"id": "google/gemma-3n-e4b",
"short_id": "gemma-3n-e4b",
"path": "google/gemma-3n-e4b",
"size_bytes": 5858917459,
"ctx_in": 32768,
"ctx_out": 4096,
"has_tools": false,
"has_vision": true,
"tested_max_context": 27000,
"loadable_max_context": null,
"last_known_good_context": 27000,
"last_known_bad_context": 30000,
"context_test_status": "completed",
"context_test_log": "/Users/Shared/lmstudio/lmstrix/context_tests/google_gemma-3n-e4b_context_test.log",
"context_test_date": "2025-07-29 20:42:27.325888",
"failed": false,
"error_msg": ""
},
```

and for a GGUF model:

```json
"mradermacher/Meta-Llama-3-8B-Instruct-64k-GGUF/Meta-Llama-3-8B-Instruct-64k.Q8_0.gguf": {
"id": "meta-llama-3-8b-instruct-64k",
"short_id": "Meta-Llama-3-8B-Instruct-64k.Q8_0",
"path": "mradermacher/Meta-Llama-3-8B-Instruct-64k-GGUF/Meta-Llama-3-8B-Instruct-64k.Q8_0.gguf",
"size_bytes": 8541283584,
"ctx_in": 65536,
"ctx_out": 4096,
"has_tools": false,
"has_vision": false,
"tested_max_context": 65536,
"loadable_max_context": null,
"last_known_good_context": 65536,
"last_known_bad_context": null,
"context_test_status": "completed",
"context_test_log": "/Users/Shared/lmstudio/lmstrix/context_tests/meta-llama-3-8b-instruct-64k_context_test.log",
"context_test_date": "2025-07-27 23:37:28.002544",
"failed": false,
"error_msg": ""
},
```

The main difference between the model types is that a GGUF `path` attribute ends with `.gguf` and consists of f"{vendor_folder}/{model_folder}/{model_gguf_file}" while an MLX `path` attribute only has f"{vendor_folder}/{model_folder}" (because multiple files exist in the folder).

Our main LMStudio data folder typically has an `.internal` folder and inside there is a `user-concrete-model-default-config` folder.

Now inside that @.internal/user-concrete-model-default-config is an optional tree of "Concrete JSON" files that is organized using the model paths like so:

- If the model is MLX and has the path `google/gemma-3n-e4b` then we may have `.internal/user-concrete-model-default-config/google/gemma-3n-e4b.json`
- If the model is GGUF and has the path `mradermacher/Meta-Llama-3-8B-Instruct-64k-GGUF/Meta-Llama-3-8B-Instruct-64k.Q8_0.gguf` then we may have `.internal/user-concrete-model-default-config/mradermacher/Meta-Llama-3-8B-Instruct-64k-GGUF/Meta-Llama-3-8B-Instruct-64k.Q8_0.gguf.json`

These "Concrete JSON" files hold user per-model configurations.

If I only modify the default context size at which LM Studio should load the model with, the file will look like so:

```json
{
"preset": "",
"operation": {
"fields": []
},
"load": {
"fields": [
{
"key": "llm.load.contextLength",
"value": 27000
}
]
}
}
```

In addition, of interest is the Flash Attention setting which is only of interest for GGUF models:

```json
{
"preset": "",
"operation": {
"fields": []
},
"load": {
"fields": [
{
"key": "llm.load.contextLength",
"value": 65536
},
{
"key": "llm.load.llama.flashAttention",
"value": true
}
]
}
}
```

Note: The JSON may have other settings.

TASK:

When I do `lmstrix save`, the tool should read our database (lmstrix.json), and for each model that has a `tested_max_context` value it should CREATE or UPDATE the Concrete JSON file inside the `.internal/user-concrete-model-default-config` folder in the LM Studio data folder â€” the Concrete JSON file that corresponds to the model (= `path` attr of the model + `.json` extension)

We should create in memory the skeleton structure:

```
{
"preset": "",
"operation": {
"fields": []
},
"load": {
"fields": [
]
}
}
```

We should read the Concrete JSON data structure if it exists, and MERGE-OVERWRITE it onto our skeleton structure

Then in the `load["fields"]` list, we should check if a dict exists with the key `"key"` and the value `"llm.load.contextLength"` and if so, we should update that very dict, or otherwise create it. In that dict, we add the `"value"` key with the value equal to the model's `tested_max_context`.

Then, if we gave `--flash=True` (or just `--flash`) in our `lmstrix save` command, then (ONLY FOR GGUF MODELS!!!) in the `load["fields"]` list, we should check if a dict exists with the key `"key"` and the value `"llm.load.llama.flashAttention"` and if so, we should update that very dict, or otherwise create it. In that dict, we add the `"value"` key with the value `true`.
