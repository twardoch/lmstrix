This works: 

```
m$ python -c "import lmstudio as lms; model = lms.llm('llama-3.1-8b-sarcasm@q3_k_l', config={'contextLength': 2048}); print(model.complete('Capital of Poland? In one word: ', config={'maxTokens': 32}))"
 Warsaw
Warsaw is the largest and most populous city in Poland. It has a population of around 1,700,000 people.
The city is
```

This hangs until I cancel: 

```
Othello:models adam$ lmstrix test 'llama-3.1-8b-sarcasm@q3_k_l'

Testing model: llama-3.1-8b-sarcasm@q3_k_l
Declared context limit: 131,072 tokens
Threshold: 102,400 tokens

Phase 1: Verifying model loads at 2,048 tokens...
  â†’ Testing context size: 2,048 tokens...
^C{"channel_id": 2, "event": "Received unhandled message {'type': 'channelError', 'channelId': 2, 'error': {'title': 'Model unloaded.', 'stack': 'Error: Model unloaded.\\n    at _0x36289e._0x1c3570 (/Applications/LM Studio.app/Contents/Resources/app/.webpack/main/index.js:88:13462)\\n    at _0x36289e.emit (node:events:518:28)\\n    at _0x36289e.onChildExit (/Applications/LM Studio.app/Contents/Resources/app/.webpack/main/index.js:78:201503)\\n    at _0x28a5b9.<anonymous> (/Applications/LM Studio.app/Contents/Resources/app/.webpack/main/index.js:78:200839)\\n    at _0x28a5b9.emit (node:events:518:28)\\n    at ForkUtilityProcess.<anonymous> (/Applications/LM Studio.app/Contents/Resources/app/.webpack/main/index.js:453:17554)\\n    at ForkUtilityProcess.emit (node:events:518:28)\\n    at ForkUtilityProcess.a.emit (node:electron/js2c/browser_init:2:74608)', 'rootTitle': 'Model unloaded.'}} for already closed channel", "ws_url": "ws://localhost:1234/llm"}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/bin/lmstrix", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py", line 266, in main
    fire.Fire(LMStrixCLI)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py", line 206, in test
    updated_model = tester.test_model(
                    ^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py", line 304, in test_model
    initial_result = self._test_at_context(model.id, min_context, log_path)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py", line 133, in _test_at_context
    response = self.client.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py", line 115, in completion
    response = llm.complete(prompt)
               ^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

Othello:models adam$
```

So it's 