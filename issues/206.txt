This clearly works and shows that the model is fine: 

```
$ python -c "import lmstudio as lms; model = lms.llm('llama-3.1-8b-sarcasm@q3_k_l', config={'contextLength': 2048}); print(model.complete('Capital of Poland? In one word: ', config={'maxTokens': 32}))"
 Warsaw
Population of Warsaw : over 1.8 million people in the city limits, with a larger metropolitan area population of about 3.
```

So there is something in our lmstrix code that is not working: 

```
$ lmstrix test "llama-3.1-8b-sarcasm@q3_k_l"

Testing model: llama-3.1-8b-sarcasm@q3_k_l
Declared context limit: 131,072 tokens
Threshold: 102,400 tokens

Phase 1: Verifying model loads at 2,048 tokens...
  ⚠️  Note: Model has known issues, using adjusted settings...
  → Testing context size: 2,048 tokens...
```

Again, I had to cancel


```
^C^C^CTraceback (most recent call last):
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py", line 112, in completion
    response = future.result(timeout=timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py", line 451, in result
    self._condition.wait(timeout)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 359, in wait
    gotit = waiter.acquire(True, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py", line 164, in _test_at_context
    response = self.client.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py", line 106, in completion
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py", line 647, in __exit__
    self.shutdown(wait=True)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py", line 239, in shutdown
    t.join()
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1149, in join
    self._wait_for_tstate_lock()
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/threading.py", line 1169, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/bin/lmstrix", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py", line 266, in main
    fire.Fire(LMStrixCLI)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py", line 206, in test
    updated_model = tester.test_model(
                    ^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py", line 335, in test_model
    initial_result = self._test_at_context(model.id, min_context, log_path)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py", line 265, in _test_at_context
    llm.unload()
KeyboardInterrupt
```