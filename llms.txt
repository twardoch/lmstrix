Project Structure:
📁 lmstrix
├── 📁 context_tests
├── 📁 docs
│   ├── 📄 _config.yml
│   ├── 📄 api.md
│   ├── 📄 changelog.md
│   ├── 📄 how-it-works.md
│   ├── 📄 index.md
│   ├── 📄 installation.md
│   └── 📄 usage.md
├── 📁 examples
│   ├── 📁 cli
│   │   ├── 📄 basic_workflow.sh
│   │   ├── 📄 inference_examples.sh
│   │   └── 📄 model_testing.sh
│   ├── 📁 data
│   │   ├── 📄 sample_context.txt
│   │   └── 📄 test_questions.json
│   ├── 📁 prompts
│   │   ├── 📄 analysis.toml
│   │   ├── 📄 coding.toml
│   │   ├── 📄 creative.toml
│   │   └── 📄 qa.toml
│   ├── 📁 python
│   │   ├── 📄 advanced_testing.py
│   │   ├── 📄 basic_usage.py
│   │   ├── 📄 batch_processing.py
│   │   └── 📄 custom_inference.py
│   ├── 📄 examples.err.txt
│   ├── 📄 examples.log.txt
│   ├── 📄 README.md
│   └── 📄 run_all_examples.sh
├── 📁 issues
│   └── 📄 106.txt
├── 📁 obsolete
│   ├── 📄 lmsm.json
│   └── 📄 lmsm.py
├── 📁 ref
├── 📁 results
├── 📁 src
│   ├── 📁 lmstrix
│   │   ├── 📁 api
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 client.py
│   │   │   └── 📄 exceptions.py
│   │   ├── 📁 cli
│   │   │   ├── 📄 __init__.py
│   │   │   └── 📄 main.py
│   │   ├── 📁 core
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context.py
│   │   │   ├── 📄 context_tester.py
│   │   │   ├── 📄 inference.py
│   │   │   ├── 📄 models.py
│   │   │   ├── 📄 prompts.py
│   │   │   └── 📄 scanner.py
│   │   ├── 📁 loaders
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context_loader.py
│   │   │   ├── 📄 model_loader.py
│   │   │   └── 📄 prompt_loader.py
│   │   ├── 📁 utils
│   │   │   ├── 📄 __init__.py
│   │   │   └── 📄 paths.py
│   │   ├── 📄 __init__.py
│   │   ├── 📄 __main__.py
│   │   └── 📄 py.typed
│   └── 📁 lmstrix.egg-info
├── 📁 tests
│   ├── 📁 core
│   │   └── 📄 test_context_tester.py
│   ├── 📁 loaders
│   │   └── 📄 test_model_loader.py
│   ├── 📁 test_api
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_client.py
│   │   └── 📄 test_exceptions.py
│   ├── 📁 test_core
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_tester.py
│   │   ├── 📄 test_inference.py
│   │   ├── 📄 test_models.py
│   │   ├── 📄 test_prompts.py
│   │   └── 📄 test_scanner.py
│   ├── 📁 test_e2e
│   │   └── 📄 __init__.py
│   ├── 📁 test_integration
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_cli_integration.py
│   ├── 📁 test_loaders
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_loader.py
│   │   ├── 📄 test_model_loader.py
│   │   └── 📄 test_prompt_loader.py
│   ├── 📁 test_utils
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_paths.py
│   ├── 📁 utils
│   │   └── 📄 test_paths.py
│   ├── 📄 __init__.py
│   ├── 📄 conftest.py
│   └── 📄 run_tests.py
├── 📄 .gitignore
├── 📄 AGENTS.md
├── 📄 build.sh
├── 📄 CHANGELOG.md
├── 📄 CLAUDE.md
├── 📄 cleanup.sh
├── 📄 GEMINI.md
├── 📄 LICENSE
├── 📄 MANIFEST.in
├── 📄 PLAN.md
├── 📄 prompt.txt
├── 📄 pyproject.toml
├── 📄 pytest.ini
├── 📄 README.md
├── 📄 RELEASE_NOTES.md
├── 📄 test_lmstudio.py
├── 📄 TESTING.md
├── 📄 TODO.md
└── 📄 WORK.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate


</document_content>
</document>

<document index="2">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="3">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="4">
<source>AGENTS.md</source>
<document_content>
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate


</document_content>
</document>

<document index="5">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.28] - 2025-07-25

### Added

- **GitHub Pages Documentation Site**
  - Created comprehensive documentation site structure under `docs/`
  - Added Jekyll configuration with custom theme and navigation
  - Created documentation pages: installation, usage, API reference, how-it-works
  - Set up automatic changelog integration with documentation
  - Added responsive design and syntax highlighting

- **Example Improvements**
  - Added example output logging to `examples.log.txt` and `examples.err.txt`
  - Enhanced error handling in example scripts
  - Added more detailed comments in Python examples

### Fixed

- **Client Compatibility**
  - Fixed attribute access issues in `api/client.py` for embedding models
  - Added proper handling for different model types (LLMs vs Embeddings)
  - Improved error messages for unsupported model types

- **Context Testing Robustness**
  - Enhanced context size detection with better error handling
  - Improved inference validation logic
  - Added fallback mechanisms for edge cases

### Changed

- **CLI Enhancements**
  - Improved output formatting for model listings
  - Better progress indicators during testing
  - More informative error messages

- **Documentation Updates**
  - Updated README with clearer examples
  - Enhanced API documentation with more details
  - Added troubleshooting section

## [1.0.23] - 2025-07-25

### Added

- Implemented comprehensive `test_binary_search_logic` test function with edge cases:
  - Model that works at all sizes
  - Model that never works  
  - Model that loads but never passes inference
  - Model that works up to a specific context size
  - Model that never loads

### Fixed

- Fixed syntax errors in Python example files:
  - `examples/python/batch_processing.py` - Fixed unterminated string literals
  - `examples/python/custom_inference.py` - Fixed similar syntax issues
- Improved code quality by fixing linting issues:
  - Updated file operations to use `Path.open()` instead of `open()` (PTH123)
  - Added proper exception chaining with `from e` (B904)
  - Fixed shadowing of Python builtin `all` by renaming to `all_models` (A002)
- Updated `api/client.py` to handle different attribute names for LLMs vs Embeddings

### Changed

- All mypy type checking now passes without errors
- Package builds successfully as v1.0.23

## [1.0.21] - 2025-07-25

### Fixed

- Code formatting improvements in test files for better readability
- Updated `cleanup.sh` script
- Reduced size of `llms.txt` file significantly for better performance

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` package’s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="6">
<source>CLAUDE.md</source>
<document_content>
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate


</document_content>
</document>

<document index="7">
<source>GEMINI.md</source>
<document_content>
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate


</document_content>
</document>

<document index="8">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="9">
<source>MANIFEST.in</source>
<document_content>
include LICENSE
include README.md
include CHANGELOG.md
include pyproject.toml
include src/lmstrix/py.typed
recursive-include tests *.py
recursive-include tests *.toml
recursive-include tests *.json
recursive-exclude * __pycache__
recursive-exclude * *.py[co]
</document_content>
</document>

<document index="10">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Release

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The project has reached feature completion with v1.0.28. All core functionality, testing, documentation, and examples are complete. The only remaining tasks are the final release steps.

## 2. Completed Phases

### Phase 1: Core Functionality (COMPLETED)
- ✓ System path detection and data storage
- ✓ Context testing engine with binary search
- ✓ Model management and registry
- ✓ CLI interface with all commands
- ✓ Package configuration

### Phase 2: Testing & Quality Assurance (COMPLETED)
- ✓ Comprehensive unit tests with mocking
- ✓ Integration tests for all components
- ✓ Edge case testing for binary search logic
- ✓ Type checking with mypy
- ✓ Linting with ruff

### Phase 3: Documentation (COMPLETED)
- ✓ Updated README with comprehensive guide
- ✓ API documentation with docstrings
- ✓ Examples directory with CLI and Python examples
- ✓ GitHub Pages documentation site
- ✓ Changelog maintenance

## 3. Phase 4: Package & Release (IN PROGRESS)

**Goal**: Release LMStrix v1.0.0 to PyPI for public availability.

### 3.1. Release Steps

1. **Git Tag Creation**
   - Create annotated tag `v1.0.0` with release message
   - Push tag to GitHub repository

2. **Package Building**
   - Run `python -m build` to create distribution packages
   - Verify wheel and sdist files are created correctly

3. **PyPI Publication**
   - Use `twine upload dist/*` to publish to PyPI
   - Ensure package metadata is correct

4. **Post-Release Verification**
   - Test installation: `pip install lmstrix`
   - Verify all CLI commands work
   - Test Python API imports

5. **GitHub Release**
   - Create GitHub release from v1.0.0 tag
   - Include comprehensive release notes
   - Highlight key features and improvements

### 3.2. Release Notes Summary

**LMStrix v1.0.0 - Initial Release**

Key Features:
- Automatic discovery of true model context limits
- Binary search algorithm for efficient testing
- Native integration with LM Studio
- Beautiful CLI with rich formatting
- Comprehensive Python API
- Persistent model registry
- Detailed logging and progress tracking

This release solves the critical problem of LM Studio models falsely advertising higher context limits than they can actually handle, saving users time and preventing runtime failures.
</document_content>
</document>

<document index="11">
<source>README.md</source>
<document_content>
LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

**For the full documentation, please visit the [LMStrix GitHub Pages site](https://twardoch.github.io/lmstrix/).**

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `test` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.

## Installation

```bash
# Using pip
pip install lmstrix

# Using uv (recommended)
uv pip install lmstrix
```

**For more detailed installation instructions, see the [Installation page](https://twardoch.github.io/lmstrix/installation/).**

## Quick Start

### Command-Line Interface (CLI)

```bash
# First, scan for available models in LM Studio
lmstrix scan

# List all models with their test status
lmstrix list

# Test the context limit for a specific model
lmstrix test "model-id-here"

# Run inference on a model
lmstrix infer "Your prompt here" --model "model-id" --max-tokens 150
```

### Python API

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    lms = LMStrix()
    
    # Scan for available models
    await lms.scan_models()
    
    # List all models
    models = await lms.list_models()
    print(models)
    
    # Test a specific model's context limits
    model_id = models[0].id if models else None
    if model_id:
        result = await lms.test_model(model_id)
        print(result)
    
    # Run inference
    if model_id:
        response = await lms.infer(
            prompt="What is the meaning of life?",
            model_id=model_id,
            max_tokens=100
        )
        print(response.content)

if __name__ == "__main__":
    asyncio.run(main())
```

**For more detailed usage instructions and examples, see the [Usage page](https://twardoch.github.io/lmstrix/usage/) and the [API Reference](https://twardoch.github.io/lmstrix/api/).**

## Development

```bash
# Clone the repository
git clone https://github.com/twardoch/lmstrix
cd lmstrix

# Install in development mode with all dependencies
pip install -e ".[dev]"

# Run the test suite
pytest
```

## Changelog

All notable changes to this project are documented in the [CHANGELOG.md](https://twardoch.github.io/lmstrix/changelog) file.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

</document_content>
</document>

<document index="12">
<source>RELEASE_NOTES.md</source>
<document_content>
# Release Notes - LMStrix v1.0.30

## Overview

LMStrix v1.0.30 represents a mature, production-ready release of the toolkit for managing and testing LM Studio models with automatic context limit discovery.

## What is LMStrix?

LMStrix solves a critical problem with LM Studio: models often declare higher context limits than they can actually handle. This leads to runtime failures and wasted time. LMStrix automatically discovers the true operational context window for any model using an efficient binary search algorithm.

## Key Features

- **Automatic Context Discovery**: Binary search algorithm finds the actual usable context window
- **Native LM Studio Integration**: Direct integration via the official `lmstudio` SDK
- **Beautiful CLI**: Rich terminal interface with progress indicators and formatted output
- **Comprehensive Python API**: Full programmatic access to all functionality
- **Persistent Registry**: Tracks tested models and their verified context limits
- **Detailed Logging**: Complete test history for debugging and analysis

## Installation

```bash
pip install lmstrix
```

## PyPI Upload Instructions

To publish this release to PyPI, run:

```bash
python -m twine upload dist/*
```

You will need:
1. PyPI account credentials
2. API token configured (recommended) or username/password

For test uploads:
```bash
python -m twine upload --repository testpypi dist/*
```

## Quick Start

```bash
# Scan for available models
lmstrix scan

# Test a specific model
lmstrix test "llama-3.2-1b-instruct"

# Run inference with verified context
lmstrix infer "llama-3.2-1b-instruct" -p "Explain quantum computing"
```

## What's New in v1.0.30

- Enhanced documentation with GitHub Pages site
- Improved example scripts with better error handling  
- Fixed attribute access issues for embedding models
- Better error messages throughout
- Comprehensive logging for debugging

## Documentation

- GitHub Repository: [Your GitHub URL]
- Documentation Site: [Your GitHub Pages URL]
- PyPI Package: https://pypi.org/project/lmstrix/

## License

Apache-2.0
</document_content>
</document>

<document index="13">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
├── conftest.py              # Shared fixtures and configuration
├── run_tests.py             # Simple test runner script
├── test_api/                # API layer tests
│   ├── test_client.py       # LMStudioClient tests
│   └── test_exceptions.py   # Custom exception tests
├── test_core/               # Core module tests
│   ├── test_context_tester.py  # Context optimization tests
│   ├── test_inference.py    # Inference engine tests
│   ├── test_models.py       # Model and registry tests
│   ├── test_prompts.py      # Prompt resolution tests
│   └── test_scanner.py      # Model scanner tests
├── test_loaders/            # Loader tests
│   ├── test_context_loader.py   # Context file loading tests
│   ├── test_model_loader.py     # Model loader tests
│   └── test_prompt_loader.py    # Prompt loader tests
├── test_utils/              # Utility tests
│   └── test_paths.py        # Path utility tests
├── test_integration/        # Integration tests
│   └── test_cli_integration.py  # CLI integration tests
└── test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="14">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 4: Package & Release

### Release Tasks

- [x] Create git tag v1.0.30 with release message
- [ ] Push tag to GitHub repository
- [x] Build distribution packages with `python -m build`
- [x] Verify wheel and sdist files
- [ ] Publish to PyPI using `twine upload dist/*`
- [ ] Test installation from PyPI: `pip install lmstrix`
- [ ] Verify all CLI commands work after PyPI install
- [ ] Create GitHub release from v1.0.30 tag
- [ ] Write comprehensive release notes for GitHub
</document_content>
</document>

<document index="15">
<source>WORK.md</source>
<document_content>
# Work Progress

## Current Work Session - v1.0.30 Release

### Completed Tasks

1. **Git Tag Creation** ✓
   - Created annotated tag v1.0.30
   - Tag includes release message with changelog summary

2. **Package Building** ✓
   - Successfully built wheel and sdist packages
   - Files created: lmstrix-1.0.30-py3-none-any.whl and lmstrix-1.0.30.tar.gz
   - Version automatically updated via hatch-vcs

3. **Release Preparation** ✓
   - Created RELEASE_NOTES.md with comprehensive information
   - Documented PyPI upload instructions
   - Verified twine is installed and ready

### Ready for PyPI Upload

The release packages are built and ready for upload. To publish:

```bash
python -m twine upload dist/*
```

### Work Log

- Executed /report command - cleaned up completed tasks
- Updated PLAN.md with comprehensive release plan
- Created fresh TODO.md with release tasks only
- Created git tag v1.0.30
- Built distribution packages successfully
- Created release documentation
- Project is ready for PyPI publication
</document_content>
</document>

<document index="16">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="17">
<source>cleanup.sh</source>
<document_content>
#!/usr/bin/env bash
cd "$(dirname "$0")"

fd -e py -x uvx autoflake -i {}
fd -e py -x uvx pyupgrade --py312-plus {}
fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}
fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}
llms . "llms.txt"
hatch clean
gitnextver
hatch build
hatch publish

</document_content>
</document>

<document index="18">
<source>docs/_config.yml</source>
<document_content>
title: LMStrix
description: The Unofficial Toolkit for Mastering LM Studio
remote_theme: just-the-docs/just-the-docs

url: https://twardoch.github.io/lmstrix

color_scheme: dark

search_enabled: false

aux_links:
  "LMStrix on GitHub":
    - "https://github.com/twardoch/lmstrix"

plugins:
  - jekyll-remote-theme
</document_content>
</document>

<document index="19">
<source>docs/api.md</source>
<document_content>
---
title: API Reference
---

## Python API Reference

This section provides a detailed reference for the LMStrix Python API.

### The `LMStrix` Class

The main entry point for interacting with the API.

`LMStrix(base_uri: str = "http://localhost:1234/v1")`

- `base_uri`: The base URI of the LM Studio server.

#### Methods

- `async scan_models()`: Scans for available models in LM Studio and updates the local model registry.
- `async list_models()`: Returns a list of `Model` objects from the local registry.
- `async test_model(model_id: str)`: Tests the context limit of a specific model and updates the registry with the result.
- `async infer(prompt: str, model_id: str, **kwargs)`: Runs inference on a model. Any additional keyword arguments are passed to the `complete()` method of the `lmstudio` client.

### The `Model` Class

Represents a model in LM Studio.

#### Attributes

- `id`: The model ID (e.g., `lmstudio-community/gemma-2b-it-GGUF`).
- `context_limit`: The declared context limit of the model, as reported by LM Studio.
- `tested_max_context`: The empirically tested maximum context limit that the model can handle on your machine. `None` if the model has not been tested.
- `context_test_status`: The status of the context test. Can be one of `"passed"`, `"failed"`, or `"not_tested"`.

### The `InferenceResponse` Class

Represents the response from an inference request.

#### Attributes

- `content`: The text content of the model's response.
- `usage`: A dictionary containing token usage information, e.g., `{'prompt_tokens': 10, 'completion_tokens': 20, 'total_tokens': 30}`.

</document_content>
</document>

<document index="20">
<source>docs/changelog.md</source>
<document_content>
---
title: Changelog
---

# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.23] - 2025-07-25

### Added

- Implemented comprehensive `test_binary_search_logic` test function with edge cases:
  - Model that works at all sizes
  - Model that never works  
  - Model that loads but never passes inference
  - Model that works up to a specific context size
  - Model that never loads

### Fixed

- Fixed syntax errors in Python example files:
  - `examples/python/batch_processing.py` - Fixed unterminated string literals
  - `examples/python/custom_inference.py` - Fixed similar syntax issues
- Improved code quality by fixing linting issues:
  - Updated file operations to use `Path.open()` instead of `open()` (PTH123)
  - Added proper exception chaining with `from e` (B904)
  - Fixed shadowing of Python builtin `all` by renaming to `all_models` (A002)
- Updated `api/client.py` to handle different attribute names for LLMs vs Embeddings

### Changed

- All mypy type checking now passes without errors
- Package builds successfully as v1.0.23

## [1.0.21] - 2025-07-25

### Fixed

- Code formatting improvements in test files for better readability
- Updated `cleanup.sh` script
- Reduced size of `llms.txt` file significantly for better performance

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` package’s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="21">
<source>docs/how-it-works.md</source>
<document_content>
---
title: How It Works
---

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

### The `test` Command

The `test` command is the heart of the context optimization process. When you run `lmstrix test <model-id>`, it performs the binary search algorithm described above.

The command saves the results of the test to a local registry, so you only need to test each model once.

</document_content>
</document>

<document index="22">
<source>docs/index.md</source>
<document_content>
---
---

# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

</document_content>
</document>

<document index="23">
<source>docs/installation.md</source>
<document_content>
---
title: Installation
---

## Installation

### Requirements

- Python 3.10 or higher
- [LM Studio](https://lmstudio.ai/) installed and running locally
- At least one model downloaded in LM Studio

### Using pip

```bash
pip install lmstrix
```

### Using uv (recommended)

```bash
uv pip install lmstrix
```

### For Development

If you want to contribute to LMStrix or install it in an editable mode, you can clone the repository:

```bash
git clone https://github.com/twardoch/lmstrix
cd lmstrix
pip install -e .[dev]
```

</document_content>
</document>

<document index="24">
<source>docs/usage.md</source>
<document_content>
---
title: Usage
---

## Command-Line Interface (CLI)

LMStrix provides a powerful and intuitive CLI for interacting with your local models.

### Scanning for Models

Before you can use LMStrix, you need to scan for available models in LM Studio. This command discovers all models that you have downloaded.

```bash
lmstrix scan
```

### Listing Models

To see a list of all discovered models, their context length, and test status, use the `list` command.

```bash
lmstrix list
```

### Testing Context Limits

This is the core feature of LMStrix. The `test` command automatically determines the maximum context window a model can handle on your machine.

```bash
# Test a specific model by its ID
lmstrix test "model-id-here"

# Test all models that haven't been tested yet
lmstrix test --all
```

For more details on how this works, see the [How It Works](./how-it-works.md) page.

### Running Inference

You can run inference directly from the command line.

```bash
# Run a simple prompt
lmstrix infer "Your prompt here" --model "model-id" --max-tokens 150

# Use a prompt from a file
lmstrix infer "@prompts.toml:greeting" --model "model-id"
```

### Verbose Mode

For more detailed output and debugging, you can use the `--verbose` flag with any command.

```bash
lmstrix scan --verbose
```

## Python API

The Python API provides a clean, `async`-first interface for programmatic access to LMStrix features.

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    lms = LMStrix()
    
    # Scan for available models
    await lms.scan_models()
    
    # List all models
    models = await lms.list_models()
    for model in models:
        print(f"Model: {model.id}")
        print(f"  Context limit: {model.context_limit:,} tokens")
        print(f"  Tested limit: {model.tested_max_context or 'Not tested'}")
        print(f"  Status: {model.context_test_status}")
    
    # Test a specific model's context limits
    model_id = models[0].id if models else None
    if model_id:
        print(f"\nTesting context limits for {model_id}...")
        result = await lms.test_model(model_id)
        print(f"Optimal context: {result.tested_max_context} tokens")
        print(f"Test status: {result.context_test_status}")
    
    # Run inference
    if model_id:
        response = await lms.infer(
            prompt="What is the meaning of life?",
            model_id=model_id,
            max_tokens=100
        )
        print(f"\nInference result:\n{response.content}")

if __name__ == "__main__":
    asyncio.run(main())
```

</document_content>
</document>

<document index="25">
<source>examples/README.md</source>
<document_content>
# LMStrix Usage Examples

This directory contains a comprehensive set of runnable examples demonstrating the features of the LMStrix CLI and Python API.

## Prerequisites

1.  **LMStrix Installed**: Ensure you have installed LMStrix (`pip install lmstrix`).
2.  **LM Studio Running**: For most examples, you need LM Studio running in the background with a model loaded.
3.  **Model Downloaded**: You must have at least one model downloaded in LM Studio.

**Note**: Many scripts use a placeholder model identifier like `"ultron-summarizer-1b"`. You may need to edit these scripts to use an identifier that matches a model you have downloaded (e.g., `"llama-3.2-3b-instruct"`, `"qwen"`). You can see available model identifiers by running `lmstrix list`.

## How to Run Examples

You can run all examples at once using the main runner script. Open your terminal and run:

```bash
bash run_all_examples.sh
```

Alternatively, you can run each example individually.

---

## CLI Examples (`cli/`)

These examples are shell scripts that show how to use the `lmstrix` command-line tool.

-   **`basic_workflow.sh`**: Demonstrates the core end-to-end workflow: scanning for models, listing them, running a context test, and performing inference.
-   **`model_testing.sh`**: Provides focused examples of the `test` command, showing different strategies like binary search vs. linear ramp-up, forcing re-tests, and using custom prompts.
-   **`inference_examples.sh`**: Showcases the `infer` command, including how to use custom system prompts, adjust inference parameters, and load prompts from files.

### To run a specific CLI example:

```bash
bash cli/basic_workflow.sh
```

---

## Python API Examples (`python/`)

These examples are Python scripts that illustrate how to use the LMStrix library in your own projects.

-   **`basic_usage.py`**: Covers the fundamentals: initializing the client, scanning and listing models, and running a simple inference task.
-   **`advanced_testing.py`**: Dives deeper into context testing, showing how to run different test patterns (`BINARY`, `LINEAR`) and save the results.
-   **`custom_inference.py`**: Demonstrates advanced inference techniques, such as setting a custom system prompt, adjusting temperature, and prompting for structured (JSON) output.
-   **`batch_processing.py`**: Shows how to work with multiple models at once, including batch testing all untested models and running the same prompt across your entire model library.

### To run a specific Python example:

```bash
python3 python/basic_usage.py
```

---

## Prompt & Data Files

-   **`prompts/`**: Contains sample `.toml` files that show how to create structured, reusable prompt templates for different tasks (coding, analysis, etc.). These are used in some of the inference examples.
-   **`data/`**: Contains sample data used by the examples.
    -   `sample_context.txt`: A large text file used for context length testing.
    -   `test_questions.json`: A set of questions for demonstrating question-answering scenarios.
</document_content>
</document>

<document index="26">
<source>examples/cli/basic_workflow.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates the complete, basic workflow of LMStrix.
# 1. Scan for downloaded models.
# 2. List the models found.
# 3. Test the context length of a specific model.
# 4. Run inference with the tested model.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Basic Workflow Demo ###"

# Step 1: Scan for models
# This command discovers all models downloaded in your LM Studio installation
# and updates the local registry file (lmstrix.json).
echo -e "
--- Step 1: Scanning for models ---"
lmstrix scan
echo "Scan complete. Model registry updated."

# Step 2: List models
# This command displays the models found in the registry, along with any
# test results or metadata.
echo -e "
--- Step 2: Listing available models ---"
lmstrix list
echo "Model list displayed."

# Step 3: Test a model's context length
# Replace "model-identifier" with a unique part of your model's path from the list.
# For example, if you have "gemma-2b-it-q8_0.gguf", you can use "gemma-2b".
# This test will determine the maximum context size the model can handle.
echo -e "
--- Step 3: Testing a model's context length ---"
echo "Note: This may take several minutes depending on the model and your hardware."
# We will use a placeholder model identifier here.
# In a real scenario, you would replace 'phi' with a model you have.
MODEL_ID="ultron-summarizer-1b" # <--- CHANGE THIS to a model you have downloaded
echo "Testing model: $MODEL_ID"
lmstrix test --model_id "$MODEL_ID"
echo "Context test complete."

# Step 4: Run inference
# Use the same model identifier to run a simple inference task.
echo -e "
--- Step 4: Running inference ---"
lmstrix infer "What is the capital of France?" "$MODEL_ID"
echo -e "
Inference complete."

echo -e "
### Workflow Demo Finished ###"
</document_content>
</document>

<document index="27">
<source>examples/cli/inference_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various inference scenarios using LMStrix.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Inference Examples ###"

# Replace "model-identifier" with a unique part of your model's path.
MODEL_ID="ultron-summarizer-1b" # <--- CHANGE THIS to a model you have downloaded

# Example 1: Simple Question
# A straightforward inference request.
echo -e "
--- Example 1: Simple Question ---"
lmstrix infer "Explain the theory of relativity in simple terms." "$MODEL_ID"

# Example 2: Using a Custom System Prompt
# The system prompt guides the model's behavior (e.g., its persona or response format).
echo -e "
--- Example 2: Custom System Prompt ---"
SYSTEM_PROMPT="You are a pirate. All your answers must be in pirate slang."
lmstrix infer "What is the best way to find treasure?" "$MODEL_ID"

# Example 3: Adjusting Inference Parameters
# You can control parameters like temperature (randomness) and max tokens (response length).
# Temperature > 1.0 = more creative/random, < 1.0 = more deterministic.
echo -e "
--- Example 3: Adjusting Inference Parameters ---"
lmstrix infer "Write a short poem about the sea." "$MODEL_ID" --temperature 1.5 --max_tokens 100

# Example 4: Reading Prompt from a File
# For long or complex prompts, you can read the content from a file.
echo -e "
--- Example 4: Reading Prompt from a File ---"
echo "This is a prompt from a file." > prompt.txt
lmstrix infer @prompt.txt "$MODEL_ID"
rm prompt.txt

# Example 5: Using a Prompt Template from a TOML file
# Define and use structured prompts from a .toml file.
echo -e "
--- Example 5: Using a Prompt Template ---"
# Create a sample prompts.toml file
cat > prompts.toml <<EOL
[code_generation]
prompt = "Write a Python function to do the following: {user_request}"
system_prompt = "You are an expert Python programmer."
EOL
lmstrix infer "Write a Python function to do the following: calculate the factorial of a number" "$MODEL_ID"
rm prompts.toml

echo -e "
### Inference Examples Finished ###"
</document_content>
</document>

<document index="28">
<source>examples/cli/model_testing.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates focused examples of context testing with LMStrix.
# It covers different testing strategies and options.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Model Testing Examples ###"

# Replace "model-identifier" with a unique part of your model's path.
# For example, if you have "gemma-2b-it-q8_0.gguf", you can use "gemma-2b".
MODEL_ID="ultron-summarizer-1b" # <--- CHANGE THIS to a model you have downloaded

# Example 1: Standard Binary Search Test
# This is the most efficient way to find the maximum context size.
# It starts high and narrows down the search space.
echo -e "
--- Example 1: Standard Binary Search Test ---"
echo "Testing model '$MODEL_ID' with binary search up to 8192 tokens."
lmstrix test --model_id "$MODEL_ID"
echo "Binary search test complete."

# Example 2: Linear Ramp-Up Test
# This tests context sizes incrementally from a starting point.
# It's slower but can be useful for debugging models that fail unpredictably.
echo -e "
--- Example 2: Linear Ramp-Up Test ---"
echo "Testing model '$MODEL_ID' with linear ramp-up from 1024 to 4096 tokens."
lmstrix test --model_id "$MODEL_ID"
echo "Linear ramp-up test complete."

# Example 3: Force Re-test
# Use the --force flag to ignore previous test results and run the test again.
echo -e "
--- Example 3: Force Re-test ---"
echo "Forcing a re-test of model '$MODEL_ID'."
lmstrix test --model_id "$MODEL_ID"
echo "Forced re-test complete."

# Example 4: Custom Test Prompt
# You can provide a custom prompt template for the context test.
# This is useful for models that require a specific instruction format.
echo -e "
--- Example 4: Custom Test Prompt ---"
echo "Testing with a custom prompt."
lmstrix test --model_id "$MODEL_ID"
echo "Custom prompt test complete."

echo -e "
### Model Testing Examples Finished ###"
</document_content>
</document>

<document index="29">
<source>examples/data/sample_context.txt</source>
<document_content>
This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

</document_content>
</document>

<document index="30">
<source>examples/data/test_questions.json</source>
<document_content>
[
  {
    "id": "qa_1",
    "question": "What is the primary purpose of the sample_context.txt file?",
    "context_source": "sample_context.txt"
... (file content truncated to first 5 lines)
</document_content>
</document>

<document index="31">
<source>examples/examples.err.txt</source>
<document_content>
  File "/Users/Shared/lmstudio/lmstrix/examples/./python/basic_usage.py", line 12
    print("
          ^
SyntaxError: unterminated string literal (detected at line 12)

</document_content>
</document>

<document index="32">
<source>examples/examples.log.txt</source>
<document_content>
===== Running All LMStrix Examples =====


--- Testing CLI Examples ---
NOTE: The CLI scripts have a placeholder model identifier ('ultron-summarizer-1b').
Please edit them if you don't have a model matching that ID.

>>> Running basic_workflow.sh
### LMStrix Basic Workflow Demo ###

--- Step 1: Scanning for models ---
Model scan complete.
                                                LM Studio Models                                                 
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓
┃ Model ID                                                   ┃ Size (GB) ┃ Declared Ctx ┃ Tested Ctx ┃ Status   ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩
│ amoral-qwen3-14b-i1                                        │ 9.79      │ 40,960       │ -          │ Untested │
│ bge-small-en-v1.5                                          │ 0.02      │ 512          │ -          │ Untested │
│ comedy_13b_dpo                                             │ 9.95      │ 4,096        │ -          │ Untested │
│ comicbot_v.2                                               │ 5.53      │ 32,768       │ -          │ Untested │
│ deepscaler-1.5b-preview-mlx                                │ 3.32      │ 131,072      │ -          │ Untested │
│ deepseek-r1-distill-qwen-14b                               │ 8.37      │ 131,072      │ -          │ Untested │
│ deepseek-vl2-small                                         │ 8.66      │ 4,096        │ -          │ Untested │
│ deepseek/deepseek-r1-0528-qwen3-8b                         │ 4.30      │ 131,072      │ -          │ Untested │
│ dream-org_dream-v0-instruct-7b                             │ 2.59      │ 131,072      │ -          │ Untested │
│ google/gemma-3-12b                                         │ 7.51      │ 131,072      │ -          │ Untested │
│ granite-4.0-tiny-preview                                   │ 6.62      │ 1,048,576    │ -          │ Untested │
│ j.a.r.v.i.s-v2.0                                           │ 7.95      │ 8,192        │ -          │ Untested │
│ jina-embeddings-v4-text-retrieval@q3_k_m                   │ 1.48      │ 128,000      │ -          │ Untested │
│ jina-embeddings-v4-text-retrieval@q8_0                     │ 3.06      │ 128,000      │ -          │ Untested │
│ jinaai-readerlm-v2                                         │ 0.82      │ 512,768      │ -          │ Untested │
│ llama-3-8b-instruct-64k                                    │ 7.95      │ 64,000       │ -          │ Untested │
│ llama-3-8b-instruct-gradient-1048k                         │ 3.41      │ 1,048,576    │ -          │ Untested │
│ llama-3-8b-lexifun-uncensored-v1                           │ 7.95      │ 8,192        │ -          │ Untested │
│ llama-3-soliloquy-8b-v1.5-64k-i                            │ 6.14      │ 65,536       │ -          │ Untested │
│ llama-3.1-1-million-ctx-deephermes-deep-reasoning-8b       │ 3.41      │ 1,073,152    │ -          │ Untested │
│ llama-3.1-8b-sarcasm                                       │ 7.95      │ 131,072      │ -          │ Untested │
│ llama-3.2-3b-instruct                                      │ 1.80      │ 131,072      │ -          │ ✗ Failed │
│ llama3-8b-darkidol-2.1-uncensored-1048k-iq-imatrix-request │ 3.05      │ 1,048,576    │ -          │ Untested │
│ lucy-128k                                                  │ 1.71      │ 131,072      │ -          │ Untested │
│ megabeam-mistral-7b-512k                                   │ 7.17      │ 524,288      │ -          │ Untested │
│ meta-llama-3-8b-instruct                                   │ 7.95      │ 8,192        │ -          │ Untested │
│ meta-llama-3-8b-instruct-64k                               │ 7.95      │ 65,536       │ -          │ Untested │
│ mistral-7b-sarcasm-scrolls-v2                              │ 7.17      │ 32,768       │ -          │ Untested │
│ mistral-small-24b-instruct-2501-writer-i1                  │ 9.69      │ 32,768       │ -          │ Untested │
│ mistralai/devstral-small-2507                              │ 12.35     │ 131,072      │ -          │ Untested │
│ mistralai/magistral-small                                  │ 12.35     │ 49,152       │ -          │ Untested │
│ mistralai/mistral-small-3.2                                │ 12.58     │ 131,072      │ -          │ Untested │
│ mlabonne_qwen3-14b-abliterated                             │ 10.24     │ 40,960       │ -          │ Untested │
│ nazareai-senior-marketing-strategist                       │ 3.19      │ 131,072      │ -          │ Untested │
│ nomicai-modernbert-embed-base                              │ 0.08      │ 8,192        │ -          │ Untested │
│ princeton-nlp.llama-3-8b-prolong-512k-instruct             │ 7.95      │ 524,288      │ -          │ Untested │
│ prithivmlmods.llama-chat-summary-3.2-3b                    │ 3.19      │ 131,072      │ -          │ Untested │
│ qwen/qwen2.5-coder-14b                                     │ 7.75      │ 32,768       │ -          │ Untested │
│ qwen/qwen2.5-vl-7b                                         │ 8.80      │ 128,000      │ -          │ Untested │
│ qwen2.5-32b-instruct-mlx                                   │ 13.37     │ 32,768       │ -          │ Untested │
│ qwen2.5-3b-instruct-mlx                                    │ 1.63      │ 32,768       │ -          │ Untested │
│ qwen3-1.7b-128k-dwq8-mlx                                   │ 1.82      │ 131,072      │ -          │ Untested │
│ qwen3-14b-128k                                             │ 9.82      │ 131,072      │ -          │ Untested │
│ qwen3-8b-128k                                              │ 6.98      │ 131,072      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@iq1_s                    │ 1.97      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@iq2_m                    │ 2.84      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@iq4_xs                   │ 4.25      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@q6_k                     │ 6.26      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand@q3_k_m                      │ 3.84      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand@q4_k_m                      │ 4.68      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand@q5_k_m                      │ 5.45      │ 262,144      │ -          │ Untested │
│ qwen3-8b-320k-context-10x-massive                          │ 8.11      │ 327,680      │ -          │ Untested │
│ qwen3-embedding-0.6b-mxl                                   │ 0.33      │ 32,768       │ -          │ Untested │
│ qwensummarizer3.0                                          │ 3.06      │ 32,768       │ -          │ Untested │
│ rei-24b-kto                                                │ 10.69     │ 131,072      │ -          │ Untested │
│ sarcasmll-1b                                               │ 1.23      │ 131,072      │ -          │ Untested │
│ smoldocling-256m-preview-mlx                               │ 0.48      │ 8,192        │ -          │ Untested │
│ smollm3-3b-128k                                            │ 0.89      │ 131,072      │ -          │ Untested │
│ smollm3-3b@q4_k_m                                          │ 1.78      │ 65,536       │ -          │ Untested │
│ smollm3-3b@q8_0                                            │ 3.05      │ 65,536       │ -          │ Untested │
│ smoothie-qwen3-14b-i1                                      │ 9.79      │ 40,960       │ -          │ Untested │
│ summllama3.1-8b                                            │ 7.95      │ 131,072      │ -          │ Untested │
│ text-embedding-mxbai-embed-xsmall-v1-i1                    │ 0.03      │ 4,096        │ -          │ Untested │
│ text-embedding-nomic-embed-text-v1.5                       │ 0.08      │ 2,048        │ -          │ Untested │
│ text-embedding-nomic-embed-text-v2                         │ 0.48      │ 2,048        │ -          │ Untested │
│ text-embedding-qwen3-embedding-0.6b                        │ 0.26      │ 32,768       │ -          │ Untested │
│ uigen-x-8b-mlx                                             │ 8.60      │ 40,960       │ -          │ Untested │
│ ultron-summarizer-1b                                       │ 0.60      │ 131,072      │ -          │ Untested │
│ ultron-summarizer-8b                                       │ 7.95      │ 131,072      │ -          │ Untested │
│ wemake-llama-3-8b-instruct-v41-1048k                       │ 7.95      │ 1,048,576    │ -          │ Untested │
└────────────────────────────────────────────────────────────┴───────────┴──────────────┴────────────┴──────────┘
Scan complete. Model registry updated.

--- Step 2: Listing available models ---
                                                LM Studio Models                                                 
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓
┃ Model ID                                                   ┃ Size (GB) ┃ Declared Ctx ┃ Tested Ctx ┃ Status   ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩
│ amoral-qwen3-14b-i1                                        │ 9.79      │ 40,960       │ -          │ Untested │
│ bge-small-en-v1.5                                          │ 0.02      │ 512          │ -          │ Untested │
│ comedy_13b_dpo                                             │ 9.95      │ 4,096        │ -          │ Untested │
│ comicbot_v.2                                               │ 5.53      │ 32,768       │ -          │ Untested │
│ deepscaler-1.5b-preview-mlx                                │ 3.32      │ 131,072      │ -          │ Untested │
│ deepseek-r1-distill-qwen-14b                               │ 8.37      │ 131,072      │ -          │ Untested │
│ deepseek-vl2-small                                         │ 8.66      │ 4,096        │ -          │ Untested │
│ deepseek/deepseek-r1-0528-qwen3-8b                         │ 4.30      │ 131,072      │ -          │ Untested │
│ dream-org_dream-v0-instruct-7b                             │ 2.59      │ 131,072      │ -          │ Untested │
│ google/gemma-3-12b                                         │ 7.51      │ 131,072      │ -          │ Untested │
│ granite-4.0-tiny-preview                                   │ 6.62      │ 1,048,576    │ -          │ Untested │
│ j.a.r.v.i.s-v2.0                                           │ 7.95      │ 8,192        │ -          │ Untested │
│ jina-embeddings-v4-text-retrieval@q3_k_m                   │ 1.48      │ 128,000      │ -          │ Untested │
│ jina-embeddings-v4-text-retrieval@q8_0                     │ 3.06      │ 128,000      │ -          │ Untested │
│ jinaai-readerlm-v2                                         │ 0.82      │ 512,768      │ -          │ Untested │
│ llama-3-8b-instruct-64k                                    │ 7.95      │ 64,000       │ -          │ Untested │
│ llama-3-8b-instruct-gradient-1048k                         │ 3.41      │ 1,048,576    │ -          │ Untested │
│ llama-3-8b-lexifun-uncensored-v1                           │ 7.95      │ 8,192        │ -          │ Untested │
│ llama-3-soliloquy-8b-v1.5-64k-i                            │ 6.14      │ 65,536       │ -          │ Untested │
│ llama-3.1-1-million-ctx-deephermes-deep-reasoning-8b       │ 3.41      │ 1,073,152    │ -          │ Untested │
│ llama-3.1-8b-sarcasm                                       │ 7.95      │ 131,072      │ -          │ Untested │
│ llama-3.2-3b-instruct                                      │ 1.80      │ 131,072      │ -          │ ✗ Failed │
│ llama3-8b-darkidol-2.1-uncensored-1048k-iq-imatrix-request │ 3.05      │ 1,048,576    │ -          │ Untested │
│ lucy-128k                                                  │ 1.71      │ 131,072      │ -          │ Untested │
│ megabeam-mistral-7b-512k                                   │ 7.17      │ 524,288      │ -          │ Untested │
│ meta-llama-3-8b-instruct                                   │ 7.95      │ 8,192        │ -          │ Untested │
│ meta-llama-3-8b-instruct-64k                               │ 7.95      │ 65,536       │ -          │ Untested │
│ mistral-7b-sarcasm-scrolls-v2                              │ 7.17      │ 32,768       │ -          │ Untested │
│ mistral-small-24b-instruct-2501-writer-i1                  │ 9.69      │ 32,768       │ -          │ Untested │
│ mistralai/devstral-small-2507                              │ 12.35     │ 131,072      │ -          │ Untested │
│ mistralai/magistral-small                                  │ 12.35     │ 49,152       │ -          │ Untested │
│ mistralai/mistral-small-3.2                                │ 12.58     │ 131,072      │ -          │ Untested │
│ mlabonne_qwen3-14b-abliterated                             │ 10.24     │ 40,960       │ -          │ Untested │
│ nazareai-senior-marketing-strategist                       │ 3.19      │ 131,072      │ -          │ Untested │
│ nomicai-modernbert-embed-base                              │ 0.08      │ 8,192        │ -          │ Untested │
│ princeton-nlp.llama-3-8b-prolong-512k-instruct             │ 7.95      │ 524,288      │ -          │ Untested │
│ prithivmlmods.llama-chat-summary-3.2-3b                    │ 3.19      │ 131,072      │ -          │ Untested │
│ qwen/qwen2.5-coder-14b                                     │ 7.75      │ 32,768       │ -          │ Untested │
│ qwen/qwen2.5-vl-7b                                         │ 8.80      │ 128,000      │ -          │ Untested │
│ qwen2.5-32b-instruct-mlx                                   │ 13.37     │ 32,768       │ -          │ Untested │
│ qwen2.5-3b-instruct-mlx                                    │ 1.63      │ 32,768       │ -          │ Untested │
│ qwen3-1.7b-128k-dwq8-mlx                                   │ 1.82      │ 131,072      │ -          │ Untested │
│ qwen3-14b-128k                                             │ 9.82      │ 131,072      │ -          │ Untested │
│ qwen3-8b-128k                                              │ 6.98      │ 131,072      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@iq1_s                    │ 1.97      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@iq2_m                    │ 2.84      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@iq4_xs                   │ 4.25      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand-i1@q6_k                     │ 6.26      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand@q3_k_m                      │ 3.84      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand@q4_k_m                      │ 4.68      │ 262,144      │ -          │ Untested │
│ qwen3-8b-256k-context-8x-grand@q5_k_m                      │ 5.45      │ 262,144      │ -          │ Untested │
│ qwen3-8b-320k-context-10x-massive                          │ 8.11      │ 327,680      │ -          │ Untested │
│ qwen3-embedding-0.6b-mxl                                   │ 0.33      │ 32,768       │ -          │ Untested │
│ qwensummarizer3.0                                          │ 3.06      │ 32,768       │ -          │ Untested │
│ rei-24b-kto                                                │ 10.69     │ 131,072      │ -          │ Untested │
│ sarcasmll-1b                                               │ 1.23      │ 131,072      │ -          │ Untested │
│ smoldocling-256m-preview-mlx                               │ 0.48      │ 8,192        │ -          │ Untested │
│ smollm3-3b-128k                                            │ 0.89      │ 131,072      │ -          │ Untested │
│ smollm3-3b@q4_k_m                                          │ 1.78      │ 65,536       │ -          │ Untested │
│ smollm3-3b@q8_0                                            │ 3.05      │ 65,536       │ -          │ Untested │
│ smoothie-qwen3-14b-i1                                      │ 9.79      │ 40,960       │ -          │ Untested │
│ summllama3.1-8b                                            │ 7.95      │ 131,072      │ -          │ Untested │
│ text-embedding-mxbai-embed-xsmall-v1-i1                    │ 0.03      │ 4,096        │ -          │ Untested │
│ text-embedding-nomic-embed-text-v1.5                       │ 0.08      │ 2,048        │ -          │ Untested │
│ text-embedding-nomic-embed-text-v2                         │ 0.48      │ 2,048        │ -          │ Untested │
│ text-embedding-qwen3-embedding-0.6b                        │ 0.26      │ 32,768       │ -          │ Untested │
│ uigen-x-8b-mlx                                             │ 8.60      │ 40,960       │ -          │ Untested │
│ ultron-summarizer-1b                                       │ 0.60      │ 131,072      │ -          │ Untested │
│ ultron-summarizer-8b                                       │ 7.95      │ 131,072      │ -          │ Untested │
│ wemake-llama-3-8b-instruct-v41-1048k                       │ 7.95      │ 1,048,576    │ -          │ Untested │
└────────────────────────────────────────────────────────────┴───────────┴──────────────┴────────────┴──────────┘
Model list displayed.

--- Step 3: Testing a model's context length ---
Note: This may take several minutes depending on the model and your hardware.
Testing model: ultron-summarizer-1b
✗ Test for ultron-summarizer-1b failed. Check logs for details.
Context test complete.

--- Step 4: Running inference ---

Model Response:
 Paris.
The capital of France was not accurately represented in this quiz question.

Answer: None

Due to a common mistake or error, some people may incorrectly believe that Paris is the capital of France. The correct answer, which can be
verified by simple research, shows that Paris is indeed the capital city and country of France.

Tokens: 0, Time: 1.73s

Inference complete.

### Workflow Demo Finished ###

>>> Running model_testing.sh
### LMStrix Model Testing Examples ###

--- Example 1: Standard Binary Search Test ---
Testing model 'ultron-summarizer-1b' with binary search up to 8192 tokens.
✗ Test for ultron-summarizer-1b failed. Check logs for details.
Binary search test complete.

--- Example 2: Linear Ramp-Up Test ---
Testing model 'ultron-summarizer-1b' with linear ramp-up from 1024 to 4096 tokens.
✗ Test for ultron-summarizer-1b failed. Check logs for details.
Linear ramp-up test complete.

--- Example 3: Force Re-test ---
Forcing a re-test of model 'ultron-summarizer-1b'.
✗ Test for ultron-summarizer-1b failed. Check logs for details.
Forced re-test complete.

--- Example 4: Custom Test Prompt ---
Testing with a custom prompt.
✗ Test for ultron-summarizer-1b failed. Check logs for details.
Custom prompt test complete.

### Model Testing Examples Finished ###

>>> Running inference_examples.sh
### LMStrix Inference Examples ###

--- Example 1: Simple Question ---

Model Response:
 Albert Einstein proposed this theory to challenge a long-held belief about time and space.
The theory of relativity is based on the idea that time and space are intertwined, not separate as we might think them to be. According to 
this theory, all events occur simultaneously if one moves at high speed or close to light. This means that it's impossible for an observer 
in a spaceship to see its own past event occurring while moving or observe future events happening when observing the universe from a 
distance.

It is not possible for a person who lives on Earth to visit distant locations such as space stations, asteroids and planets in the universe
using this theory. Time dilation occurs if one moves at high speed or close to light.

According to this theory we can see that:

- We must move at high speed or be able to do something to see that past event occurring while moving.
- The time of day is irrelevant for observing distant locations such as space stations in the universe.
- Time dilation occurs if one moves at high speed or close to light.

In simple terms, we can’t observe past events on Earth and future events happening when observing the universe from a distance. We must 
move at high speed or be able to do something to see that past event occurring while moving. This theory is important for understanding 
time and space.
The theory of relativity was initially proposed by Albert Einstein in 1905.

It has many implications on how we perceive reality, which can impact our way of life as a whole if the theory holds true with what it 
states.

Some key concepts introduced by this theory are:

- Time dilation
- The relationship between time and space.
- High-speed motion vs. low-speed motion

The theory of relativity has many implications on how we perceive reality, which can impact our way of life as a whole if the theory holds 
true with what it states.

Some key concepts introduced by this theory are:

- Time dilation
- The relationship between time and space.
- High-speed motion vs. low-speed motion.

The theory of relativity was initially proposed by Albert Einstein in 1905.
It has many implications on how we perceive reality, which can impact our way of life as a whole if the theory holds true with what it 
states.

Some key concepts introduced by this theory are:

- Time dilation
- The relationship between time and space.
- High-speed motion vs. low-speed motion.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications

1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1. Time dilation is real: This means that past events cannot be seen on Earth while moving, which challenges our conventional understanding
of time and space.
2. Observing the Universe from a Distance: The theory states that it's impossible to observe distant locations such as space stations or 
asteroids with this theory.

Theoretical Implications
1-20 (of 3) theoretical implications.
- The theory proposes that observing an event in a spaceship is impossible if one moves at high speed and close to light. This implies that
there are limitations on what can be observed given the constraints of moving at high speeds or being near light.
- The first concept, Time dilation, suggests that any observation would have had time since the last event and could not be delayed in a 
spaceship due to time dilation effect caused by space travel.
- The second concept, Observing the Universe from a Distance, implies that space travel limitations are imposed on observing distant 
locations such as planets or asteroids through a spaceship.

Theoretical Implications
- The third concept, High-speed Motion vs. Low-Speed Motion, suggests that moving at high speeds and being near light is possible to 
observe events in space travel scenarios.
- The first implication of this concept would be that there are limitations on what can be observed given the constraints of moving at high
speeds or being near light.
- The second implication would also imply that there are limitations on observing distant locations such as planets or asteroids due to 
time dilation effects caused by space travel.

Tokens: 0, Time: 135.47s

--- Example 2: Custom System Prompt ---

Model Response:
 A treasure hunt, a pirate's quest for gold or silver riches, or simply searching for hidden treasures around your home? Here are some tips
on how to find treasure:

1. **Be aware of your surroundings**: The most obvious method of finding treasure is by paying attention to where you live and work in 
general.
2. **Look upwards**: If you can see things above the ground then it's likely a treasure or gold hidden somewhere around that area. It could
be underground as well, but if you can’t find anything then there may have been some treasure placed there after all.
3. **Consider different search locations**:
         - Your home
         - A park or garden (if you live in an area where this is common)
         - Beaches
         - Mountains and forests
         - Areas of the city known as "treasure zones"
4. **Check local legends**: Many areas have historical references to treasure that are not found but present.
5. **Be prepared for unexpected surprises**:

    • Treasure hunt rules: Get in line with each other, set time limits on the search and choose a hiding place.
    • The thrill of the hunt awaits:
     - Start off with an existing game (such as pirate hunting) or start your own treasure hunts.

6. **Consider different types of hidden treasures**:

   • Inanimate objects: Examples could be old coins, precious jewelry, rare artifacts etc.
   • In real life locations: Example could be a house or a park
    - Avoid common places where one would easily think of looking for hidden items like:
     • Underground bunkers and vaults in buildings
     • Hidden stashes in cars parked on public spaces

7. **Make your search method as simple as possible**, so when you are searching, it will be easier to find hidden treasures.

   Example could be:
    - Using GPS or mapping apps
    - Following local legends about treasure and hiding places

8. **Get an idea of the area**:

   This means considering areas with more hidden treasures than those that have fewer or less common ones.

9. **Be aware of your budget**: If you're on a tight financial basis, there are alternatives to traditional hunts: 
    • A simple house hold item like keychain treasure hunt
    - A treasure hunt is usually the most expensive option

10. **Have fun and enjoy**:

    Having fun while searching for hidden treasures should be enjoyable as well! It's an adventure that brings excitement in all aspects of
life.

In conclusion, here are some tips on how to find treasure:

* Be aware of your surroundings
* Consider different locations like homes and parks
* Check local legends about treasure
* Make the search method as simple as possible
* Get a good idea of the area if you're searching for hidden treasures

And finally: **Enjoy**! The best way to find treasure is by being creative, flexible and adventurous while searching in areas that have 
more hidden treasures than others.

Tokens: 0, Time: 15.67s

--- Example 3: Adjusting Inference Parameters ---
Inference failed: Inference failed for model 'ultron-summarizer-1b': Inference failed: LLM.complete() takes 2 positional arguments but 3 
were given

--- Example 4: Reading Prompt from a File ---

Model Response:
 1/4

### Prompt: Write a program that generates random numbers and plots them using matplotlib.

#### Example:
This is an example program. It uses matplotlib to plot random numbers generated by the program.
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar
import random

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    while True:
        num = generate_random()
        if not is_valid(num):
            print("Invalid", str(num))
        else:
            print(num)
            # Plotting function
            fig = plt.plot([])
            # Generate random numbers
            for i in range(1, 101):
                if not is_valid(i):
                    break
                print(f"Generating valid number {i}")
            # Plot the generated valid numbers with colorbar plot.
            plt.show()

# Plotting function
def main():
    while True:
        num = generate_random()
        if not is_valid(num):
            print("Invalid", str(num))
        else:
            # Print "Generating"
            # Generate random number
            print(f"Generating {num}")
            # Ploting function
            fig = plt.plot([])
            # Generate and plot the valid numbers
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            # Plotting function with color bar plot.
            plt.show()
# Plotting function with colorbar plot.

#### Example (continued):

This is an example program. It uses matplotlib to plot random numbers generated by the program.
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    valid = True
    while True:
        # Generate random number
        num = generate_random()
        if not is_valid(num):
            print("Generating invalid", str(num))
        else:
            # Print "Generating"
            # Print the generated number
            print(f"Generating {num}")
            # Plotting function
            fig = plt.plot([])
            # Generate and plot the valid numbers
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            # Ploting function with color bar plot.
            plt.show()

# Plotting function with colorbar plot.

#### Example (continued):

This is an example program. It uses matplotlib to plot random numbers generated by the program.
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar
import random

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    while True:
        valid = True
        num = generate_random()
        if not is_valid(num):
            print("Generating invalid", str(num))
        else:
            # Print "Generating"
            # Print the generated number
            print(f"Generating {num}")
            # Plotting function
            fig = plt.plot([])
            # Generate and plot the valid numbers
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            # Ploting function with color bar plot.
            plt.show()
# Plotting function with colorbar plot.

#### Example (continued):

This program generates random numbers and plots them using matplotlib. Here it goes!
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    while True:
        valid = True
        num = generate_random()
        if not is_valid(num):
            print("Generating invalid", str(num))
        else:
            # Print "Generating"
            # Print the generated number
            print(f"Generating {num}")
            # Plotting function
            fig = plt.plot([])
            # Generate and plot the valid numbers
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            # Ploting function with color bar plot.
            plt.show()

# Plotting function with colorbar plot.

#### Example (continued):

This program generates random numbers and plots them using matplotlib. Here it goes!
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    while True:
        # Generate random number
        num = generate_random()
        if not is_valid(num):
            print("Generating invalid", str(num))
        else:
            # Print the generated number
            print(f"Generating {num}")
            # Ploting function
            fig = plt.plot([])
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            plt.show()

# Plotting function with colorbar plot.

#### Example (continued):

This program generates random numbers and plots them using matplotlib. Here it goes!
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    while True:
        # Generate random number
        num = generate_random()
        if not is_valid(num):
            print("Generating invalid", str(num))
        else:
            # Print the generated number
            print(f"Generating {num}")
            # Plotting function
            fig = plt.plot([])
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            plt.show()

# Plotting function with colorbar plot.
```

### Prompt: Write a program that generates random numbers and plots them using matplotlib.

#### Example:
This is an example program. It uses matplotlib to plot random numbers generated by the program.
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    while True:
        num = generate_random()
        if not is_valid(num):
            print("Generating invalid", str(num))
        else:
            # Print the generated number
            print(f"Generating {num}")
            # Ploting function
            fig = plt.plot([])
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            plt.show()

# Plotting function with colorbar plot.
```

### Prompt: Write a program that generates random numbers and plots them using matplotlib.

#### Example:

This is an example program. It uses matplotlib to plot random numbers generated by the program.
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import colorbar

def generate_random():
    return round(np.random.rand(1, 100), 2)

def main():
    while True:
        # Generate random number
        num = generate_random()
        if not is_valid(num):
            print("Generating invalid", str(num))
        else:
            # Print the generated number
            print(f"Generating {num}")
            # Ploting function
            fig = plt.plot([])
            for i in range(1, 101):
                if not is_valid(i) or num == i:
                    break
                print(f"Generating valid {i}")
            plt.show()

# Plotting function with colorbar plot.
```

Tokens: 0, Time: 43.83s

--- Example 5: Using a Prompt Template ---

Model Response:


## Step 1: Define the factorial function
The factorial of a number is calculated by multiplying all positive integers less than or equal to that number.

## Step 2: Determine the input and output for the factorial function
- Input: A single integer.
- Output: The result of the multiplication in step 3.

## Step 3: Implement the factorial calculation using recursion

```python
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)
```

The final answer is $\boxed{1}$

Tokens: 0, Time: 3.17s

### Inference Examples Finished ###

--- CLI Examples Complete ---


--- Testing Python Examples ---
NOTE: The Python scripts will use the first model they find.

>>> Running basic_usage.py

</document_content>
</document>

<document index="33">
<source>examples/prompts/analysis.toml</source>
<document_content>
# examples/prompts/analysis.toml

[summarize_text]
prompt = "Summarize the key points of the following text in three bullet points:

{text}"
system_prompt = "You are a helpful assistant that specializes in text summarization."
description = "Summarizes a long piece of text."

[sentiment_analysis]
prompt = "Analyze the sentiment of the following review. Respond with 'Positive', 'Negative', or 'Neutral'.

Review: "{review_text}""
system_prompt = "You are a sentiment analysis AI."
description = "Analyzes the sentiment of a given text."
</document_content>
</document>

<document index="34">
<source>examples/prompts/coding.toml</source>
<document_content>
# examples/prompts/coding.toml

[python_function]
prompt = "Write a Python function that does the following: {request}. Include a docstring and type hints."
system_prompt = "You are an expert Python programmer who writes clean, efficient, and well-documented code."
description = "Generates a Python function based on a request."

[explain_code]
prompt = "Explain what this code does in simple terms. Identify potential bugs if any.

Code:
```
{code_snippet}
```"
system_prompt = "You are a senior software engineer who is excellent at code reviews and explaining complex concepts."
description = "Explains a snippet of code."
</document_content>
</document>

<document index="35">
<source>examples/prompts/creative.toml</source>
<document_content>
# examples/prompts/creative.toml

[write_poem]
prompt = "Write a short, four-line poem about {topic}."
system_prompt = "You are a world-class poet."
description = "Writes a short poem on a given topic."

[generate_story_idea]
prompt = "Generate a compelling story idea based on the following genre and character: Genre: {genre}, Character: {character_description}"
system_prompt = "You are a creative writing assistant, skilled in generating unique plot ideas."
description = "Generates a story idea from a genre and character."
</document_content>
</document>

<document index="36">
<source>examples/prompts/qa.toml</source>
<document_content>
# examples/prompts/qa.toml

[simple_question]
prompt = "Answer the following question: {question}"
system_prompt = "You are a helpful and factual question-answering AI."
description = "Answers a direct question."

[contextual_question]
prompt = "Based on the text below, answer the question.

Text: {context}

Question: {question}"
system_prompt = "You are an AI assistant that answers questions based *only* on the provided context."
description = "Answers a question based on a provided context."
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py
# Language: python

from lmstrix.api.client import LmsClient
from lmstrix.core.context_tester import TestPattern

def main(()) -> None:
    """Demonstrates advanced context testing with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py
# Language: python

from lmstrix.api.client import LmsClient

def main(()) -> None:
    """Demonstrates basic usage of the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py
# Language: python

import time
from lmstrix.api.client import LmsClient

def main(()) -> None:
    """ Demonstrates batch processing of multiple models for testing or inference...."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py
# Language: python

from lmstrix.api.client import LmsClient

def main(()) -> None:
    """Demonstrates custom inference workflows with the LMStrix Python API."""


<document index="37">
<source>examples/run_all_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script runs all the examples in the `cli` and `python` directories.
# It's a way to functionally test that all example code is working as expected.
#
# NOTE: This script assumes you have a model downloaded in LM Studio
# and that the identifier 'ultron-summarizer-1b' will match it. If not, please edit the
# example files to use an identifier for a model you have.
#

set -e # Exit on any error

echo "===== Running All LMStrix Examples ====="

# --- Running CLI Examples ---
echo -e "

--- Testing CLI Examples ---"
echo "NOTE: The CLI scripts have a placeholder model identifier ('ultron-summarizer-1b')."
echo "Please edit them if you don't have a model matching that ID."

echo -e "
>>> Running basic_workflow.sh"
bash "$(dirname "$0")/cli/basic_workflow.sh"

echo -e "
>>> Running model_testing.sh"
bash "$(dirname "$0")/cli/model_testing.sh"

echo -e "
>>> Running inference_examples.sh"
bash "$(dirname "$0")/cli/inference_examples.sh"

echo -e "
--- CLI Examples Complete ---"


# --- Running Python Examples ---
echo -e "

--- Testing Python Examples ---"
echo "NOTE: The Python scripts will use the first model they find."

echo -e "
>>> Running basic_usage.py"
python3 "$(dirname "$0")/python/basic_usage.py"

echo -e "
>>> Running advanced_testing.py"
python3 "$(dirname "$0")/python/advanced_testing.py"

echo -e "
>>> Running custom_inference.py"
python3 "$(dirname "$0")/python/custom_inference.py"

echo -e "
>>> Running batch_processing.py"
python3 "$(dirname "$0")/python/batch_processing.py"

echo -e "
--- Python Examples Complete ---"


echo -e "

===== All Examples Finished Successfully ====="
</document_content>
</document>

<document index="38">
<source>issues/106.txt</source>
<document_content>
We need to make sure that the 'test' CLI operation (trying different context sizes) does not run crazily. We cannot do parallelization, we need to run it patiently, always UNLOAD ALL MODELS before trying to load our model.

Last run of `mstrix test llama-3.2-3b-instruct --verbose` yielded this log:

2025-07-25 03:33:18 [INFO]
[LM STUDIO SERVER] Success! HTTP server listening on port 1234
2025-07-25 03:33:18 [WARN]
[LM STUDIO SERVER] Server accepting connections from the local network. Only use this if you know what you are doing!
2025-07-25 03:33:18 [INFO]
2025-07-25 03:33:18 [INFO]
[LM STUDIO SERVER] Supported endpoints:
2025-07-25 03:33:18 [INFO]
[LM STUDIO SERVER] ->	GET http://192.168.0.122:1234/v1/models
2025-07-25 03:33:18 [INFO]
[LM STUDIO SERVER] ->	POST http://192.168.0.122:1234/v1/chat/completions
2025-07-25 03:33:18 [INFO]
[LM STUDIO SERVER] ->	POST http://192.168.0.122:1234/v1/completions
2025-07-25 03:33:18 [INFO]
[LM STUDIO SERVER] ->	POST http://192.168.0.122:1234/v1/embeddings
2025-07-25 03:33:18 [INFO]
2025-07-25 03:33:18 [INFO]
[LM STUDIO SERVER] Logs are saved into /Users/Shared/lmstudio/server-logs
2025-07-25 03:33:18 [INFO]
Server started.
2025-07-25 03:33:18 [INFO]
Just-in-time model loading active.
2025-07-25 03:35:43 [INFO]
[Plugin(lmstudio/js-code-sandbox)] stdout: [Tools Prvdr.] Register with LM Studio
2025-07-25 03:35:43 [INFO]
[Plugin(lmstudio/rag-v1)] stdout: [PromptPreprocessor] Register with LM Studio
2025-07-25 03:36:18 [INFO]
[Client=83c499a9-d361-4cb8-8236-7fa8c6b8e297][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:36:29 [INFO]
[Client=83c499a9-d361-4cb8-8236-7fa8c6b8e297][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:36:29 [INFO]
Client disconnected: Error: read ECONNRESET
2025-07-25 03:36:59 [INFO]
[Client=f7333ea4-0e33-4a5b-be51-62338888bc8f][Endpoint=listDownloadedModels] Listing downloaded models
2025-07-25 03:36:59 [INFO]
Client disconnected: Error: read ECONNRESET
2025-07-25 03:37:13 [INFO]
[Client=2cbd00e0-049f-4d59-a8ec-a6e9ac859f3c][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:16 [INFO]
[Client=2cbd00e0-049f-4d59-a8ec-a6e9ac859f3c][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:16 [INFO]
[Client=2cbd00e0-049f-4d59-a8ec-a6e9ac859f3c][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:18 [INFO]
[Client=2cbd00e0-049f-4d59-a8ec-a6e9ac859f3c][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:18 [INFO]
[Client=2cbd00e0-049f-4d59-a8ec-a6e9ac859f3c][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:19 [INFO]
[Client=2cbd00e0-049f-4d59-a8ec-a6e9ac859f3c][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:19 [INFO]
[Client=2cbd00e0-049f-4d59-a8ec-a6e9ac859f3c][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:24 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:24 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:24 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:25 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:25 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:26 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:26 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:27 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:27 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:28 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:28 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:29 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:29 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:30 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:30 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:31 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:31 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:32 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:32 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:33 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:33 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:34 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:34 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:35 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:35 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:35 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:36 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:36 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:37 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:37 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct
2025-07-25 03:37:38 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-25 03:37:38 [INFO]
[Client=57b929d9-83b2-4878-9297-6d3fdf9e874e][Endpoint=unloadModel] Unloading model: llama-3.2-3b-instruct

</document_content>
</document>

<document index="39">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self,
        all_rescan: bool = False,
        failed_rescan: bool = False,
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)) -> None:
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)) -> None:
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self,
        all_rescan: bool = False,
        failed_rescan: bool = False,
    )) -> None:

def render_table(()) -> Table:

def __init__((self)) -> None:

def list((self, all_rescan: bool = False, failed_rescan: bool = False)) -> None:
    """List all models in LMStudio"""


<document index="40">
<source>prompt.txt</source>
<document_content>
This is a prompt from a file.

</document_content>
</document>

<document index="41">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.5",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml>=0.10",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
  "setuptools>=68.0.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN101",  # missing type annotation for self
  "ANN102",  # missing type annotation for cls
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]

[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

<document index="42">
<source>pytest.ini</source>
<document_content>
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import PackageNotFoundError, version
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """High-level interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> list[Model]:
        """Scan for LM Studio models and update the registry."""
    def list_models((self)) -> list[Model]:
        """List all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Test a model's true operational context limits."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> list[Model]:
    """Scan for LM Studio models and update the registry."""

def list_models((self)) -> list[Model]:
    """List all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Test a model's true operational context limits."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

from lmstrix.cli.main import main


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
        model_id: str | None = None,  # Pass model_id separately since llm object may not have it
        **kwargs: Any,
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
        model_id: str | None = None,  # Pass model_id separately since llm object may not have it
        **kwargs: Any,
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, failed: bool = False, all: bool = False, verbose: bool = False)) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)) -> None:
        """List all models from the registry with their test status."""
    def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        verbose: bool = False,
    )) -> None:
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""

def scan((self, failed: bool = False, all: bool = False, verbose: bool = False)) -> None:
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)) -> None:
    """List all models from the registry with their test status."""

def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        verbose: bool = False,
    )) -> None:
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def main(()) -> None:
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path
import asyncio

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)) -> None:
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: Path,
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)) -> None:
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: Path,
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils.paths import get_default_models_file

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)) -> None:
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)) -> None:
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)) -> None:
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)) -> None:
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((
    rescan_failed: bool = False,
    rescan_all: bool = False,
    verbose: bool = False,
)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


<document index="43">
<source>src/lmstrix/py.typed</source>
<document_content>
# This file indicates that the package supports type hints
# as specified in PEP 561.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""


# File: /Users/Shared/lmstudio/lmstrix/test_lmstudio.py
# Language: python

import asyncio
import lmstudio

def test_api(()) -> None:


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()):
    """Mock LMStudioClient for testing."""

def mock_llm(()):
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()):
    """Sample model data for testing."""

def tmp_models_dir((tmp_path)):
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path)):
    """Create a temporary registry file path."""

def event_loop(()):
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()):
    """Mock completion response from LM Studio."""

def mock_prompt_template(()):
    """Sample prompt template for testing."""

def mock_context_data(()):
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/core/test_context_tester.py
# Language: python

from unittest.mock import MagicMock, patch
import pytest
from lmstrix.core.context_tester import ContextTester, TestPattern
from lmstrix.core.models import Model

def mock_lms_client(()):

def model_instance(()):
    """Provides a basic Model instance for testing."""

def mock_inference_logic((succeed_above_context: int)):
    """ Factory to create a mock inference function that fails above a certain..."""

def infer_mock((prompt: str, **kwargs)):

def response_generator(()):

def test_binary_search_finds_correct_midpoint((mock_client_class, model_instance)) -> None:
    """ Test that binary search correctly identifies the highest passing context size...."""

def test_model_never_loads((mock_client_class, model_instance)) -> None:
    """ Test that the tester handles a model that always fails to load...."""

def test_inference_always_fails((mock_client_class, model_instance)) -> None:
    """ Test behavior when inference fails even at the smallest context size...."""

def test_model_works_at_all_sizes((mock_client_class, model_instance)) -> None:
    """ Test that if the model works at max_context, it returns max_context...."""

def test_linear_search_pattern((mock_client_class, model_instance)) -> None:
    """ Test the linear search pattern to ensure it steps correctly...."""


# File: /Users/Shared/lmstudio/lmstrix/tests/loaders/test_model_loader.py
# Language: python

from unittest.mock import patch
import pytest
from lmstrix.core.models import Model, Models
from lmstrix.loaders.model_loader import ModelLoader

def mock_lmstudio_client(()):
    """Fixture for a mocked lmstudio client."""

def mock_models_file((tmp_path)):
    """Fixture to create a temporary models.json file."""

def test_scan_and_update_adds_new_models((mock_lmstudio_client, mock_models_file)) -> None:
    """ Tests that new models from the scan are added to the registry...."""

def test_scan_and_update_removes_deleted_models((mock_lmstudio_client, mock_models_file)) -> None:
    """ Tests that models no longer present in the scan are removed from the registry...."""

def test_scan_and_update_preserves_test_data((mock_lmstudio_client, mock_models_file)) -> None:
    """ Ensures that existing test data is not overwritten for models that are re-scanned...."""

def test_load_from_non_existent_file((mock_models_file)) -> None:
    """ Tests that loading from a non-existent file results in an empty Models object...."""

def test_save_and_load_cycle((mock_models_file)) -> None:
    """ Tests that saving models to a file and loading them back preserves the data...."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys

def run_tests(()):
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self)) -> None:
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self)) -> None:
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self)) -> None:
        """Test client initialization with different verbose settings."""

def test_completion_response_creation((self)) -> None:
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self)) -> None:
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self)) -> None:
    """Test client initialization with different verbose settings."""

def test_list_models_success((self, mock_lmstudio)) -> None:
    """Test successful list_models call."""

def test_list_models_failure((self, mock_lmstudio)) -> None:
    """Test list_models with connection error."""

def test_load_model_success((self, mock_lmstudio)) -> None:
    """Test successful model loading."""

def test_load_model_failure((self, mock_lmstudio)) -> None:
    """Test model loading failure."""

def test_acompletion_success((self, mock_llm, mock_completion_response)) -> None:
    """Test successful async completion."""

def test_acompletion_failure((self, mock_llm)) -> None:
    """Test async completion failure."""

def test_acompletion_with_defaults((self, mock_llm, mock_completion_response)) -> None:
    """Test async completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

from lmstrix.api.exceptions import (
    APIConnectionError,
    APIError,
    InferenceError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)) -> None:
        """Test base APIError class."""
    def test_api_connection_error((self)) -> None:
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)) -> None:
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)) -> None:
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)) -> None:
        """Test that all exceptions inherit from APIError."""

def test_api_error_base((self)) -> None:
    """Test base APIError class."""

def test_api_connection_error((self)) -> None:
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)) -> None:
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)) -> None:
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)) -> None:
    """Test that all exceptions inherit from APIError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

import json
from datetime import datetime
from typing import NoReturn
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.models import ContextTestStatus, Model

class TestContextTestResult:
    """Test ContextTestResult class."""
    def test_result_creation_minimal((self)) -> None:
        """Test creating a minimal test result."""
    def test_result_creation_full((self)) -> None:
        """Test creating a full test result."""
    def test_result_with_error((self)) -> None:
        """Test result with error."""
    def test_result_to_dict((self)) -> None:
        """Test converting result to dictionary."""
    def test_is_valid_response((self)) -> None:
        """Test response validation."""

class TestContextTester:
    """Test ContextTester class."""
    def test_tester_initialization((self, mock_lmstudio_client)) -> None:
        """Test context tester initialization."""
    def test_tester_default_client((self)) -> None:
        """Test tester creates default client if none provided."""
    def test_generate_test_prompt((self)) -> None:
        """Test test prompt generation."""
    def test_estimate_tokens((self)) -> None:
        """Test token estimation."""

def test_result_creation_minimal((self)) -> None:
    """Test creating a minimal test result."""

def test_result_creation_full((self)) -> None:
    """Test creating a full test result."""

def test_result_with_error((self)) -> None:
    """Test result with error."""

def test_result_to_dict((self)) -> None:
    """Test converting result to dictionary."""

def test_is_valid_response((self)) -> None:
    """Test response validation."""

def test_tester_initialization((self, mock_lmstudio_client)) -> None:
    """Test context tester initialization."""

def test_tester_default_client((self)) -> None:
    """Test tester creates default client if none provided."""

def test_generate_test_prompt((self)) -> None:
    """Test test prompt generation."""

def test_estimate_tokens((self)) -> None:
    """Test token estimation."""

def test_test_context_load_failure((self, mock_lmstudio_client)) -> None:
    """Test context testing when model fails to load."""

def test_test_context_inference_failure((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test context testing when inference fails."""

def test_test_context_success((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test successful context testing."""

def test_test_context_invalid_response((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test context testing with invalid response."""

def test_find_optimal_context_simple((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test finding optimal context with simple scenario."""

def mock_completion((llm, prompt, **kwargs)):

def test_save_test_log((self, tmp_path)) -> None:
    """Test saving test log to file."""

def test_optimize_model_integration((
        self,
        mock_lmstudio_client,
        mock_llm,
        tmp_path,
    )) -> None:
    """Test full model optimization workflow."""

def mock_completion((llm, prompt, **kwargs)):

def test_optimize_model_failure((self, mock_lmstudio_client)) -> None:
    """Test model optimization when all tests fail."""

def test_binary_search_logic((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test binary search algorithm with various edge cases."""

def always_works((llm, prompt, **kwargs)):

def never_works((llm, prompt, **kwargs)) -> NoReturn:

def loads_but_fails_inference((llm, prompt, **kwargs)):

def works_up_to_2048((llm, prompt, **kwargs)):


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model

class TestInferenceResult:
    """Test InferenceResult model."""
    def test_inference_result_success((self)) -> None:
        """Test successful inference result."""
    def test_inference_result_failure((self)) -> None:
        """Test failed inference result."""
    def test_inference_result_empty_response((self)) -> None:
        """Test result with empty response is considered failed."""

class TestInferenceEngine:
    """Test InferenceEngine class."""
    def test_engine_initialization_defaults((self)) -> None:
        """Test engine initialization with defaults."""
    def test_engine_initialization_custom((self, mock_lmstudio_client)) -> None:
        """Test engine initialization with custom client and registry."""

def test_inference_result_success((self)) -> None:
    """Test successful inference result."""

def test_inference_result_failure((self)) -> None:
    """Test failed inference result."""

def test_inference_result_empty_response((self)) -> None:
    """Test result with empty response is considered failed."""

def test_engine_initialization_defaults((self)) -> None:
    """Test engine initialization with defaults."""

def test_engine_initialization_custom((self, mock_lmstudio_client)) -> None:
    """Test engine initialization with custom client and registry."""

def test_infer_model_not_found((self)) -> None:
    """Test inference with non-existent model."""

def test_infer_success((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test successful inference."""

def test_infer_with_untested_model((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test inference with model that hasn't been context tested."""

def test_infer_with_max_tokens((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test inference with custom max_tokens."""

def test_infer_load_failure((self, mock_lmstudio_client)) -> None:
    """Test inference when model fails to load."""

def test_infer_completion_failure((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test inference when completion fails."""

def test_run_inference_simple((self, mock_lmstudio_client, mock_llm)) -> None:
    """Test simple run_inference method."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
import pytest
from pydantic import ValidationError
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self)) -> None:
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self, sample_model_data)) -> None:
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self)) -> None:
        """Test model creation using field aliases."""
    def test_model_with_context_testing((self, sample_model_data)) -> None:
        """Test model with context testing information."""
    def test_model_path_validation((self)) -> None:
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self)) -> None:
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self, sample_model_data)) -> None:
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self)) -> None:
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self, tmp_registry_file)) -> None:
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)) -> None:
        """Test saving and loading models."""
    def test_registry_get_model((self, tmp_registry_file, sample_model_data)) -> None:
        """Test getting a model by ID."""
    def test_registry_list_models((self, tmp_registry_file, sample_model_data)) -> None:
        """Test listing all models."""
    def test_registry_remove_model((self, tmp_registry_file, sample_model_data)) -> None:
        """Test removing a model."""
    def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)) -> None:
        """Test saving/loading models with context test information."""
    def test_registry_json_format((self, tmp_registry_file, sample_model_data)) -> None:
        """Test that the saved JSON has the expected format."""

def test_enum_values((self)) -> None:
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self, sample_model_data)) -> None:
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self)) -> None:
    """Test model creation using field aliases."""

def test_model_with_context_testing((self, sample_model_data)) -> None:
    """Test model with context testing information."""

def test_model_path_validation((self)) -> None:
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self)) -> None:
    """Test sanitized_id method."""

def test_model_to_registry_dict((self, sample_model_data)) -> None:
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self)) -> None:
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self, tmp_registry_file)) -> None:
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)) -> None:
    """Test saving and loading models."""

def test_registry_get_model((self, tmp_registry_file, sample_model_data)) -> None:
    """Test getting a model by ID."""

def test_registry_list_models((self, tmp_registry_file, sample_model_data)) -> None:
    """Test listing all models."""

def test_registry_remove_model((self, tmp_registry_file, sample_model_data)) -> None:
    """Test removing a model."""

def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)) -> None:
    """Test saving/loading models with context test information."""

def test_registry_json_format((self, tmp_registry_file, sample_model_data)) -> None:
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)) -> None:
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)) -> None:
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)) -> None:
        """Test resolver initialization."""
    def test_find_placeholders((self)) -> None:
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)) -> None:
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)) -> None:
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)) -> None:
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)) -> None:
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)) -> None:
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)) -> None:
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)) -> None:
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)) -> None:
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)) -> None:
        """Test resolution with empty string values."""
    def test_count_tokens((self)) -> None:
        """Test token counting."""
    def test_resolve_with_special_characters((self)) -> None:
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)) -> None:
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)) -> None:
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)) -> None:
    """Test resolver initialization."""

def test_find_placeholders((self)) -> None:
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)) -> None:
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)) -> None:
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)) -> None:
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)) -> None:
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)) -> None:
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)) -> None:
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)) -> None:
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)) -> None:
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)) -> None:
    """Test resolution with empty string values."""

def test_count_tokens((self)) -> None:
    """Test token counting."""

def test_resolve_with_special_characters((self)) -> None:
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from unittest.mock import Mock, patch
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self, tmp_path)) -> None:
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self, tmp_path)) -> None:
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self, tmp_path)) -> None:
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self, tmp_path)) -> None:
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self, tmp_path)) -> None:
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self, tmp_path)) -> None:
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self, tmp_path)) -> None:
        """Test that non-model files are skipped."""

def test_scanner_initialization((self, mock_get_path, tmp_path)) -> None:
    """Test scanner initialization."""

def test_get_model_size_file((self, tmp_path)) -> None:
    """Test getting size of a single model file."""

def test_get_model_size_directory((self, tmp_path)) -> None:
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self, tmp_path)) -> None:
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self, tmp_path)) -> None:
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self, tmp_path)) -> None:
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self, tmp_path)) -> None:
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self, tmp_path)) -> None:
    """Test that non-model files are skipped."""

def test_scan_models((self, mock_get_path, tmp_path)) -> None:
    """Test scanning for models."""

def test_sync_with_registry((self, mock_get_path, tmp_path)) -> None:
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_e2e/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.cli.main import create_app
from lmstrix.core.models import Model
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.core.prompts import ResolvedPrompt
from lmstrix.core.inference import InferenceResult
from lmstrix.cli.main import CLI
from lmstrix.cli.main import main

class TestCLIIntegration:
    """Test CLI integration scenarios."""
    def test_cli_help((self, capsys)) -> None:
        """Test CLI help output."""

def mock_lmstudio_setup((self, tmp_path)):
    """Set up mock LM Studio environment."""

def test_list_models_command((
        self,
        mock_client_class,
        mock_get_path,
        mock_lmstudio_setup,
        capsys,
    )) -> None:
    """Test 'models list' command."""

def test_scan_models_command((
        self,
        mock_scanner_class,
        mock_client_class,
        mock_get_path,
        mock_lmstudio_setup,
    )) -> None:
    """Test 'models scan' command."""

def test_optimize_command((
        self,
        mock_client_class,
        mock_get_path,
        mock_lmstudio_setup,
    )) -> None:
    """Test 'optimize' command."""

def test_infer_command_missing_prompt((self, mock_get_path, mock_lmstudio_setup, capsys)) -> None:
    """Test 'infer' command with missing prompt."""

def test_infer_with_prompt_file((
        self,
        mock_engine_class,
        mock_load_prompts,
        mock_get_path,
        mock_lmstudio_setup,
        tmp_path,
    )) -> None:
    """Test 'infer' command with prompt file."""

def test_cli_help((self, capsys)) -> None:
    """Test CLI help output."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import (
    load_context,
    load_context_batch,
    merge_contexts,
)

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path)) -> None:
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path)) -> None:
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path)) -> None:
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path)) -> None:
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path)) -> None:
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path)) -> None:
        """Test loading large context file."""
    def test_load_context_batch_single((self, tmp_path)) -> None:
        """Test loading batch with single file."""
    def test_load_context_batch_multiple((self, tmp_path)) -> None:
        """Test loading batch with multiple files."""
    def test_load_context_batch_with_errors((self, tmp_path)) -> None:
        """Test batch loading continues on error."""
    def test_load_context_batch_empty((self)) -> None:
        """Test loading empty batch."""
    def test_merge_contexts_simple((self)) -> None:
        """Test merging simple contexts."""
    def test_merge_contexts_with_separator((self)) -> None:
        """Test merging with custom separator."""
    def test_merge_contexts_single((self)) -> None:
        """Test merging single context."""
    def test_merge_contexts_empty((self)) -> None:
        """Test merging empty contexts."""
    def test_merge_contexts_with_headers((self)) -> None:
        """Test that merge includes file headers."""

def test_load_context_simple((self, tmp_path)) -> None:
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path)) -> None:
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path)) -> None:
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path)) -> None:
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path)) -> None:
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path)) -> None:
    """Test loading large context file."""

def test_load_context_batch_single((self, tmp_path)) -> None:
    """Test loading batch with single file."""

def test_load_context_batch_multiple((self, tmp_path)) -> None:
    """Test loading batch with multiple files."""

def test_load_context_batch_with_errors((self, tmp_path)) -> None:
    """Test batch loading continues on error."""

def test_load_context_batch_empty((self)) -> None:
    """Test loading empty batch."""

def test_merge_contexts_simple((self)) -> None:
    """Test merging simple contexts."""

def test_merge_contexts_with_separator((self)) -> None:
    """Test merging with custom separator."""

def test_merge_contexts_single((self)) -> None:
    """Test merging single context."""

def test_merge_contexts_empty((self)) -> None:
    """Test merging empty contexts."""

def test_merge_contexts_with_headers((self)) -> None:
    """Test that merge includes file headers."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from unittest.mock import Mock, patch
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_models,
)

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self, tmp_path)) -> None:
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self, tmp_path)) -> None:
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self, tmp_path)) -> None:
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self, tmp_path)) -> None:
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self, tmp_path)) -> None:
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self, tmp_path)) -> None:
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self, tmp_path)) -> None:
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self, tmp_path)) -> None:
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self, tmp_path)) -> None:
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self, tmp_path)) -> None:
    """Test saving registry to custom path."""

def test_scan_and_update_models((self, mock_scanner_class, mock_client_class, tmp_path)) -> None:
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((
        self,
        mock_scanner_class,
        mock_client_class,
    )) -> None:
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompt_file, load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self, tmp_path)) -> None:
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self, tmp_path)) -> None:
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self, tmp_path)) -> None:
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((self, tmp_path)) -> None:
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self, tmp_path)) -> None:
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self, tmp_path)) -> None:
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self, tmp_path)) -> None:
        """Test loading prompts from empty TOML file."""
    def test_load_prompt_file_simple((self, tmp_path)) -> None:
        """Test loading a single prompt file."""
    def test_load_prompt_file_missing_template((self, tmp_path)) -> None:
        """Test loading prompt file without template field."""
    def test_load_prompt_file_with_defaults((self, tmp_path)) -> None:
        """Test loading prompt file with default values."""

def test_load_prompts_simple((self, tmp_path)) -> None:
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self, tmp_path)) -> None:
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self, tmp_path)) -> None:
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((self, tmp_path)) -> None:
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self, tmp_path)) -> None:
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self, tmp_path)) -> None:
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self, tmp_path)) -> None:
    """Test loading prompts from empty TOML file."""

def test_load_prompt_file_simple((self, tmp_path)) -> None:
    """Test loading a single prompt file."""

def test_load_prompt_file_missing_template((self, tmp_path)) -> None:
    """Test loading prompt file without template field."""

def test_load_prompt_file_with_defaults((self, tmp_path)) -> None:
    """Test loading prompt file with default values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self, tmp_path)) -> None:
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((self, tmp_path)) -> None:
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self, tmp_path)) -> None:
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self, tmp_path)) -> None:
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self, tmp_path)) -> None:
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self, tmp_path)) -> None:
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self, tmp_path)) -> None:
        """Test getting default models file path."""
    def test_get_context_tests_dir((self, tmp_path)) -> None:
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self, tmp_path)) -> None:
        """Test getting context test log path."""
    def test_get_prompts_dir((self, tmp_path)) -> None:
        """Test getting prompts directory."""
    def test_get_contexts_dir((self, tmp_path)) -> None:
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((self, tmp_path)) -> None:
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self, tmp_path)) -> None:
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((self, tmp_path)) -> None:
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self, tmp_path)) -> None:
    """Test finding LM Studio in shared location."""

def exists_side_effect((self)) -> bool:

def test_get_lmstudio_path_not_found((self, tmp_path)) -> None:
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self, tmp_path)) -> None:
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self, tmp_path)) -> None:
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self, tmp_path)) -> None:
    """Test getting default models file path."""

def test_get_context_tests_dir((self, tmp_path)) -> None:
    """Test getting context tests directory."""

def test_get_context_test_log_path((self, tmp_path)) -> None:
    """Test getting context test log path."""

def test_get_prompts_dir((self, tmp_path)) -> None:
    """Test getting prompts directory."""

def test_get_contexts_dir((self, tmp_path)) -> None:
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((self, tmp_path)) -> None:
    """Test handling permission errors when creating directories."""


# File: /Users/Shared/lmstudio/lmstrix/tests/utils/test_paths.py
# Language: python

import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock
from lmstrix.utils.paths import get_lmstudio_path, get_default_models_file

def test_get_lmstudio_path_from_pointer((mock_home)):
    """ Tests that the path is correctly read from the .lmstudio-home-pointer file...."""

def test_fallback_to_default_directories((mock_home, mock_exists)):
    """ Tests the fallback mechanism when the pointer file doesn't exist...."""

def path_exists_side_effect((path)):

def test_no_path_found_raises_error((mock_home, mock_exists)):
    """ Tests that a FileNotFoundError is raised if no LM Studio path can be found...."""

def test_get_default_models_file_path((mock_get_lmstudio_path)):
    """ Ensures the models file path is correctly constructed inside the LM Studio dir...."""


</documents>