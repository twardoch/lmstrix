Project Structure:
📁 lmstrix
├── 📁 .github
│   ├── 📁 ISSUE_TEMPLATE
│   │   └── 📄 documentation.md
│   ├── 📁 workflows
│   │   ├── 📄 docs-preview.yml
│   │   └── 📄 docs.yml
│   ├── 📄 pull_request_template.md
│   └── 📄 README.md
├── 📁 _keep_this
│   └── 📁 adam
│       ├── 📁 analyse
│       │   ├── 📁 0
│       │   ├── 📁 input
│       │   ├── 📁 output
│       │   ├── 📁 output-all
│       │   ├── 📁 output-paths
│       │   ├── 📁 review
│       │   └── 📁 untitled folder
│       └── 📁 out
├── 📁 docs
│   ├── 📄 _config.yml
│   ├── 📄 api.md
│   ├── 📄 how-it-works.md
│   ├── 📄 index.md
│   ├── 📄 installation.md
│   └── 📄 usage.md
├── 📁 examples
│   ├── 📁 cli
│   │   ├── 📄 basic_workflow.sh
│   │   ├── 📄 context_control_examples.sh
│   │   ├── 📄 inference_examples.sh
│   │   ├── 📄 model_state_demo.sh
│   │   └── 📄 model_testing.sh
│   ├── 📁 data
│   │   ├── 📄 sample_context.txt
│   │   └── 📄 test_questions.json
│   ├── 📁 prompts
│   │   ├── 📄 analysis.toml
│   │   ├── 📄 coding.toml
│   │   ├── 📄 creative.toml
│   │   └── 📄 qa.toml
│   ├── 📁 python
│   │   ├── 📄 __init__.py
│   │   ├── 📄 advanced_testing.py
│   │   ├── 📄 basic_usage.py
│   │   ├── 📄 batch_processing.py
│   │   ├── 📄 custom_inference.py
│   │   └── 📄 prompt_templates_demo.py
│   ├── 📁 specialized
│   │   └── 📄 elo_liczby.py
│   ├── 📄 prompts.toml
│   ├── 📄 README.md
│   └── 📄 run_all_examples.sh
├── 📁 issues
│   └── 📄 401.txt
├── 📁 src
│   ├── 📁 lmstrix
│   │   ├── 📁 api
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 client.py
│   │   │   ├── 📄 exceptions.py
│   │   │   └── 📄 main.py
│   │   ├── 📁 core
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 concrete_config.py
│   │   │   ├── 📄 context.py
│   │   │   ├── 📄 context_tester.py
│   │   │   ├── 📄 inference.py
│   │   │   ├── 📄 inference_manager.py
│   │   │   ├── 📄 models.py
│   │   │   ├── 📄 prompts.py
│   │   │   └── 📄 scanner.py
│   │   ├── 📁 loaders
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context_loader.py
│   │   │   ├── 📄 model_loader.py
│   │   │   └── 📄 prompt_loader.py
│   │   ├── 📁 utils
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context_parser.py
│   │   │   ├── 📄 logging.py
│   │   │   ├── 📄 paths.py
│   │   │   └── 📄 state.py
│   │   ├── 📄 __init__.py
│   │   ├── 📄 __main__.py
│   │   └── 📄 py.typed
│   └── 📁 lmstrix.egg-info
├── 📁 src_docs
│   ├── 📁 includes
│   │   └── 📄 mkdocs.md
│   ├── 📁 javascripts
│   │   └── 📄 mathjax.js
│   ├── 📁 md
│   │   ├── 📄 cli-interface.md
│   │   ├── 📄 configuration.md
│   │   ├── 📄 context-testing.md
│   │   ├── 📄 index.md
│   │   ├── 📄 installation.md
│   │   ├── 📄 model-management.md
│   │   ├── 📄 performance.md
│   │   ├── 📄 prompt-templating.md
│   │   ├── 📄 python-api.md
│   │   └── 📄 quickstart.md
│   ├── 📁 stylesheets
│   │   └── 📄 extra.css
│   └── 📄 mkdocs.yml
├── 📁 tests
│   ├── 📁 test_api
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_client.py
│   │   └── 📄 test_exceptions.py
│   ├── 📁 test_core
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_tester.py
│   │   ├── 📄 test_inference.py
│   │   ├── 📄 test_models.py
│   │   ├── 📄 test_prompts.py
│   │   └── 📄 test_scanner.py
│   ├── 📁 test_integration
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_cli_integration.py
│   ├── 📁 test_loaders
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_loader.py
│   │   ├── 📄 test_model_loader.py
│   │   └── 📄 test_prompt_loader.py
│   ├── 📁 test_utils
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_paths.py
│   ├── 📄 __init__.py
│   ├── 📄 conftest.py
│   └── 📄 run_tests.py
├── 📄 .gitignore
├── 📄 AGENTS.md
├── 📄 build.sh
├── 📄 CHANGELOG.md
├── 📄 CLAUDE.md
├── 📄 cleanup.txt
├── 📄 debug_inference.py
├── 📄 debug_test_logic.py
├── 📄 GEMINI.md
├── 📄 get_out_ctx.py
├── 📄 LICENSE
├── 📄 PLAN.md
├── 📄 publish.sh
├── 📄 pyproject.toml
├── 📄 README.md
├── 📄 test_model_out_ctx.py
├── 📄 test_prompt_example.txt
├── 📄 test_streaming.py
├── 📄 TESTING.md
├── 📄 TODO.md
└── 📄 WORK.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="2">
<source>.github/ISSUE_TEMPLATE/documentation.md</source>
<document_content>
---
name: Documentation Issue
about: Report issues with documentation or suggest improvements
title: '[DOCS] '
labels: documentation
assignees: ''
---

<!-- this_file: .github/ISSUE_TEMPLATE/documentation.md -->

## Documentation Issue Type
<!-- Select the type of documentation issue -->
- [ ] Error/Mistake in existing documentation
- [ ] Missing documentation
- [ ] Unclear or confusing explanation
- [ ] Broken link or reference
- [ ] Formatting/Display issue
- [ ] Accessibility issue
- [ ] Mobile/responsive issue
- [ ] Documentation suggestion/improvement

## Location
<!-- Where is the issue located? -->
**Page/Section:** (e.g., Installation Guide, Python API Reference)
**URL/File:** (e.g., docs/installation.md, specific URL)
**Line/Section:** (if applicable)

## Description
<!-- Describe the issue clearly -->

### Current Content
<!-- What does the documentation currently say? (if applicable) -->

### Expected Content
<!-- What should the documentation say instead? -->

### Suggested Fix
<!-- If you have a suggestion for how to fix this, please describe it -->

## Impact
<!-- How does this affect users? -->
- [ ] Blocks users from completing tasks
- [ ] Causes confusion but has workarounds
- [ ] Minor improvement
- [ ] Critical for accessibility/usability

## Additional Context
<!-- Add any other context, screenshots, or examples -->

## Environment (if relevant)
- Browser: 
- Device: 
- Screen reader (if applicable):

## Checklist
- [ ] I have searched existing issues to avoid duplicates
- [ ] I have provided enough information to reproduce/understand the issue
- [ ] I would be willing to help fix this issue
</document_content>
</document>

<document index="3">
<source>.github/README.md</source>
<document_content>
# .github Directory

<!-- this_file: .github/README.md -->

This directory contains GitHub workflow configurations and templates for the LMStrix documentation system.

## 📁 Directory Structure

```
.github/
├── workflows/
│   ├── docs.yml           # Main documentation build and deploy workflow
│   └── docs-preview.yml   # PR preview and quality checks
├── ISSUE_TEMPLATE/
│   └── documentation.md   # Documentation issue template
├── pull_request_template.md  # PR template
└── README.md             # This file
```

## 🔄 Workflows

### `docs.yml` - Main Documentation Workflow

**Triggers:**
- Push to `main` or `develop` branches (paths: `src_docs/**`, `src/lmstrix/**`)
- Pull requests to `main` (same paths)
- Manual workflow dispatch

**Jobs:**
1. **Build** - Builds MkDocs documentation from `src_docs/`
2. **Deploy** - Deploys to GitHub Pages (main branch only)
3. **Quality Check** - Validates links and structure
4. **Performance** - Tests build performance

**Key Features:**
- ✅ Full MkDocs Material theme support
- ✅ Git revision date plugin for last modified dates
- ✅ Minification for optimized loading
- ✅ Strict build mode to catch errors
- ✅ Automatic GitHub Pages deployment
- ✅ Link validation and structure checks
- ✅ Performance benchmarking

### `docs-preview.yml` - Documentation Preview

**Triggers:**
- Pull requests to `main` (paths: `src_docs/**`, `src/lmstrix/**`)

**Jobs:**
1. **Preview** - Builds documentation and creates preview artifact
2. **Accessibility Check** - Basic accessibility validation
3. **Mobile Preview** - Mobile responsiveness checks

**Key Features:**
- 🔍 PR comment with preview summary
- 📦 Downloadable preview artifact
- ♿ Accessibility validation
- 📱 Mobile responsiveness checks
- 📊 Build statistics and change summary

## 🛠️ Setup Instructions

### 1. Enable GitHub Pages

1. Go to your repository **Settings** → **Pages**
2. Set **Source** to "GitHub Actions"
3. The documentation will be available at: `https://[username].github.io/[repository]/`

### 2. Configure Repository Permissions

Ensure the following permissions in **Settings** → **Actions** → **General**:

- ✅ **Workflow permissions**: "Read and write permissions"
- ✅ **Allow GitHub Actions to create and approve pull requests**

### 3. Set Branch Protection (Optional)

For production repositories, consider setting up branch protection:

1. **Settings** → **Branches** → **Add rule**
2. Branch pattern: `main`
3. ✅ **Require status checks to pass**
4. ✅ **Require branches to be up to date**
5. Select: `build`, `quality-check`, `preview`

## 📚 Documentation Workflow

### For Contributors

1. **Make Changes**: Edit files in `src_docs/md/`
2. **Test Locally**: 
   ```bash
   cd src_docs
   mkdocs serve
   ```
3. **Create PR**: Push changes and create pull request
4. **Review Preview**: Check the PR comment for preview link
5. **Address Feedback**: Fix any issues found by quality checks

### For Maintainers

1. **Review PR**: Check preview and quality check results
2. **Merge**: Merge to main triggers automatic deployment
3. **Verify**: Check deployed documentation at GitHub Pages URL

## 🔧 Customization

### Workflow Customization

**Add new quality checks** in `docs.yml`:
```yaml
- name: Custom Check
  run: |
    cd src_docs
    # Your custom validation script
```

**Modify build dependencies** in both workflows:
```yaml
- name: Install dependencies
  run: |
    pip install mkdocs mkdocs-material
    pip install your-additional-plugin
```

**Change trigger paths**:
```yaml
on:
  push:
    paths:
      - 'src_docs/**'
      - 'your-custom-path/**'
```

### Template Customization

**Documentation Issue Template**: Edit `ISSUE_TEMPLATE/documentation.md`
**PR Template**: Edit `pull_request_template.md`

### GitHub Pages Configuration

**Custom domain**: Add `CNAME` file to `docs/` directory
**Custom 404 page**: Add `404.html` to `src_docs/md/`

## 🚨 Troubleshooting

### Common Issues

**Build Fails**:
- Check MkDocs configuration syntax
- Verify all referenced files exist
- Review workflow logs for specific errors

**Pages Not Deploying**:
- Ensure GitHub Pages is enabled
- Check workflow permissions
- Verify `docs/` directory contains built files

**Quality Checks Fail**:
- Fix broken internal links
- Ensure all required files exist
- Check navigation structure in `mkdocs.yml`

**Preview Not Available**:
- Check if workflow completed successfully
- Download artifact manually from workflow run
- Verify PR targets the correct branch

### Debug Commands

```bash
# Test locally
cd src_docs
mkdocs build --verbose --strict

# Check configuration
python -c "import yaml; yaml.safe_load(open('mkdocs.yml'))"

# Validate links manually
grep -r "\]\(" md/ | grep -v "http"
```

## 📞 Support

If you encounter issues with these workflows:

1. Check the [GitHub Actions documentation](https://docs.github.com/en/actions)
2. Review workflow run logs for specific errors
3. Create an issue using the documentation template
4. Check MkDocs and Material theme documentation

## 🔄 Workflow Updates

This workflow configuration is designed to be:
- **Maintainable**: Clear structure and documentation
- **Reliable**: Multiple quality checks and validations
- **Efficient**: Caching and optimized builds
- **Accessible**: Accessibility and mobile checks
- **User-friendly**: Clear preview and feedback

When updating workflows, please:
1. Test changes on a feature branch first
2. Update this README with any new features
3. Maintain backward compatibility when possible
4. Document any breaking changes
</document_content>
</document>

<document index="4">
<source>.github/pull_request_template.md</source>
<document_content>
<!-- this_file: .github/pull_request_template.md -->

## Description
<!-- Provide a brief description of the changes -->

## Type of Change
<!-- Mark the relevant option -->
- [ ] Documentation update/fix
- [ ] New documentation section
- [ ] Code changes affecting documentation
- [ ] Configuration changes
- [ ] Workflow/CI improvements

## Documentation Changes
<!-- If this PR affects documentation, please describe: -->

### Pages Modified
<!-- List the documentation pages that were changed -->
- [ ] Installation Guide
- [ ] Quick Start
- [ ] Configuration
- [ ] CLI Interface
- [ ] Python API
- [ ] Context Testing
- [ ] Model Management
- [ ] Prompt Templating
- [ ] Performance & Optimization
- [ ] Other: ___________

### Change Summary
<!-- Summarize what was changed in the documentation -->

## Testing
<!-- How have you tested these changes? -->
- [ ] Built documentation locally (`mkdocs serve`)
- [ ] Verified all links work
- [ ] Checked formatting and rendering
- [ ] Tested on mobile/responsive design
- [ ] Validated navigation structure
- [ ] Spell-checked content

## Preview
<!-- If applicable, provide screenshots or describe how to preview changes -->

## Checklist
<!-- Please check all that apply -->
- [ ] Documentation builds without errors
- [ ] All internal links are working
- [ ] Code examples are tested and working
- [ ] Content follows the existing style and tone
- [ ] Navigation is updated (if needed)
- [ ] No spelling or grammar errors
- [ ] Images have appropriate alt text (if applicable)
- [ ] Content is accessible and mobile-friendly

## Additional Notes
<!-- Any additional information, context, or considerations -->

## Related Issues
<!-- Link any related issues -->
Fixes #
Relates to #
</document_content>
</document>

<document index="5">
<source>.github/workflows/docs-preview.yml</source>
<document_content>
# this_file: .github/workflows/docs-preview.yml

name: Documentation Preview

on:
  pull_request:
    branches: [ main ]
    paths:
      - 'src_docs/**'
      - 'src/lmstrix/**'
      - '.github/workflows/docs-preview.yml'

jobs:
  preview:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mkdocs
        pip install mkdocs-material
        pip install mkdocs-minify-plugin
        pip install mkdocs-git-revision-date-localized-plugin
        pip install pymdown-extensions
    
    - name: Build documentation preview
      run: |
        cd src_docs
        mkdocs build --verbose
        echo "✅ Documentation preview built successfully"
    
    - name: Generate preview summary
      run: |
        cd src_docs
        echo "📊 Documentation Preview Summary" > preview_summary.md
        echo "=====================================" >> preview_summary.md
        echo "" >> preview_summary.md
        echo "**Built from:** \`${{ github.head_ref }}\`" >> preview_summary.md
        echo "**Commit:** \`${{ github.sha }}\`" >> preview_summary.md
        echo "**Author:** ${{ github.actor }}" >> preview_summary.md
        echo "" >> preview_summary.md
        echo "### 📈 Build Statistics" >> preview_summary.md
        echo "- **Total pages:** $(find ../docs -name "*.html" | wc -l)" >> preview_summary.md
        echo "- **Build time:** $(date)" >> preview_summary.md
        echo "- **Size:** $(du -sh ../docs | cut -f1)" >> preview_summary.md
        echo "" >> preview_summary.md
        echo "### 📝 Changed Files" >> preview_summary.md
        git diff --name-only origin/main...HEAD | grep -E '\.(md|yml)$' | while read file; do
          echo "- \`$file\`" >> preview_summary.md
        done
        echo "" >> preview_summary.md
        echo "### 🔗 Navigation Structure" >> preview_summary.md
        echo "\`\`\`" >> preview_summary.md
        python -c "
        import yaml
        with open('mkdocs.yml') as f:
            config = yaml.safe_load(f)
        nav = config.get('nav', [])
        def print_nav(items, indent=0):
            for item in items:
                if isinstance(item, dict):
                    for key, value in item.items():
                        print('  ' * indent + f'- {key}')
                        if isinstance(value, list):
                            print_nav(value, indent + 1)
                        elif isinstance(value, str):
                            print('  ' * (indent + 1) + f'→ {value}')
                else:
                    print('  ' * indent + f'- {item}')
        print_nav(nav)
        " >> preview_summary.md
        echo "\`\`\`" >> preview_summary.md
    
    - name: Upload preview artifact
      uses: actions/upload-artifact@v3
      with:
        name: documentation-preview
        path: |
          docs/
          src_docs/preview_summary.md
        retention-days: 7
    
    - name: Comment PR with preview info
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('src_docs/preview_summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## 📖 Documentation Preview Ready!
            
            The documentation has been built successfully from this PR.
            
            ${summary}
            
            📦 **Download Preview:** Check the "documentation-preview" artifact in the workflow run.
            
            > 💡 To test locally: \`cd src_docs && mkdocs serve\``
          });

  accessibility-check:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mkdocs mkdocs-material
        pip install accessibility-checker || echo "accessibility-checker not available"
    
    - name: Build documentation
      run: |
        cd src_docs
        mkdocs build
    
    - name: Basic accessibility check
      run: |
        cd docs
        echo "🔍 Running basic accessibility checks..."
        
        # Check for basic accessibility issues
        echo "Checking for missing alt text..."
        if grep -r "img.*src" . | grep -v "alt="; then
          echo "⚠️ Found images without alt text"
        else
          echo "✅ All images have alt text"
        fi
        
        echo "Checking for proper heading structure..."
        for file in $(find . -name "*.html"); do
          h1_count=$(grep -o "<h1" "$file" | wc -l)
          if [ "$h1_count" -gt 1 ]; then
            echo "⚠️ Multiple H1 tags found in $file"
          fi
        done
        
        echo "Checking for color contrast indicators..."
        if grep -r "color.*#" . | head -5; then
          echo "ℹ️ Found color definitions (manual review recommended)"
        fi
        
        echo "✅ Basic accessibility check completed"

  mobile-preview:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mkdocs mkdocs-material
    
    - name: Build documentation
      run: |
        cd src_docs
        mkdocs build
    
    - name: Check mobile responsiveness indicators
      run: |
        cd docs
        echo "📱 Checking mobile responsiveness indicators..."
        
        # Check for viewport meta tag
        if grep -r "viewport" . | head -5; then
          echo "✅ Viewport meta tags found"
        else
          echo "⚠️ No viewport meta tags detected"
        fi
        
        # Check for responsive CSS indicators
        if grep -r "media.*screen" . | head -5; then
          echo "✅ Responsive media queries found"
        else
          echo "ℹ️ No obvious media queries detected"
        fi
        
        # Check Material theme responsive features
        if grep -r "md-nav" . | head -3; then
          echo "✅ Material theme navigation components found"
        fi
        
        echo "📊 Mobile check completed"
</document_content>
</document>

<document index="6">
<source>.github/workflows/docs.yml</source>
<document_content>
# this_file: .github/workflows/docs.yml

name: Build and Deploy Documentation

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src_docs/**'
      - 'src/lmstrix/**'
      - '.github/workflows/docs.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src_docs/**'
      - 'src/lmstrix/**'
      - '.github/workflows/docs.yml'
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these production deployments to complete.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch all history for proper git-revision-date-localized plugin
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mkdocs
        pip install mkdocs-material
        pip install mkdocs-minify-plugin
        pip install mkdocs-git-revision-date-localized-plugin
        pip install mkdocs-git-committers-plugin-2
        pip install pymdown-extensions
        pip install mkdocs-awesome-pages-plugin
        pip install mkdocs-redirects
    
    - name: Verify MkDocs configuration
      run: |
        cd src_docs
        mkdocs --version
        python -c "import yaml; yaml.safe_load(open('mkdocs.yml'))"
        echo "✅ MkDocs configuration is valid"
    
    - name: Build documentation
      run: |
        cd src_docs
        mkdocs build --verbose --strict
        echo "✅ Documentation built successfully"
    
    - name: Verify build output
      run: |
        ls -la docs/
        echo "📁 Generated files:"
        find docs/ -name "*.html" | head -10
        echo "📊 Total HTML files: $(find docs/ -name "*.html" | wc -l)"
    
    - name: Setup Pages
      if: github.ref == 'refs/heads/main'
      uses: actions/configure-pages@v3
    
    - name: Upload artifact
      if: github.ref == 'refs/heads/main'
      uses: actions/upload-pages-artifact@v2
      with:
        path: docs/

  # Deployment job (only on main branch)
  deploy:
    if: github.ref == 'refs/heads/main'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v2

  # Documentation quality checks
  quality-check:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mkdocs mkdocs-material
        pip install markdown-link-check || echo "markdown-link-check not available"
    
    - name: Check for broken internal links
      run: |
        cd src_docs
        # Create a simple link checker script
        cat << 'EOF' > check_links.py
        import os
        import re
        import sys
        from pathlib import Path
        
        def check_internal_links():
            md_dir = Path("md")
            errors = []
            
            # Get all markdown files
            md_files = list(md_dir.glob("*.md"))
            file_names = {f.stem for f in md_files}
            
            for md_file in md_files:
                content = md_file.read_text()
                
                # Find markdown links [text](link.md)
                links = re.findall(r'\[([^\]]+)\]\(([^)]+\.md)\)', content)
                
                for link_text, link_path in links:
                    # Remove .md extension and check if file exists
                    target_file = link_path.replace('.md', '')
                    if target_file not in file_names:
                        errors.append(f"Broken link in {md_file.name}: [{link_text}]({link_path})")
            
            return errors
        
        errors = check_internal_links()
        if errors:
            print("❌ Found broken internal links:")
            for error in errors:
                print(f"  {error}")
            sys.exit(1)
        else:
            print("✅ All internal links are valid")
        EOF
        
        python check_links.py
    
    - name: Check documentation structure
      run: |
        cd src_docs
        echo "📋 Checking documentation structure..."
        
        # Check required files exist
        required_files=(
          "mkdocs.yml"
          "md/index.md"
          "md/installation.md"
          "md/quickstart.md"
          "md/configuration.md"
          "md/cli-interface.md"
          "md/python-api.md"
          "md/context-testing.md"
          "md/model-management.md"
          "md/prompt-templating.md"
          "md/performance.md"
        )
        
        for file in "${required_files[@]}"; do
          if [[ -f "$file" ]]; then
            echo "✅ $file exists"
          else
            echo "❌ $file is missing"
            exit 1
          fi
        done
        
        echo "📊 Documentation statistics:"
        echo "  Total markdown files: $(find md/ -name "*.md" | wc -l)"
        echo "  Total words: $(cat md/*.md | wc -w)"
        echo "  Total lines: $(cat md/*.md | wc -l)"
    
    - name: Validate MkDocs navigation
      run: |
        cd src_docs
        python -c "
        import yaml
        with open('mkdocs.yml') as f:
            config = yaml.safe_load(f)
        
        nav = config.get('nav', [])
        print('📖 Navigation structure:')
        
        def print_nav(items, indent=0):
            for item in items:
                if isinstance(item, dict):
                    for key, value in item.items():
                        print('  ' * indent + f'- {key}')
                        if isinstance(value, list):
                            print_nav(value, indent + 1)
                        elif isinstance(value, str):
                            print('  ' * (indent + 1) + f'→ {value}')
                else:
                    print('  ' * indent + f'- {item}')
        
        print_nav(nav)
        print('✅ Navigation structure is valid')
        "

  # Performance test (build speed)
  performance:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mkdocs mkdocs-material mkdocs-minify-plugin
    
    - name: Benchmark build performance
      run: |
        cd src_docs
        echo "⏱️ Testing documentation build performance..."
        
        # Warm-up build
        time mkdocs build --quiet
        
        # Timed builds
        echo "🏃 Running 3 timed builds..."
        for i in {1..3}; do
          rm -rf ../docs/*
          echo "Build $i:"
          time mkdocs build --quiet
        done
        
        echo "✅ Performance test completed"
</document_content>
</document>

<document index="7">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
_keep_this/adam
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
build/
celerybeat-schedule
celerybeat.pid
CLEANUP.txt
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
llms.txt
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
src/lmstrix/_version.py
target/
Thumbs.db
uv.lock
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="8">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="9">
<source>AGENTS.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="10">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

## [1.0.66] - 2025-08-05

### Fixed
- **Issue #302**: Inference Output Mismatch Investigation - Major Progress
  - Added comprehensive diagnostic logging for all inference parameters
  - Updated default temperature from 0.7 to 0.8 to match LM Studio GUI defaults
  - Added CLI parameters for configuring inference settings (top_k, temperature, etc.)
  - Fixed Rich table display to use full console width with `expand=True`
  - Enhanced response preview in test command from 20 to 60 characters for better readability
  - Created diagnostic tools: `debug_inference.py`, `debug_test_logic.py`, `get_out_ctx.py`

### Enhanced
- **Table Display Improvements**
  - All Rich tables now use full console width (9 tables updated)
  - Removed fixed width parameters for better terminal utilization
  - Improved response preview formatting with better truncation and cleanup

### Added
- **Diagnostic and Debug Tools**
  - `debug_inference.py`: Tool for debugging inference parameter differences
  - `debug_test_logic.py`: Tool for testing context logic
  - `get_out_ctx.py`: Utility for analyzing output context calculations
  - `test_model_out_ctx.py`: Testing script for model context handling

### Testing
- **Test Suite Stability Improvements**
  - Fixed all critical AttributeError issues in test suite
  - Resolved Model.validate_integrity() method errors
  - Fixed PromptResolver and ContextTester method access issues
  - Improved ModelScanner.sync_with_registry() reliability

### Added
- **Compact Output for Test Command** ✅ COMPLETED
  - Added live-updating rich table display for non-verbose test output
  - Shows model ID, context size, and test status in a clean format
  - Progress tracking for batch tests with `--all` flag
  - Maintains detailed logging in verbose mode for debugging
  - Significantly improves readability when testing multiple models
- **Custom Prompt Support for Context Testing** ✅ COMPLETED
  - Added `--prompt` parameter to test command for using custom prompt strings
  - Added `--file_prompt` parameter to test command for loading prompts from files
  - Modified ContextTester and InferenceEngine to accept custom prompts
  - Custom prompts replace the default dual-test prompts (96 digits + 2+3=5)
  - Updated help documentation with examples of custom prompt usage
  - Created test_prompt_example.txt as a demonstration file
- **Issue #307**: Streaming Inference Support ✅ COMPLETED
  - Added `stream_completion()` method to LMStudioClient using lmstudio SDK's `complete_stream()`
  - Implemented token-by-token streaming with callbacks and progress monitoring
  - Added `stream_infer()` methods to InferenceEngine and InferenceManager
  - Added CLI `--stream` flag for real-time token display during inference
  - Added `--stream-timeout` parameter (default 120s) for hang detection
  - Streaming statistics: tokens/second, time to first token
  - Full backward compatibility maintained with existing sync inference
- **Issue #306**: Batch Processing Tool ✅ COMPLETED (`_keep_this/adam/adamall.py`)
  - Processes multiple prompts across all models automatically
  - Smart model management - reuses loaded models when possible
  - Skips existing outputs for resumable batch processing
  - Safe filename generation using pathvalidate
  - Error capture to output files on failure
  - Progress tracking with percentage completion
  - Processes prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`
  - Loads models with 50% context, runs inference with 90% max context
  - Intelligent model sorting by size for optimal processing
- Created `src/lmstrix/api/main.py` with `LMStrixService` class containing all business logic
- Implemented separation of concerns with thin CLI wrapper in `__main__.py`

### Changed
- Default temperature changed from 0.7 to 0.8 to match LM Studio GUI defaults
- Default `out_ctx` now falls back to the model's `ctx_out − 1` when callers leave it as `-1`. This avoids LM Studio SDK hangs triggered by unlimited generations.
- Refactored CLI architecture - moved all business logic from `__main__.py` to `api/main.py`
- Updated `__main__.py` to be a thin wrapper that delegates to `LMStrixService`
- Modified test imports to use new module structure

### Fixed
- **Issue #303**: Fixed loguru output interference with model responses
  - Removed loguru formatting of config dictionaries and raw model responses
  - Changed problematic log messages in `api/client.py` lines 265 and 267 to simple status messages
  - Prevented KeyError and ValueError exceptions when loguru tried to parse model output
  - Ensures clean separation of diagnostic output (stderr) from model output (stdout)
- Fixed import error after removing `src/lmstrix/cli` directory
- Updated `pyproject.toml` entry point from `lmstrix.cli.main:main` to `lmstrix.__main__:main`

### Removed
- Removed `src/lmstrix/cli` directory (merged functionality into `__main__.py`)

## [1.0.59] - 2025-07-31

### Major Improvements & Bug Fixes

#### Issues 201-204 (Completed)
- **Issue 201**: Enhanced model persistence - models now stay loaded between inference calls when no explicit context is specified
- **Issue 202**: Beautiful enhanced logging with emojis, model info, config details, and prompt statistics  
- **Issue 203**: Fixed model lookup to find by both path and ID without changing JSON structure
- **Issue 204**: Added comprehensive verbose stats logging including time to first token, predicted tokens, tokens/second, and total inference time

#### Model Registry Improvements
- Fixed smart model lookup that works with both model paths and IDs
- Preserved original JSON structure keyed by path
- No data duplication - registry size maintained
- Backward compatible with existing path-based lookups

#### Enhanced Logging & Statistics
- Beautiful formatted logging with visual separators and emojis
- Complete inference statistics display including:
  - ⚡ Time to first token
  - ⏱️ Total inference time  
  - 🔢 Predicted tokens
  - 📝 Prompt tokens
  - 🎯 Total tokens
  - 🚀 Tokens/second
  - 🛑 Stop reason
- Eliminated duplicate stats display at end of output

### Added

- **Context Parameter Percentage Support**
  - `--out_ctx` parameter now supports percentage notation (e.g., "80%")
  - Created `utils/context_parser.py` for parsing context parameters
  - Percentage calculated from model's tested or declared maximum context

- **Improved CLI Output**
  - Non-verbose mode for `infer` command now shows only model response
  - Removed extra formatting and status information in quiet mode

- **Prompt File Support with TOML (Issue #104)**
  - Added `--file_prompt` parameter to load prompts from TOML files
  - Added `--dict` parameter for passing key=value pairs for placeholder resolution
  - When using `--file_prompt`, the prompt parameter refers to the prompt name in the TOML file
  - Supports nested placeholders and internal template references
  - Reports unresolved placeholders in verbose mode
  - Includes comprehensive example prompt file with various use cases

- **Enhanced Infer Context Control (Issue #103)**
  - Added `--in_ctx` parameter to control model loading context size
  - Added `--out_ctx` parameter to replace deprecated `--max_tokens`
  - Supports `--in_ctx 0` to load model without context specification
  - When `--in_ctx` is not specified, uses optimal context (tested or declared)
  - Explicit `--in_ctx` always unloads existing models and reloads with specified context
  - Smart unloading: only unloads models that were explicitly loaded with `--in_ctx`

- **Smart Model Loading**
  - Added model state detection to check if models are already loaded
  - Reuses existing loaded models when no explicit context specified
  - Added `--force-reload` flag to force model reload even if already loaded
  - Shows clear status messages about model reuse vs reload

- **Model State Persistence (Issue #201)**
  - Models now stay loaded between `infer` calls when `--in_ctx` is not specified
  - Added StateManager to track last-used model across sessions
  - Model ID parameter (`-m`) is now optional - uses last-used model when omitted
  - Significantly improves performance by avoiding repeated model loading/unloading
  - Created `examples/cli/model_state_demo.sh` demonstrating the feature

- **Context Validation**
  - Validates requested context against model's declared and tested limits
  - Warns when context exceeds safe limits
  - Suggests optimal context values

### Changed

- **Inference Command**
  - Deprecated `--max_tokens` in favor of `--out_ctx` (backward compatible with warnings)
  - Updated help text and documentation for new parameters
  - Improved model loading logic for better memory management
  - Enhanced status messages during inference operations

## [1.0.53] - 2025-07-29

### Fixed

- **Git Configuration**
  - Fixed git pull error by setting upstream branch tracking with `git branch --set-upstream-to=origin/main main`
  - Resolved "exit code(1)" error when running `git pull -v -- origin` without branch specification

### Changed

- **Version Maintenance**
  - Updated to version 1.0.53 with proper git configuration fixes

## [1.0.28 - 1.0.52] - 2025-07-25 to 2025-07-29

### Added

- **Enhanced CLI Features**
  - Added `--sort` option to `lmstrix test --all` command with same sort options as list (id, ctx, dtx, size, etc.)
  - Added `--ctx` option to `lmstrix test --all` for testing all untested models at a specific context size
  - Added `--show` option to `lmstrix list` with three output formats:
    - `id`: Plain newline-delimited list of model IDs
    - `path`: Newline-delimited list of relative paths (same as id currently)
    - `json`: Full JSON array from the registry
  - All `--show` formats respect the `--sort` option for flexible output

- **CLI Improvements**
  - Modified `--ctx` to work with `--all` flag for batch testing at specific context sizes
  - `test --all --ctx` filters models based on context limits and safety checks
  - Added proper status updates and persistence when using `--ctx` for single model tests
  - Fixed model field updates (tested_max_context, last_known_good_context) during --ctx testing

### Changed

- **Removed all asyncio dependencies (Issue #204)**
  - Converted entire codebase from async to synchronous
  - Now uses the native synchronous `lmstudio` package API directly
  - Simplified architecture by removing async/await complexity
  - Implemented signal-based timeout for Unix systems
  - All methods now return results directly without await

### Added

- **Context Size Safety Validation**
  - Added validation to prevent testing at or above `last_known_bad_context`
  - CLI `--ctx` parameter now checks against `last_known_bad_context` and limits to 90% of last bad
  - Automatic testing algorithms now respect `last_known_bad_context` during iterations
  - Added warning messages when context size approaches 80% or 90% of last known bad context
  - Prevents system crashes by avoiding previously failed context sizes

- **Enhanced Context Testing Strategy (Issue #201)**
  - Added `--threshold` parameter to test command (default: 102,400 tokens)
  - Prevents system crashes by limiting initial test size
  - New incremental testing algorithm: test at 1024, then threshold, then increment by 10,240
  - Optimized batch testing for `--all` flag with pass-based approach
  - Models sorted by declared context size to minimize loading/unloading
  - Rich table output showing test results with efficiency percentages

- **Smart Context Testing with Progress Saving**
  - Context tests now start with small context (32) to verify model loads
  - Added fields to track last known good/bad context sizes
  - Tests can resume from previous state if interrupted
  - Progress is saved to JSON after each test iteration
  - Changed test prompt from "2+2=" to "Say hello" for better reliability

### Fixed

- **Terminology Improvements**
  - Changed "Loaded X models" to "Read X models" to avoid confusion with LM Studio's model loading
  - Replaced generic "Check logs for details" with specific error messages

- **Context Testing Stability**
  - Added delays between model load/unload operations to prevent rapid cycling
  - Fixed connection reset issues caused by too-rapid operations
  - Enhanced binary search logging to show progress clearly

### Changed

- **Model Data Structure**
  - Added `last_known_good_context` field for resumable testing
  - Added `last_known_bad_context` field for resumable testing
  - Updated registry serialization to include new fields

## [1.0.28] - 2025-07-25

### Added

- **GitHub Pages Documentation Site**
  - Created comprehensive documentation site structure under `docs/`
  - Added Jekyll configuration with custom theme and navigation
  - Created documentation pages: installation, usage, API reference, how-it-works
  - Set up automatic changelog integration with documentation
  - Added responsive design and syntax highlighting

- **Example Improvements**
  - Added example output logging to `examples.log.txt` and `examples.err.txt`
  - Enhanced error handling in example scripts
  - Added more detailed comments in Python examples

### Fixed

- **Client Compatibility**
  - Fixed attribute access issues in `api/client.py` for embedding models
  - Added proper handling for different model types (LLMs vs Embeddings)
  - Improved error messages for unsupported model types

- **Context Testing Robustness**
  - Enhanced context size detection with better error handling
  - Improved inference validation logic
  - Added fallback mechanisms for edge cases

### Changed

- **CLI Enhancements**
  - Improved output formatting for model listings
  - Better progress indicators during testing
  - More informative error messages

- **Documentation Updates**
  - Updated README with clearer examples
  - Enhanced API documentation with more details
  - Added troubleshooting section

## [1.0.23] - 2025-07-25

### Added

- Implemented comprehensive `test_binary_search_logic` test function with edge cases:
  - Model that works at all sizes
  - Model that never works  
  - Model that loads but never passes inference
  - Model that works up to a specific context size
  - Model that never loads

### Fixed

- Fixed syntax errors in Python example files:
  - `examples/python/batch_processing.py` - Fixed unterminated string literals
  - `examples/python/custom_inference.py` - Fixed similar syntax issues
- Improved code quality by fixing linting issues:
  - Updated file operations to use `Path.open()` instead of `open()` (PTH123)
  - Added proper exception chaining with `from e` (B904)
  - Fixed shadowing of Python builtin `all` by renaming to `all_models` (A002)
- Updated `api/client.py` to handle different attribute names for LLMs vs Embeddings

### Changed

- All mypy type checking now passes without errors
- Package builds successfully as v1.0.23

## [1.0.21] - 2025-07-25

### Fixed

- Code formatting improvements in test files for better readability
- Updated `cleanup.sh` script
- Reduced size of `llms.txt` file significantly for better performance

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` package’s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="11">
<source>CLAUDE.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1.  **API Layer** (`src/lmstrix/api/`)
    -   `client.py`: Async client for LM Studio server API with retry logic using `tenacity`.
    -   `exceptions.py`: Custom exception hierarchy for better error handling.

2.  **Core Engine** (`src/lmstrix/core/`)
    -   `context_tester.py`: Binary search algorithm to find optimal context window size, with `rich` progress bar integration.
    -   `inference.py`: Handles the inference process, including prompt building.
    -   `models.py`: Model registry with persistence for tracking tested context limits.
    -   `scanner.py`: Discovers and catalogs available LM Studio models.
    -   `prompts.py`: Prompt resolution and template management.
    -   `context.py`: Manages context, including prompt templates and token counting using `tiktoken`.

3.  **Loaders** (`src/lmstrix/loaders/`)
    -   `model_loader.py`: Manages model registry persistence (JSON).
    -   `prompt_loader.py`: Loads prompt templates from TOML files.
    -   `context_loader.py`: Loads context data from text files.

4.  **CLI** (`src/lmstrix/cli/`)
    -   `main.py`: `fire`-based CLI with commands: `scan`, `list`, `test`, `infer`.
    -   Uses `rich` for beautiful terminal output.

### 2.2. Critical Design Patterns

-   **Async-First**: All API operations use `async/await` for high performance.
-   **Retry Logic**: Uses `tenacity` for automatic retries with exponential backoff.
-   **Model Registry**: Persists discovered models and their tested limits to JSON.
-   **Two-Phase Prompts**: Separates prompt template structure from runtime context.
-   **Binary Search**: Efficiently finds maximum context window through targeted testing.

### 2.3. Dependencies

-   `lmstudio-python`: Official LM Studio Python SDK.
-   `httpx`: Async HTTP client.
-   `pydantic`: Data validation and models.
-   `fire`: CLI framework.
-   `rich`: Terminal formatting.
-   `tenacity`: Retry logic.
-   `tiktoken`: Token counting.
-   `loguru`: Logging.

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="12">
<source>GEMINI.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="13">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="14">
<source>PLAN.md</source>
<document_content>
# LMStrix Current Development Plan


## ACTIVE: Issue #302 - Inference Output Mismatch (Major Progress Made)

### Current Status
**Major diagnostic improvements completed in v1.0.66:**
- ✅ Added comprehensive diagnostic logging for all inference parameters
- ✅ Updated default temperature from 0.7 to 0.8 to match LM Studio GUI
- ✅ Added CLI parameters for configuring inference settings
- ✅ Created diagnostic tools for parameter comparison
- ✅ Fixed table display and response preview issues

### Remaining Issues to Investigate
1. **Context Length**: GUI uses full 131072, CLI may reduce to 65536
2. **max_predict**: GUI uses -1 (unlimited), CLI calculates specific values
3. **Stop Token Configuration**: May differ between GUI and CLI
4. **Prompt Formatting**: Possible differences in chat template application

## Current Top Priority

### Issue #302 - Inference Output Mismatch (FINAL PHASE)
**Priority: HIGH - Core diagnostic work completed**

#### Remaining Implementation Steps:

##### Step 1: Context Length Investigation ⚠️
- Fix context reduction from full model context to reduced values
- Use full model context by default unless explicitly limited
- Add warning when context is automatically reduced
- Test with full context vs reduced context

##### Step 2: Stop Token Configuration ⚠️
- Compare stop token configuration between GUI and CLI
- Test with stop tokens disabled to rule out early termination
- Verify chat template application doesn't introduce stop tokens

##### Step 3: Final Validation ⚠️
- Run side-by-side comparison with identical parameters
- Verify token counts match between lmstrix and LM Studio
- Create regression test to prevent future issues

### Issue #105 - Adam.toml Simplification (Next Priority)
**Priority: Medium** (after Issue #302 is resolved)
- Simplify adam.toml structure to use flat format instead of nested groups
- Add --text and --text_file parameters to infer command for direct text input
- Update all prompt examples to use simplified approach
- Ensure backward compatibility with existing TOML files


## Future Development Phases

### Phase A: Core Simplification (2-3 weeks)
1. **Configuration Unification**
   - Create utils/config.py for centralized configuration handling
   - Consolidate path handling functions
   - Remove redundant configuration code

2. **Error Handling Standardization**
   - Review and simplify custom exception hierarchy
   - Standardize error messages across codebase
   - Implement consistent logging patterns

### Phase B: CLI Enhancement (1-2 weeks)
1. **Command Improvements**
   - Enhance `scan` command with better progress reporting
   - Improve `list` command with filtering and sorting options
   - Add `reset` command for clearing model test data

2. **User Experience**
   - Better error messages with helpful suggestions
   - Improved help text and documentation
   - Enhanced progress indicators for long-running operations

### Phase C: Testing & Documentation (1 week)
1. **Test Suite Completion**
   - Ensure >90% test coverage maintained
   - Add integration tests for new features
   - Performance benchmarking of improvements

2. **Documentation Updates**
   - Update README.md with latest features
   - Create comprehensive CLI reference
   - Update examples to demonstrate new capabilities

## Success Metrics

- **Functionality**: All existing CLI commands work without regression
- **Performance**: Model loading and inference speed improvements
- **Usability**: Cleaner, more informative user interface
- **Maintainability**: Reduced complexity, better code organization
- **Documentation**: Up-to-date and comprehensive user guides


## Long-term Vision

The goal is to make LMStrix the most user-friendly and efficient tool for managing and testing LM Studio models, with:
- Real-time streaming inference with progress feedback and hang detection
- Intuitive CLI interface with beautiful, informative output
- Smart model management with automatic optimization
- Batch processing capabilities for productivity workflows
- Comprehensive testing capabilities with clear results
- Excellent developer experience with clean, well-documented code
</document_content>
</document>

<document index="15">
<source>README.md</source>
<document_content>
# LMStrix

LMStrix is a professional Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and Python API for managing, testing, and running local language models, with a standout feature: **Adaptive Context Optimization**.

## Key Features

- **🔍 Automatic Context Discovery**: Binary search algorithm to find the true operational context limit of any model
- **📊 Beautiful Verbose Logging**: Enhanced stats display with emojis showing inference metrics, timing, and token usage
- **🚀 Smart Model Management**: Models persist between calls to reduce loading overhead
- **🎯 Flexible Inference Engine**: Run inference with powerful prompt templating and percentage-based output control
- **📋 Comprehensive Model Registry**: Track models, their context limits, and test results with JSON persistence
- **🛡️ Safety Controls**: Configurable thresholds and fail-safes to prevent system crashes
- **💻 Rich CLI Interface**: Beautiful terminal output with progress indicators and formatted tables
- **📈 Compact Test Output**: Live-updating tables show test progress without verbose logging clutter

## Installation

```bash
# Using pip
pip install lmstrix

# Using uv (recommended)
uv pip install lmstrix
```

## Quick Start

### Command-Line Interface

```bash
# Scan for available models in LM Studio
lmstrix scan

# List all models with their context limits and test status
lmstrix list

# Test context limit for a specific model
lmstrix test llama-3.2-3b-instruct

# Test all untested models with safety threshold
lmstrix test --all --threshold 102400

# Run inference with enhanced verbose logging
lmstrix infer "What is the capital of Poland?" -m llama-3.2-3b-instruct --verbose

# Run inference with percentage-based output tokens
lmstrix infer "Explain quantum computing" -m llama-3.2-3b-instruct --out_ctx "25%"

# Use file-based prompts with templates
lmstrix infer summary -m llama-3.2-3b-instruct --file_prompt adam.toml --text_file document.txt

# Direct text input for prompts
lmstrix infer "Summarize: {{text}}" -m llama-3.2-3b-instruct --text "Your content here"
```

### Enhanced Verbose Output

When using `--verbose`, LMStrix provides comprehensive statistics:

```
════════════════════════════════════════════════════════════
🤖 MODEL: llama-3.2-3b-instruct
🔧 CONFIG: maxTokens=26214, temperature=0.7
📝 PROMPT (1 lines, 18 chars): Capital of Poland?
════════════════════════════════════════════════════════════
⠸ Running inference...
════════════════════════════════════════════════════════════
📊 INFERENCE STATS
⚡ Time to first token: 0.82s
⏱️  Total inference time: 11.66s
🔢 Predicted tokens: 338
📝 Prompt tokens: 5
🎯 Total tokens: 343
🚀 Tokens/second: 32.04
🛑 Stop reason: eosFound
════════════════════════════════════════════════════════════
```

### Python API

```python
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.core.inference_manager import InferenceManager

# Load model registry
registry = load_model_registry()

# List available models
models = registry.list_models()
print(f"Available models: {len(models)}")

# Run inference
manager = InferenceManager(verbose=True)
result = manager.infer(
    model_id="llama-3.2-3b-instruct",
    prompt="What is the meaning of life?",
    out_ctx=100,
    temperature=0.7
)

if result["succeeded"]:
    print(f"Response: {result['response']}")
    print(f"Tokens used: {result['tokens_used']}")
    print(f"Time: {result['inference_time']:.2f}s")
```

## Context Testing & Optimization

LMStrix uses a sophisticated binary search algorithm to discover true model context limits:

### Safety Features
- **Threshold Protection**: Configurable maximum context size to prevent system crashes
- **Progressive Testing**: Starts with small contexts and increases safely
- **Persistent Results**: Saves test results to avoid re-testing

### Testing Commands
```bash
# Test specific model
lmstrix test llama-3.2-3b-instruct

# Test all models with custom threshold
lmstrix test --all --threshold 65536

# Test at specific context size
lmstrix test --all --ctx 32768

# Reset and re-test a model
lmstrix test llama-3.2-3b-instruct --reset

# Test with custom prompt
lmstrix test llama-3.2-3b-instruct --prompt "What is 2+2?"

# Test with file-based prompt
lmstrix test llama-3.2-3b-instruct --file_prompt test_prompt.txt
```

### Compact Output (Default)
When running without `--verbose`, tests display a clean, live-updating table:
```
Model                                   Context      Status
llama-3.2-3b-instruct                   32,768      Testing...
→
Model                                   Context      Status  
llama-3.2-3b-instruct                   32,768      ✓ Success
```

For batch testing with `--all`, a progress column is added to track multiple models.

## Model Management

### Registry Commands
```bash
# Scan for new models
lmstrix scan --verbose

# List models with different sorting
lmstrix list --sort size        # Sort by size
lmstrix list --sort ctx         # Sort by tested context
lmstrix list --show json        # Export as JSON

# Check system health
lmstrix health --verbose
```

### Model Persistence
Models stay loaded between inference calls for improved performance:
- When no explicit context is specified, models remain loaded
- Last-used model is remembered for subsequent calls
- Explicit context changes trigger model reloading

## Prompt Templating

LMStrix supports flexible prompt templating with TOML files:

```toml
# adam.toml
[aps]
prompt = """
You are an AI assistant skilled in Abstractive Proposition Segmentation.
Convert the following text: {{text}}
"""

[summary] 
prompt = "Create a comprehensive summary: {{text}}"
```

Use with CLI:
```bash
lmstrix infer aps --file_prompt adam.toml --text "Your text here"
lmstrix infer summary --file_prompt adam.toml --text_file document.txt
```

## Development

```bash
# Clone repository
git clone https://github.com/twardoch/lmstrix.git
cd lmstrix

# Install for development
pip install -e ".[dev]"

# Run tests
pytest

# Run linting
hatch run lint:all
```

## Project Structure

```
src/lmstrix/
├── cli/main.py              # CLI interface
├── core/
│   ├── inference_manager.py # Unified inference engine
│   ├── models.py            # Model registry
│   └── context_tester.py    # Context limit testing
├── api/client.py            # LM Studio API client
├── loaders/                 # Data loading utilities
└── utils/                   # Helper utilities
```

## Features in Detail

### Adaptive Context Optimizer
- Binary search algorithm for efficient context limit discovery
- Safety thresholds to prevent system crashes
- Automatic persistence of test results
- Resume capability for interrupted tests

### Enhanced Logging
- Beautiful emoji-rich output in verbose mode
- Comprehensive inference statistics
- Progress indicators for long operations
- Clear error messages with context

### Smart Model Management
- Automatic model discovery from LM Studio
- Persistent registry with JSON storage
- Model state tracking (loaded/unloaded)
- Batch operations for multiple models

## Requirements

- Python 3.11+
- LM Studio installed and configured
- Models downloaded in LM Studio

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions welcome! Please read our contributing guidelines and submit pull requests for any improvements.
</document_content>
</document>

<document index="16">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
├── conftest.py              # Shared fixtures and configuration
├── run_tests.py             # Simple test runner script
├── test_api/                # API layer tests
│   ├── test_client.py       # LMStudioClient tests
│   └── test_exceptions.py   # Custom exception tests
├── test_core/               # Core module tests
│   ├── test_context_tester.py  # Context optimization tests
│   ├── test_inference.py    # Inference engine tests
│   ├── test_models.py       # Model and registry tests
│   ├── test_prompts.py      # Prompt resolution tests
│   └── test_scanner.py      # Model scanner tests
├── test_loaders/            # Loader tests
│   ├── test_context_loader.py   # Context file loading tests
│   ├── test_model_loader.py     # Model loader tests
│   └── test_prompt_loader.py    # Prompt loader tests
├── test_utils/              # Utility tests
│   └── test_paths.py        # Path utility tests
├── test_integration/        # Integration tests
│   └── test_cli_integration.py  # CLI integration tests
└── test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="17">
<source>TODO.md</source>
<document_content>
# LMStrix TODO List

## Current Active Work

### Priority 1: Issue #302 - Inference Output Mismatch (Final Phase)

**Major diagnostic work completed in v1.0.66. Remaining items:**

- [ ] Fix context length handling (GUI uses full 131072, CLI reduces to 65536)
- [ ] Investigate stop token configuration differences between GUI and CLI
- [ ] Compare maxTokens calculation: GUI uses -1 (unlimited), CLI calculates specific values
- [ ] Run side-by-side comparison with identical parameters
- [ ] Create regression test to prevent future recurrence

### Priority 2: Issue #105 - Adam.toml Simplification

- [ ] Update prompt loader to handle flat TOML structure (no nested groups)
- [ ] Add --text and --text_file parameters to infer command for direct text input
- [ ] Add backward compatibility for existing nested TOML files
- [ ] Update examples to use simplified flat format
- [ ] Create migration guide for existing users

## Future Development (Lower Priority)

### Code Quality Improvements

- [ ] Ruff linting improvements (143 errors identified)
  - [ ] Fix BLE001 blind exception catching (46 occurrences)
  - [ ] Fix A002 builtin shadowing (3 occurrences)
  - [ ] Address TRY300 statement placement issues (8 occurrences)
  - [ ] Remove commented-out code (ERA001)
- [ ] Add comprehensive type hints to public APIs
- [ ] Ensure all functions have proper docstrings
- [ ] Standardize error handling patterns

### Configuration and Utilities

- [ ] Create utils/config.py for centralized configuration handling
- [ ] Consolidate path handling functions
- [ ] Improve model loading optimization and reuse detection

### CLI Enhancement

- [ ] Add `reset` command for clearing model test data
- [ ] Add `health` command for system diagnostics
- [ ] Enhance `scan` and `list` commands with better filtering

### Testing and Documentation

- [ ] Ensure >90% test coverage maintained
- [ ] Update README.md with latest features
- [ ] Create comprehensive CLI reference
- [ ] Add integration tests for new features


</document_content>
</document>

<document index="18">
<source>WORK.md</source>
<document_content>
# Current Work Progress

## 1. Recently Completed Work

### 1.1. Issue #307 - Streaming Inference Support ✅

#### 1.1.1. What was done:
1. **Added streaming support to LMStudioClient** (`src/lmstrix/api/client.py`):
   - Implemented `stream_completion()` method using lmstudio SDK's `complete_stream()`
   - Added token-by-token callbacks with `on_prediction_fragment` and `on_first_token`
   - Implemented timeout watchdog (default 120s) to detect stalled generations
   - Added streaming statistics (tokens/second, time to first token)

2. **Extended InferenceEngine and InferenceManager** with streaming:
   - Added `stream_infer()` method to both classes
   - Maintains same model loading/reuse logic as regular inference
   - Supports all existing parameters plus streaming-specific ones

3. **Updated CLI with --stream flag**:
   - Added `--stream` and `--stream-timeout` parameters to `infer` command
   - Tokens are displayed in real-time to stdout as they are generated
   - Maintains backward compatibility - regular inference still works

#### 1.1.2. How to use:
```bash
# Regular inference (blocking)
lmstrix infer "Hello world" -m model-id

# Streaming inference (real-time)
lmstrix infer "Hello world" -m model-id --stream

# With custom timeout
lmstrix infer "Hello world" -m model-id --stream --stream-timeout 180
```

### 1.2. Issue #306 - Batch Processing Tool ✅

#### 1.2.1. What was done:
1. **Created adamall.py batch processing tool** (`_keep_this/adam/adamall.py`):
   - Processes 6 specific prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`
   - Smart model management - reuses loaded models when possible
   - Skips existing outputs for resumable processing
   - Safe filename generation using pathvalidate
   - Error capture to output files on failure

2. **Key features implemented**:
   - Loads models with 50% context, runs inference with 90% of max context
   - Sorts models by size (descending) for optimal processing order
   - Progress tracking with percentage and ETA
   - Comprehensive error handling and logging

#### 1.2.2. How to use:
```bash
cd _keep_this/adam
python adamall.py
```

Output files will be generated in `_keep_this/adam/out/` with names like:
- `think_aps--model_name.txt`
- `translate--model_name.txt`

## 2. ACTIVE: Issue #302 - Fix Inference Output Mismatch

### 2.1. Problem Summary
When running the same translation prompt:
- **LM Studio GUI**: Produces proper Polish translation (639 tokens)
- **lmstrix CLI**: Only outputs `</translate>` (4 tokens)

### 2.2. Root Cause Analysis
Found several configuration differences:
1. Temperature: GUI=0.8, CLI=0.7 → Updated default to 0.8 ✅
2. top_k: GUI=20, CLI=40 → Now configurable via CLI ✅
3. Context: GUI=131072, CLI=65536 (reduced)
4. max_predict: GUI=-1, CLI=117964
5. Stop tokens configuration may differ

### 2.3. Current Work Items

#### 2.3.1. Add Diagnostic Logging ✅
- [x] Log exact prompt with escape sequences visible
- [x] Log all inference parameters in detail
- [x] Add comparison with LM Studio defaults
- [x] Log stop token configuration

#### 2.3.2. Parameter Alignment
- [x] Change default temperature to 0.8
- [x] Add CLI flags for inference parameters
- [ ] Fix maxTokens calculation
- [ ] Add stop token configuration

#### 2.3.3. Context Length Fix
- [ ] Fix context reduction issue
- [ ] Use full model context by default
- [ ] Add warning for context reduction

#### 2.3.4. Testing
- [ ] Compare with LM Studio output
- [ ] Verify token counts match
- [ ] Test translation quality

### 2.4. Implementation Progress
Added diagnostic logging and streaming support. Need to investigate stop token issue next...

### 2.5. Rich Table Width Fix ✅
Updated all Rich tables to take 100% console width:
- Added `expand=True` to all Table constructors (9 tables total)
- Removed all fixed `width` parameters from `add_column()` calls
- Tables now automatically use full terminal width for better readability

### 2.6. Response Preview Enhancement ✅
Fixed response preview display in `lmstrix test` command:
- **Problem**: Response preview was truncated to only 20 characters, showing "||Custom prompt respon||" instead of meaningful content
- **Solution**: Created `_format_response_preview()` helper function that:
  - Increases preview length from 20 to 60 characters (configurable)
  - Cleans up whitespace and newlines for better table display
  - Adds ellipsis ("...") when truncated
  - Maintains "❌" for failed responses
- **Result**: Users now see more meaningful response content like "||Custom prompt response for testing context limits and model inference...||"

### 2.7. Test Suite Fixes (Priority 0) ✅
All critical AttributeError issues have been resolved:
1. **Model.validate_integrity()** ✅
2. **PromptResolver methods** ✅  
3. **ContextTester methods** ✅
4. **ModelScanner.sync_with_registry()** ✅

### 2.7. Issues 201-204 ✅
- Model persistence between calls
- Beautiful enhanced logging
- Fixed model lookup for paths/IDs
- Comprehensive inference statistics

## 3. Next Steps After Issue #302
1. Complete remaining test fixes
2. Issue #105 - Adam.toml simplification
3. Context testing streamlining
4. Model loading optimization
</document_content>
</document>

<document index="19">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="20">
<source>cleanup.txt</source>
<document_content>
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=106,col=9,endLine=106,endColumn=20::src/lmstrix/loaders/model_loader.py:106:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=186,col=9,endLine=186,endColumn=25::src/lmstrix/loaders/model_loader.py:186:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=361,col=9,endLine=361,endColumn=20::src/lmstrix/loaders/model_loader.py:361:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=602,col=9,endLine=602,endColumn=13::src/lmstrix/api/main.py:602:9: ARG002 Unused method argument: `sort`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=670,col=20,endLine=670,endColumn=29::src/lmstrix/api/main.py:670:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=834,col=24,endLine=834,endColumn=33::src/lmstrix/api/main.py:834:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (SIM108),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=848,col=17,endLine=854,endColumn=51::src/lmstrix/api/main.py:848:17: SIM108 Use ternary operator `pairs = dict_params.split(",") if "," in dict_params else dict_params.split(",")` instead of `if`-`else`-block
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=849,col=21,endLine=849,endColumn=56::src/lmstrix/api/main.py:849:21: ERA001 Found commented-out code
::error title=Ruff (PLW2901),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=857,col=21,endLine=857,endColumn=25::src/lmstrix/api/main.py:857:21: PLW2901 `for` loop variable `pair` overwritten by assignment target
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=874,col=24,endLine=874,endColumn=33::src/lmstrix/api/main.py:874:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=957,col=20,endLine=957,endColumn=29::src/lmstrix/api/main.py:957:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=1060,col=20,endLine=1060,endColumn=29::src/lmstrix/api/main.py:1060:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=1202,col=16,endLine=1202,endColumn=25::src/lmstrix/api/main.py:1202:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=64,col=17,endLine=64,endColumn=35::src/lmstrix/utils/context_parser.py:64:17: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (B904),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=73,col=13,endLine=75,endColumn=14::src/lmstrix/utils/context_parser.py:73:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=3,col=1,endLine=3,endColumn=40::src/lmstrix/utils/state.py:3:1: ERA001 Found commented-out code
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=26,col=20,endLine=26,endColumn=29::src/lmstrix/utils/state.py:26:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=35,col=16,endLine=35,endColumn=25::src/lmstrix/utils/state.py:35:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=127,col=20,endLine=127,endColumn=29::src/lmstrix/api/client.py:127:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=155,col=13,endLine=155,endColumn=31::src/lmstrix/api/client.py:155:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=198,col=11,endLine=198,endColumn=17::src/lmstrix/api/client.py:198:11: ARG002 Unused method argument: `kwargs`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=383,col=9,endLine=383,endColumn=29::src/lmstrix/api/client.py:383:9: ARG002 Unused method argument: `model_context_length`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=392,col=11,endLine=392,endColumn=17::src/lmstrix/api/client.py:392:11: ARG002 Unused method argument: `kwargs`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py,line=3,col=1,endLine=3,endColumn=49::src/lmstrix/core/concrete_config.py:3:1: ERA001 Found commented-out code
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py,line=126,col=13,endLine=126,endColumn=24::src/lmstrix/core/concrete_config.py:126:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=122,col=13,endLine=122,endColumn=46::src/lmstrix/core/inference.py:122:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=124,col=16,endLine=124,endColumn=25::src/lmstrix/core/inference.py:124:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=132,col=24,endLine=132,endColumn=33::src/lmstrix/core/inference.py:132:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=234,col=20,endLine=234,endColumn=29::src/lmstrix/core/inference.py:234:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=339,col=16,endLine=339,endColumn=25::src/lmstrix/core/inference.py:339:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=376,col=29,endLine=376,endColumn=35::src/lmstrix/core/inference.py:376:29: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=376,col=29,endLine=377,endColumn=37::src/lmstrix/core/inference.py:376:29: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=401,col=28,endLine=401,endColumn=37::src/lmstrix/core/inference.py:401:28: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=484,col=20,endLine=484,endColumn=29::src/lmstrix/core/inference.py:484:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=3,col=1,endLine=3,endColumn=47::src/lmstrix/core/models.py:3:1: ERA001 Found commented-out code
::error title=Ruff (SIM102),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=142,col=9,endLine=144,endColumn=40::src/lmstrix/core/models.py:142:9: SIM102 Use a single `if` statement instead of nested `if` statements
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=220,col=24,endLine=220,endColumn=33::src/lmstrix/core/models.py:220:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=226,col=16,endLine=226,endColumn=25::src/lmstrix/core/models.py:226:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=7,col=1,endLine=7,endColumn=51::src/lmstrix/core/inference_manager.py:7:1: ERA001 Found commented-out code
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=180,col=24,endLine=180,endColumn=33::src/lmstrix/core/inference_manager.py:180:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=202,col=13,endLine=210,endColumn=14::src/lmstrix/core/inference_manager.py:202:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=212,col=16,endLine=212,endColumn=25::src/lmstrix/core/inference_manager.py:212:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=257,col=24,endLine=257,endColumn=33::src/lmstrix/core/inference_manager.py:257:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=316,col=13,endLine=316,endColumn=46::src/lmstrix/core/inference_manager.py:316:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=318,col=16,endLine=318,endColumn=25::src/lmstrix/core/inference_manager.py:318:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=325,col=24,endLine=325,endColumn=33::src/lmstrix/core/inference_manager.py:325:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=454,col=24,endLine=454,endColumn=33::src/lmstrix/core/inference_manager.py:454:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=144,col=16,endLine=144,endColumn=25::src/lmstrix/core/context_tester.py:144:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ANN001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=158,col=35,endLine=158,endColumn=40::src/lmstrix/core/context_tester.py:158:35: ANN001 Missing type annotation for function argument `model`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=6,col=1,endLine=6,endColumn=32::debug_inference.py:6:1: ERA001 Found commented-out code
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=75,col=9,endLine=75,endColumn=15::debug_inference.py:75:9: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=75,col=9,endLine=76,endColumn=17::debug_inference.py:75:9: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=134,col=16,endLine=134,endColumn=25::debug_inference.py:134:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=137,col=9,endLine=137,endColumn=20::debug_inference.py:137:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=139,col=12,endLine=139,endColumn=21::debug_inference.py:139:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=142,col=9,endLine=142,endColumn=25::debug_inference.py:142:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=192,col=16,endLine=192,endColumn=25::debug_inference.py:192:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=195,col=9,endLine=195,endColumn=20::debug_inference.py:195:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=197,col=12,endLine=197,endColumn=21::debug_inference.py:197:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=200,col=9,endLine=200,endColumn=25::debug_inference.py:200:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (ANN201),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=206,col=5,endLine=206,endColumn=34::debug_inference.py:206:5: ANN201 Missing return type annotation for public function `test_lmstrix_inference_engine`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=221,col=13,endLine=221,endColumn=58::debug_inference.py:221:13: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=245,col=9,endLine=245,endColumn=32::debug_inference.py:245:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=247,col=12,endLine=247,endColumn=21::debug_inference.py:247:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=250,col=9,endLine=250,endColumn=25::debug_inference.py:250:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_streaming.py,line=47,col=12,endLine=47,endColumn=21::test_streaming.py:47:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=5,col=1,endLine=5,endColumn=35::test_model_out_ctx.py:5:1: ERA001 Found commented-out code
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=61,col=12,endLine=61,endColumn=21::test_model_out_ctx.py:61:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=63,col=9,endLine=63,endColumn=25::test_model_out_ctx.py:63:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=87,col=12,endLine=87,endColumn=21::test_model_out_ctx.py:87:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=98,col=16,endLine=98,endColumn=25::test_model_out_ctx.py:98:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=107,col=12,endLine=107,endColumn=21::test_model_out_ctx.py:107:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=6,col=1,endLine=6,endColumn=33::debug_test_logic.py:6:1: ERA001 Found commented-out code
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=47,col=9,endLine=47,endColumn=15::debug_test_logic.py:47:9: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=47,col=9,endLine=48,endColumn=17::debug_test_logic.py:47:9: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=117,col=12,endLine=117,endColumn=21::debug_test_logic.py:117:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=119,col=9,endLine=119,endColumn=25::debug_test_logic.py:119:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=137,col=9,endLine=137,endColumn=15::debug_test_logic.py:137:9: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=137,col=9,endLine=138,endColumn=17::debug_test_logic.py:137:9: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=180,col=12,endLine=180,endColumn=21::debug_test_logic.py:180:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=208,col=13,endLine=208,endColumn=19::debug_test_logic.py:208:13: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=208,col=13,endLine=209,endColumn=21::debug_test_logic.py:208:13: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=232,col=16,endLine=232,endColumn=25::debug_test_logic.py:232:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/get_out_ctx.py,line=5,col=1,endLine=5,endColumn=28::get_out_ctx.py:5:1: ERA001 Found commented-out code
::error title=Ruff (PTH123),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=53,col=10,endLine=53,endColumn=14::examples/python/prompt_templates_demo.py:53:10: PTH123 `open()` should be replaced by `Path.open()`
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged

</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/debug_inference.py
# Language: python

import sys
import time
from pathlib import Path
import lmstudio
from lmstrix.api.client import LMStudioClient
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger
import traceback
import traceback
from lmstrix.core.scanner import ModelScanner
import traceback

def print_separator((title: str)) -> None:
    """Print a nice separator."""

def test_direct_lmstudio(()) -> bool | None:
    """Test direct lmstudio calls without lmstrix wrapper."""

def test_lmstrix_client(()) -> bool | None:
    """Test lmstrix client calls."""

def test_lmstrix_inference_engine(()):
    """Test lmstrix inference engine."""

def main(()) -> None:
    """Run all diagnostic tests."""


# File: /Users/Shared/lmstudio/lmstrix/debug_test_logic.py
# Language: python

import sys
import time
from pathlib import Path
import builtins
import contextlib
import lmstudio
from lmstrix.utils.logging import logger
import traceback

def test_both_prompts(()) -> None:
    """Test both standard prompts and show why tests fail."""

def test_with_more_tokens(()) -> None:
    """Test with more output tokens to see if model just needs more space."""

def test_other_models(()) -> None:
    """Test a few other models to see if they behave differently."""

def main(()) -> None:
    """Run all diagnostic tests."""


<document index="21">
<source>docs/_config.yml</source>
<document_content>
title: LMStrix
description: The Unofficial Toolkit for Mastering LM Studio
remote_theme: just-the-docs/just-the-docs

url: https://twardoch.github.io/lmstrix

color_scheme: dark

search_enabled: false

aux_links:
  "LMStrix on GitHub":
    - "https://github.com/twardoch/lmstrix"

plugins:
  - jekyll-remote-theme
</document_content>
</document>

<document index="22">
<source>docs/api.md</source>
<document_content>
---
title: API Reference
---

## Python API Reference

This section provides a detailed reference for the LMStrix Python API.

### The `LMStrix` Class

The main entry point for interacting with the API.

`LMStrix(base_uri: str = "http://localhost:1234/v1")`

- `base_uri`: The base URI of the LM Studio server.

#### Methods

- `async scan_models()`: Scans for available models in LM Studio and updates the local model registry.
- `async list_models()`: Returns a list of `Model` objects from the local registry.
- `async test_model(model_id: str)`: Tests the context limit of a specific model and updates the registry with the result.
- `async infer(prompt: str, model_id: str, **kwargs)`: Runs inference on a model. Any additional keyword arguments are passed to the `complete()` method of the `lmstudio` client.

### The `Model` Class

Represents a model in LM Studio.

#### Attributes

- `id`: The model ID (e.g., `lmstudio-community/gemma-2b-it-GGUF`).
- `context_limit`: The declared context limit of the model, as reported by LM Studio.
- `tested_max_context`: The empirically tested maximum context limit that the model can handle on your machine. `None` if the model has not been tested.
- `context_test_status`: The status of the context test. Can be one of `"passed"`, `"failed"`, or `"not_tested"`.

### The `InferenceResponse` Class

Represents the response from an inference request.

#### Attributes

- `content`: The text content of the model's response.
- `usage`: A dictionary containing token usage information, e.g., `{'prompt_tokens': 10, 'completion_tokens': 20, 'total_tokens': 30}`.

</document_content>
</document>

<document index="23">
<source>docs/how-it-works.md</source>
<document_content>
---
title: How It Works
---

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **adaptive testing algorithm** (enhanced in v1.1):

1. **Initial Verification**: Tests at 1,024 tokens to ensure the model loads properly
2. **Threshold Test**: Tests at min(threshold, declared_max) where threshold defaults to 102,400 tokens
   - This prevents system crashes from attempting very large context sizes
3. **Adaptive Search**:
   - If the threshold test succeeds and is below the declared max: incrementally increases by 10,240 tokens until failure
   - If the threshold test fails: performs binary search between 1,024 and the failed size
4. **Progress Tracking**: Saves results after each test, allowing resumption if interrupted

**Batch Testing Optimization** (new in v1.1):
When testing multiple models with `--all`, LMStrix now:
- Sorts models by declared context size (ascending)
- Tests in passes to minimize model loading/unloading
- Excludes failed models from subsequent passes
- Provides detailed progress with Rich table output

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

### The `test` Command

The `test` command is the heart of the context optimization process. When you run `lmstrix test <model-id>`, it performs the binary search algorithm described above.

The command saves the results of the test to a local registry, so you only need to test each model once.

</document_content>
</document>

<document index="24">
<source>docs/index.md</source>
<document_content>
---
---

# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

</document_content>
</document>

<document index="25">
<source>docs/installation.md</source>
<document_content>
---
title: Installation
---

## Installation

### Requirements

- Python 3.10 or higher
- [LM Studio](https://lmstudio.ai/) installed and running locally
- At least one model downloaded in LM Studio

### Using pip

```bash
pip install lmstrix
```

### Using uv (recommended)

```bash
uv pip install lmstrix
```

### For Development

If you want to contribute to LMStrix or install it in an editable mode, you can clone the repository:

```bash
git clone https://github.com/twardoch/lmstrix
cd lmstrix
pip install -e .[dev]
```

</document_content>
</document>

<document index="26">
<source>docs/usage.md</source>
<document_content>
---
title: Usage
---

## Command-Line Interface (CLI)

LMStrix provides a powerful and intuitive CLI for interacting with your local models.

### Scanning for Models

Before you can use LMStrix, you need to scan for available models in LM Studio. This command discovers all models that you have downloaded.

```bash
lmstrix scan
```

### Listing Models

To see a list of all discovered models, their context length, and test status, use the `list` command.

```bash
lmstrix list
```

### Testing Context Limits

This is the core feature of LMStrix. The `test` command automatically determines the maximum context window a model can handle on your machine.

```bash
# Test a specific model by its ID
lmstrix test "model-id-here"

# Test all models that haven't been tested yet
lmstrix test --all

# Test with a custom threshold (default: 102,400 tokens)
# This prevents system crashes by limiting the maximum initial test size
lmstrix test "model-id-here" --threshold 51200

# Test all models with a lower threshold for safety
lmstrix test --all --threshold 32768
```

**New in v1.1**: The `--threshold` parameter (default: 102,400 tokens) prevents system crashes when testing models with very large declared context sizes. The testing algorithm now:
1. Tests at 1,024 tokens to verify the model loads
2. Tests at min(threshold, declared_max)
3. If successful and below declared max, increments by 10,240 tokens
4. If failed, performs binary search to find the exact limit

For more details on how this works, see the [How It Works](./how-it-works.md) page.

### Running Inference

You can run inference directly from the command line.

```bash
# Run a simple prompt
lmstrix infer "Your prompt here" --model "model-id" --max-tokens 150

# Use a prompt from a file
lmstrix infer "@prompts.toml:greeting" --model "model-id"
```

### Verbose Mode

For more detailed output and debugging, you can use the `--verbose` flag with any command.

```bash
lmstrix scan --verbose
```

## Python API

The Python API provides a clean, `async`-first interface for programmatic access to LMStrix features.

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    lms = LMStrix()
    
    # Scan for available models
    await lms.scan_models()
    
    # List all models
    models = await lms.list_models()
    for model in models:
        print(f"Model: {model.id}")
        print(f"  Context limit: {model.context_limit:,} tokens")
        print(f"  Tested limit: {model.tested_max_context or 'Not tested'}")
        print(f"  Status: {model.context_test_status}")
    
    # Test a specific model's context limits
    model_id = models[0].id if models else None
    if model_id:
        print(f"\nTesting context limits for {model_id}...")
        result = await lms.test_model(model_id)
        print(f"Optimal context: {result.tested_max_context} tokens")
        print(f"Test status: {result.context_test_status}")
    
    # Run inference
    if model_id:
        response = await lms.infer(
            prompt="What is the meaning of life?",
            model_id=model_id,
            max_tokens=100
        )
        print(f"\nInference result:\n{response.content}")

if __name__ == "__main__":
    asyncio.run(main())
```

</document_content>
</document>

<document index="27">
<source>examples/README.md</source>
<document_content>
# LMStrix Usage Examples

This directory contains comprehensive examples demonstrating all features of the LMStrix CLI and Python API, including the latest context control, prompt templates, and model state management features.

## Prerequisites

1. **LMStrix Installed**: Ensure you have installed LMStrix (`pip install lmstrix`)
2. **LM Studio Running**: Most examples require LM Studio running with at least one model downloaded
3. **Model Downloaded**: Download models in LM Studio (e.g., Llama, Mistral, Phi, Qwen)

**Note**: Examples use "llama" as a placeholder model ID. Update this to match your downloaded models. Run `lmstrix list` to see available models.

## Quick Start

Run all examples at once:
```bash
bash run_all_examples.sh
```

Or run individual examples as shown below.

---

## CLI Examples (`cli/`)

### Core Workflow Examples

#### `basic_workflow.sh`
Complete end-to-end workflow demonstrating:
- Model scanning with verbose output
- Listing models with `--sort` and `--show` options
- Context testing with new fixed strategy
- Inference with `--in_ctx` and `--out_ctx` parameters
- Model state reuse demonstration

```bash
bash cli/basic_workflow.sh
```

#### `model_testing.sh`
Advanced context testing features:
- Fixed context testing strategy (30k, 40k, 60k, etc.)
- `--threshold` parameter for safety limits
- `--fast` mode for quick validation
- `--ctx` for specific context testing
- Batch testing with `--all`
- Test resumption capabilities

```bash
bash cli/model_testing.sh
```

#### `inference_examples.sh`
Comprehensive inference scenarios:
- Context control with `--in_ctx` and `--out_ctx`
- Load without context specification (`--in_ctx 0`)
- Model state detection and reuse
- `--force-reload` demonstration
- TOML prompt templates with `--file_prompt` and `--dict`
- Temperature control for creativity

```bash
bash cli/inference_examples.sh
```

### New Feature Examples

#### `context_control_examples.sh` *(NEW)*
Deep dive into context management:
- Understanding `--in_ctx` vs `--out_ctx`
- Memory-conscious loading strategies
- Context size performance impact
- Long document processing
- Optimal context selection

```bash
bash cli/context_control_examples.sh
```

#### `model_state_demo.sh` *(NEW)*
Model state detection and management:
- How model reuse works
- Performance comparison (load vs reuse)
- Force reload scenarios
- Model switching strategies
- Context change behavior

```bash
bash cli/model_state_demo.sh
```

---

## Python API Examples (`python/`)

### Core API Usage

#### `basic_usage.py`
Fundamentals of the LMStrix Python API:
- Initializing `LMStrix` client
- Scanning and listing models
- Basic inference with `out_ctx`
- Advanced inference with `in_ctx`
- Model state detection
- Error handling

```bash
python3 python/basic_usage.py
```

#### `advanced_testing.py`
Context testing programmatically:
- Fixed context testing strategy
- Fast mode testing
- Custom threshold limits
- Specific context testing
- Batch testing multiple models
- Test result analysis

```bash
python3 python/advanced_testing.py
```

#### `custom_inference.py`
Advanced inference techniques:
- Context control (`in_ctx` and `out_ctx`)
- Temperature adjustment
- TOML prompt template loading
- Structured JSON output
- Model state reuse
- Force reload scenarios

```bash
python3 python/custom_inference.py
```

#### `batch_processing.py`
Working with multiple models:
- Batch testing strategies
- Model response comparison
- Performance benchmarking
- Efficiency analysis
- Smart testing order

```bash
python3 python/batch_processing.py
```

### New Python Examples

#### `prompt_templates_demo.py` *(NEW)*
Advanced prompt template features:
- Creating prompts programmatically
- Nested placeholder resolution
- Loading from TOML files
- Batch prompt resolution
- Missing placeholder handling
- Context injection with truncation

```bash
python3 python/prompt_templates_demo.py
```

---

## Prompt Templates (`prompts/`)

### Main Template File

#### `prompts.toml`
Comprehensive prompt template examples:
- Greetings (formal, casual, professional)
- Templates with internal references
- Code review and explanation prompts
- Research summarization templates
- Math and science prompts
- Creative writing templates
- Q&A formats
- System prompts

### Domain-Specific Templates

- `prompts/analysis.toml` - Data analysis prompts
- `prompts/coding.toml` - Programming assistance
- `prompts/creative.toml` - Creative writing
- `prompts/qa.toml` - Question answering

---

## Data Files (`data/`)

- `sample_context.txt` - Large text for context testing
- `test_questions.json` - Sample Q&A scenarios

---

## Key Features Demonstrated

### Context Management
- **`--in_ctx`**: Control model loading context size
- **`--out_ctx`**: Control maximum generation tokens
- **Model reuse**: Automatic reuse of loaded models
- **Force reload**: Refresh models with `--force-reload`

### Testing Enhancements
- **Fixed contexts**: Tests at 30k, 40k, 60k, 80k, 100k, 120k
- **Threshold safety**: Limit initial test size
- **Fast mode**: Skip semantic verification
- **Test resumption**: Continue interrupted tests

### Prompt Templates
- **TOML loading**: Load prompts from `.toml` files
- **Placeholders**: Dynamic value substitution
- **Nested references**: Templates can reference other templates
- **Batch resolution**: Process multiple prompts at once

### Performance Features
- **Smart sorting**: Optimal model testing order
- **Model state tracking**: Know when models are loaded
- **Efficiency metrics**: Track tested vs declared ratios
- **Batch operations**: Test or query multiple models

---

## Running Examples

### Individual Examples
```bash
# CLI examples
bash cli/basic_workflow.sh
bash cli/model_testing.sh
bash cli/inference_examples.sh
bash cli/context_control_examples.sh
bash cli/model_state_demo.sh

# Python examples
python3 python/basic_usage.py
python3 python/advanced_testing.py
python3 python/custom_inference.py
python3 python/batch_processing.py
python3 python/prompt_templates_demo.py
```

### All Examples
```bash
bash run_all_examples.sh
```

---

## Tips for Success

1. **Update Model IDs**: Change "llama" to match your downloaded models
2. **Check Model List**: Run `lmstrix list` to see available models
3. **Start Small**: Begin with `basic_workflow.sh` or `basic_usage.py`
4. **Test First**: Run `lmstrix test` to find optimal context for your models
5. **Use Verbose Mode**: Add `--verbose` for detailed output
6. **Monitor Memory**: Larger contexts use more GPU/RAM

---

## Troubleshooting

If examples fail:
1. Ensure LM Studio is running
2. Check you have models downloaded
3. Update model IDs in scripts
4. Verify LMStrix installation: `lmstrix --version`
5. Check available models: `lmstrix list`

For more help, see the main [LMStrix documentation](https://github.com/AdamAdli/lmstrix).
</document_content>
</document>

<document index="28">
<source>examples/cli/basic_workflow.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates the complete, basic workflow of LMStrix.
# 1. Scan for downloaded models.
# 2. List the models found with various display options.
# 3. Test the context length of a specific model.
# 4. Run inference with the tested model using new features.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Basic Workflow Demo ###"

# Step 1: Scan for models
# This command discovers all models downloaded in your LM Studio installation
# and updates the local registry file (lmstrix.json).
echo -e "\n--- Step 1: Scanning for models ---"
lmstrix scan --verbose
echo "Scan complete. Model registry updated."

# Step 2: List models with different display options
# This demonstrates the new --show and --sort options
echo -e "\n--- Step 2a: Listing models (default view) ---"
lmstrix list
echo -e "\n--- Step 2b: Listing models sorted by context size ---"
lmstrix list --sort ctx
echo -e "\n--- Step 2c: Showing just model IDs ---"
lmstrix list --show id
echo "Model listing demonstrations complete."

# Step 3: Test a model's context length
# We'll use a common model identifier that users might have
echo -e "\n--- Step 3: Testing a model's context length ---"
echo "Note: This may take several minutes depending on the model and your hardware."
# Common model patterns users might have:
# "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded
echo "Looking for models matching: $MODEL_ID"
# Show models matching the pattern
lmstrix list --show id | grep -i "$MODEL_ID" || echo "No models found matching '$MODEL_ID'"
echo -e "\nTesting model: $MODEL_ID"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Context test complete."

# Step 4: Run inference with new context control features
# Demonstrates --out_ctx instead of deprecated --max_tokens
echo -e "\n--- Step 4a: Running basic inference ---"
lmstrix infer "What is the capital of France?" "$MODEL_ID" --out_ctx 50

echo -e "\n--- Step 4b: Running inference with specific loading context ---"
lmstrix infer "Explain quantum computing in simple terms." "$MODEL_ID" --in_ctx 4096 --out_ctx 200

echo -e "\n--- Step 4c: Checking if model is already loaded (reuse demo) ---"
lmstrix infer "What is 2+2?" "$MODEL_ID" --out_ctx 10

echo -e "\nInference demonstrations complete."

echo -e "\n### Workflow Demo Finished ###"
</document_content>
</document>

<document index="29">
<source>examples/cli/context_control_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates advanced context control features in LMStrix.
# Shows --in_ctx usage, model state detection, and context optimization.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Context Control Examples ###"

# Replace with a model identifier that matches your downloaded models
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Understanding Context Parameters
echo -e "\n--- Example 1: Understanding Context Parameters ---"
echo "LMStrix has two context parameters:"
echo "  --in_ctx: Controls the context size when LOADING the model"
echo "  --out_ctx: Controls the maximum tokens to GENERATE"
echo -e "\nLet's see them in action..."

# Example 2: Default Context Loading
echo -e "\n--- Example 2: Default Context Loading ---"
echo "When --in_ctx is not specified, LMStrix uses the optimal context:"
lmstrix infer "What is machine learning?" "$MODEL_ID" --out_ctx 50 --verbose
echo "(Model loaded with optimal context based on testing or declared limit)"

# Example 3: Specific Context Loading
echo -e "\n--- Example 3: Loading with Specific Context ---"
echo "Load model with exactly 4096 tokens of context:"
lmstrix infer "Explain neural networks briefly." "$MODEL_ID" --in_ctx 4096 --out_ctx 100 --verbose
echo "(Model loaded with 4096 token context)"

# Example 4: Model State Detection
echo -e "\n--- Example 4: Model State Detection ---"
echo "First inference - model loads:"
lmstrix infer "What is 5 + 5?" "$MODEL_ID" --out_ctx 10
echo -e "\nSecond inference - model already loaded (should be faster):"
lmstrix infer "What is 10 + 10?" "$MODEL_ID" --out_ctx 10
echo "(Notice: Model was reused, not reloaded)"

# Example 5: Context Size Impact
echo -e "\n--- Example 5: Context Size Impact ---"
echo "Smaller context (faster loading, less memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 2048 --out_ctx 10

echo -e "\nLarger context (slower loading, more memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 16384 --out_ctx 10

# Example 6: Zero Context Loading
echo -e "\n--- Example 6: Zero Context Loading (--in_ctx 0) ---"
echo "Load model without specifying context (uses model's default):"
lmstrix infer "What is Python?" "$MODEL_ID" --in_ctx 0 --out_ctx 50
echo "(Model loaded with its default context configuration)"

# Example 7: Context Optimization Strategy
echo -e "\n--- Example 7: Context Optimization Strategy ---"
echo "For best performance:"
echo "1. Test your model first to find optimal context:"
echo "   lmstrix test --model_id $MODEL_ID"
echo -e "\n2. Use the tested context for inference:"
echo "   lmstrix infer 'prompt' $MODEL_ID --in_ctx <tested_value>"
echo -e "\n3. Or let LMStrix choose automatically (omit --in_ctx)"

# Example 8: Long Context Use Case
echo -e "\n--- Example 8: Long Context Use Case ---"
echo "Processing a long document (simulated):"
LONG_PROMPT="Summarize this text: The history of artificial intelligence began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by classical philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols."
echo "Prompt length: ${#LONG_PROMPT} characters"
lmstrix infer "$LONG_PROMPT" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 9: Memory-Conscious Loading
echo -e "\n--- Example 9: Memory-Conscious Loading ---"
echo "For limited memory systems, use smaller context:"
lmstrix infer "What is RAM?" "$MODEL_ID" --in_ctx 2048 --out_ctx 50
echo "(Smaller context = less GPU/RAM usage)"

# Example 10: Comparing Context Sizes
echo -e "\n--- Example 10: Context Size Comparison ---"
echo "Let's compare the same prompt with different contexts:"

echo -e "\nWith 2K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 2048 --out_ctx 100

echo -e "\nWith 8K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 8192 --out_ctx 100

echo -e "\nNote: Larger context doesn't always mean better output for simple prompts"

echo -e "\n### Context Control Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- --in_ctx controls model loading context (memory usage)"
echo "- --out_ctx controls generation length"
echo "- Models are reused when possible for efficiency"
echo "- Optimal context depends on your use case"
echo "- Test models first to find their true limits"
</document_content>
</document>

<document index="30">
<source>examples/cli/inference_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various inference scenarios using LMStrix.
# Shows new features: --in_ctx, --out_ctx, --file_prompt, --dict, --force-reload
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Inference Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Simple Question with new --out_ctx parameter
echo -e "\n--- Example 1: Simple Question with Output Context Control ---"
lmstrix infer "Explain the theory of relativity in simple terms." "$MODEL_ID" --out_ctx 200

# Example 2: Context Control - Load model with specific context size
echo -e "\n--- Example 2: Load Model with Specific Context Size ---"
lmstrix infer "What are the benefits of renewable energy?" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 3: Load model without context specification (--in_ctx 0)
echo -e "\n--- Example 3: Load Model with Default Context ---"
lmstrix infer "What is machine learning?" "$MODEL_ID" --in_ctx 0 --out_ctx 100

# Example 4: Model State Detection - Second call reuses loaded model
echo -e "\n--- Example 4: Model State Detection (Reuse) ---"
echo "First call - loads the model:"
lmstrix infer "What is Python?" "$MODEL_ID" --out_ctx 50
echo -e "\nSecond call - reuses loaded model:"
lmstrix infer "What is JavaScript?" "$MODEL_ID" --out_ctx 50

# Example 5: Force Reload - Reload model even if already loaded
echo -e "\n--- Example 5: Force Model Reload ---"
lmstrix infer "What is artificial intelligence?" "$MODEL_ID" --force-reload --out_ctx 100

# Example 6: Using TOML Prompt Files with Parameters
echo -e "\n--- Example 6: Using Prompt Templates from TOML ---"
# First, let's check if prompts.toml exists
if [ -f "examples/prompts.toml" ]; then
    PROMPT_FILE="examples/prompts.toml"
elif [ -f "prompts.toml" ]; then
    PROMPT_FILE="prompts.toml"
else
    # Create a simple prompts.toml for demonstration
    cat > temp_prompts.toml <<EOL
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"

[code]
review = "Please review this {{language}} code and suggest improvements: {{code}}"
explain = "Explain this {{language}} code in simple terms: {{code}}"
EOL
    PROMPT_FILE="temp_prompts.toml"
fi

echo "Using prompt file: $PROMPT_FILE"
lmstrix infer greetings.formal "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "name=Alice,topic=Python" --out_ctx 100

# Example 7: Multiple Parameters with Different Format
echo -e "\n--- Example 7: Code Review with Template ---"
lmstrix infer code.review "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "language=Python,code=def factorial(n): return 1 if n <= 1 else n * factorial(n-1)" --out_ctx 300

# Example 8: Adjusting Temperature for Creative Output
echo -e "\n--- Example 8: Creative Writing with High Temperature ---"
lmstrix infer "Write a short poem about the sea." "$MODEL_ID" --temperature 1.5 --out_ctx 150

# Example 9: Low Temperature for Deterministic Output
echo -e "\n--- Example 9: Math Problem with Low Temperature ---"
lmstrix infer "Calculate: 25 * 4 + 10 - 5" "$MODEL_ID" --temperature 0.1 --out_ctx 20

# Example 10: Combining Features - Context Control + Template + Temperature
echo -e "\n--- Example 10: Combined Features Demo ---"
lmstrix infer greetings.casual "$MODEL_ID" \
    --file_prompt "$PROMPT_FILE" \
    --dict "name=Bob,topic=quantum computing" \
    --in_ctx 4096 \
    --out_ctx 150 \
    --temperature 0.8

# Cleanup temporary file if created
if [ "$PROMPT_FILE" = "temp_prompts.toml" ]; then
    rm temp_prompts.toml
fi

echo -e "\n### Inference Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- Use --out_ctx instead of deprecated --max_tokens"
echo "- Use --in_ctx to control model loading context size"
echo "- Use --file_prompt with --dict for template-based prompts"
echo "- Models are reused when already loaded (unless --force-reload)"
echo "- Combine features for advanced use cases"
</document_content>
</document>

<document index="31">
<source>examples/cli/model_state_demo.sh</source>
<document_content>
#!/bin/bash
# this_file: examples/cli/model_state_demo.sh
#
# Demonstration of model state persistence in LMStrix
# Shows how models remain loaded across multiple infer calls
#

set -e

echo "### LMStrix Model State Persistence Demo ###"
echo ""
echo "This demo shows how LMStrix now keeps models loaded between calls"
echo "for better performance and resource efficiency."
echo ""

MODEL_ID="llama-3.2-3b-instruct"  # Change this to match your model

# Step 1: First inference with explicit model and context
echo "=== Step 1: First inference with explicit model and context ==="
echo "Command: lmstrix infer \"What is 2+2?\" -m \"$MODEL_ID\" --out_ctx \"25%\" --verbose"
echo "(This will load the model with specified context)"
echo ""
lmstrix infer "What is 2+2?" -m "$MODEL_ID" --out_ctx "25%" --verbose || echo "Failed"

echo ""
echo "=== Step 2: Second inference without in_ctx ==="
echo "Command: lmstrix infer \"What is 3+3?\" -m \"$MODEL_ID\" --verbose"
echo "(This should reuse the already loaded model)"
echo ""
sleep 2
lmstrix infer "What is 3+3?" -m "$MODEL_ID" --verbose || echo "Failed"

echo ""
echo "=== Step 3: Third inference without model_id ==="
echo "Command: lmstrix infer \"What is 4+4?\" --verbose"
echo "(This should use the last-used model: $MODEL_ID)"
echo ""
sleep 2
lmstrix infer "What is 4+4?" --verbose || echo "Failed"

echo ""
echo "=== Demo Complete ==="
echo ""
echo "Key features demonstrated:"
echo "1. Models stay loaded between infer calls when in_ctx is not specified"
echo "2. Last-used model is remembered when -m is not specified"
echo "3. Better performance by avoiding repeated model loading/unloading"
echo ""
echo "Check the verbose output above to see:"
echo "- First call: 'Loading with optimal context...'"
echo "- Second call: 'Model already loaded... reusing...'"
echo "- Third call: 'Using last-used model...'"
</document_content>
</document>

<document index="32">
<source>examples/cli/model_testing.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various context testing scenarios with LMStrix.
# Shows new features: --threshold, --all --ctx, --fast, and test resumption.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Model Testing Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Standard Test with New Fixed Context Strategy
# Tests at fixed context sizes: 30k, 40k, 60k, 80k, 100k, 120k, and declared_max-1
echo -e "\n--- Example 1: Standard Context Test ---"
echo "Testing model '$MODEL_ID' with fixed context strategy"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Standard test complete."

# Example 2: Test with Threshold Limit
# Prevents initial test from exceeding specified threshold (default: 102,400)
echo -e "\n--- Example 2: Test with Custom Threshold ---"
echo "Testing with maximum initial context of 50,000 tokens"
lmstrix test --model_id "$MODEL_ID" --threshold 50000
echo "Threshold test complete."

# Example 3: Fast Mode Testing
# Only checks if model can load and generate, skips semantic verification
echo -e "\n--- Example 3: Fast Mode Test ---"
echo "Running fast test (skip semantic verification)"
lmstrix test --model_id "$MODEL_ID" --fast
echo "Fast test complete."

# Example 4: Test at Specific Context Size
# Tests a model at a specific context size only
echo -e "\n--- Example 4: Test at Specific Context ---"
echo "Testing model at exactly 8192 tokens"
lmstrix test --model_id "$MODEL_ID" --ctx 8192
echo "Specific context test complete."

# Example 5: Batch Test All Models
# Tests all untested models in the registry
echo -e "\n--- Example 5: Batch Test All Models ---"
echo "Testing all models (this may take a while)..."
echo "Note: This will test ALL models. Press Ctrl+C to cancel."
read -p "Press Enter to continue..." -t 5 || true
lmstrix test --all --verbose
echo "Batch test complete."

# Example 6: Batch Test with Context Limit
# Tests all models at a specific context size
echo -e "\n--- Example 6: Batch Test at Specific Context ---"
echo "Testing all untested models at 4096 tokens"
lmstrix test --all --ctx 4096
echo "Batch context test complete."

# Example 7: Fast Batch Testing
# Quick test of all models without semantic verification
echo -e "\n--- Example 7: Fast Batch Test ---"
echo "Running fast test on all models"
lmstrix test --all --fast
echo "Fast batch test complete."

# Example 8: Show Test Results
# Display models sorted by their tested context size
echo -e "\n--- Example 8: View Test Results ---"
echo "Models sorted by tested context size:"
lmstrix list --sort ctx
echo -e "\nModels sorted by efficiency (tested/declared ratio):"
lmstrix list --sort eff

# Example 9: Test with Smart Sorting
# When testing all models, they're sorted for efficiency
echo -e "\n--- Example 9: Smart Sorted Batch Test ---"
echo "Testing all models with smart sorting (smaller models first)"
lmstrix test --all --threshold 30000 --fast
echo "Smart sorted test complete."

# Example 10: Resume Interrupted Test
# If a test is interrupted, it can resume from where it left off
echo -e "\n--- Example 10: Test Resumption Demo ---"
echo "LMStrix automatically saves progress during testing."
echo "If a test is interrupted, simply run the same command again."
echo "The test will resume from the last successful context size."

echo -e "\n### Model Testing Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- New fixed context testing strategy (30k, 40k, 60k, etc.)"
echo "- Use --threshold to limit maximum initial test size"
echo "- Use --fast for quick testing without semantic checks"
echo "- Use --ctx to test at specific context sizes"
echo "- Batch testing with --all supports all options"
echo "- Tests automatically resume if interrupted"
</document_content>
</document>

<document index="33">
<source>examples/data/sample_context.txt</source>
<document_content>
This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

</document_content>
</document>

<document index="34">
<source>examples/data/test_questions.json</source>
<document_content>
[
  {
    "id": "qa_1",
    "question": "What is the primary purpose of the sample_context.txt file?",
    "context_source": "sample_context.txt"
... (file content truncated to first 5 lines)
</document_content>
</document>

<document index="35">
<source>examples/prompts/analysis.toml</source>
<document_content>
# examples/prompts/analysis.toml

[summarize_text]
prompt = "Summarize the key points of the following text in three bullet points:

{text}"
system_prompt = "You are a helpful assistant that specializes in text summarization."
description = "Summarizes a long piece of text."

[sentiment_analysis]
prompt = "Analyze the sentiment of the following review. Respond with 'Positive', 'Negative', or 'Neutral'.

Review: "{review_text}""
system_prompt = "You are a sentiment analysis AI."
description = "Analyzes the sentiment of a given text."
</document_content>
</document>

<document index="36">
<source>examples/prompts/coding.toml</source>
<document_content>
# examples/prompts/coding.toml

[python_function]
prompt = "Write a Python function that does the following: {request}. Include a docstring and type hints."
system_prompt = "You are an expert Python programmer who writes clean, efficient, and well-documented code."
description = "Generates a Python function based on a request."

[explain_code]
prompt = "Explain what this code does in simple terms. Identify potential bugs if any.

Code:
```
{code_snippet}
```"
system_prompt = "You are a senior software engineer who is excellent at code reviews and explaining complex concepts."
description = "Explains a snippet of code."
</document_content>
</document>

<document index="37">
<source>examples/prompts/creative.toml</source>
<document_content>
# examples/prompts/creative.toml

[write_poem]
prompt = "Write a short, four-line poem about {topic}."
system_prompt = "You are a world-class poet."
description = "Writes a short poem on a given topic."

[generate_story_idea]
prompt = "Generate a compelling story idea based on the following genre and character: Genre: {genre}, Character: {character_description}"
system_prompt = "You are a creative writing assistant, skilled in generating unique plot ideas."
description = "Generates a story idea from a genre and character."
</document_content>
</document>

<document index="38">
<source>examples/prompts/qa.toml</source>
<document_content>
# examples/prompts/qa.toml

[simple_question]
prompt = "Answer the following question: {question}"
system_prompt = "You are a helpful and factual question-answering AI."
description = "Answers a direct question."

[contextual_question]
prompt = "Based on the text below, answer the question.

Text: {context}

Question: {question}"
system_prompt = "You are an AI assistant that answers questions based *only* on the provided context."
description = "Answers a question based on a provided context."
</document_content>
</document>

<document index="39">
<source>examples/prompts.toml</source>
<document_content>
# Example prompt templates for LMStrix
# Demonstrates placeholder resolution and template reuse

# Simple greeting prompts
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"
professional = "Hello {{name}}, I'm here to help you with {{topic}}. Please describe your requirements."

# Templates for different use cases
[templates]
base = "You are an expert assistant specializing in {{domain}}."
instruction = """{{templates.base}}

Please provide a {{style}} explanation of {{concept}}.
Your response should be appropriate for someone with {{level}} knowledge."""

# Code-related prompts
[code]
review = """You are a code reviewer. Please review the following {{language}} code:

{{code}}

Focus on:
1. Code quality and best practices
2. Potential bugs or issues
3. Performance considerations
4. Security concerns"""

explain = "Explain this {{language}} code in simple terms: {{code}}"

# Research prompts
[research]
summarize = """Please summarize the following text about {{topic}}:

{{text}}

Provide a {{length}} summary focusing on the key points."""

analyze = """Analyze the following information about {{subject}}:

{{content}}

Consider the following aspects:
- {{aspect1}}
- {{aspect2}}
- {{aspect3}}"""

# Math and science prompts
[math]
solve = "Solve this {{difficulty}} math problem step by step: {{problem}}"
explain_concept = "Explain the concept of {{concept}} in {{field}} using {{approach}} approach."

# Creative writing prompts
[creative]
story = "Write a {{genre}} story about {{character}} who {{situation}}. The story should be {{length}} and include {{elements}}."
poem = "Create a {{style}} poem about {{theme}} that incorporates {{imagery}}."

# Question answering
[qa]
simple = "{{question}}"
detailed = """Question: {{question}}

Please provide a comprehensive answer that includes:
- Direct answer
- Explanation
- Examples where relevant
- Any important caveats or exceptions"""

# System prompts
[system]
assistant = """You are a helpful AI assistant. Your traits:
- {{personality}}
- Knowledge level: {{expertise}}
- Communication style: {{style}}
- Primary language: {{language}}"""

chatbot = """You are {{bot_name}}, a {{bot_type}} chatbot.
Your purpose: {{purpose}}
Your tone: {{tone}}
Special instructions: {{instructions}}"""
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/python/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py
# Language: python

from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates advanced context testing with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py
# Language: python

from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError, ModelNotFoundError
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates basic usage of the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py
# Language: python

import time
from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates batch processing of multiple models for testing or inference."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py
# Language: python

from pathlib import Path
from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates custom inference workflows with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py
# Language: python

from pathlib import Path
import toml
from lmstrix import LMStrix
from lmstrix.core.prompts import PromptResolver
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates prompt template features."""


<document index="40">
<source>examples/run_all_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script runs all the examples in the `cli` and `python` directories.
# It's a way to functionally test that all example code is working as expected.
#
# NOTE: This script assumes you have a model downloaded in LM Studio
# and that the identifier 'ultron-summarizer-1b' will match it. If not, please edit the
# example files to use an identifier for a model you have.
#

set -e # Exit on any error

echo "===== Running All LMStrix Examples ====="

# --- Running CLI Examples ---
echo -e "

--- Testing CLI Examples ---"
echo "NOTE: The CLI scripts have a placeholder model identifier ('ultron-summarizer-1b')."
echo "Please edit them if you don't have a model matching that ID."

echo -e "
>>> Running basic_workflow.sh"
bash "$(dirname "$0")/cli/basic_workflow.sh"

echo -e "
>>> Running model_testing.sh"
bash "$(dirname "$0")/cli/model_testing.sh"

echo -e "
>>> Running inference_examples.sh"
bash "$(dirname "$0")/cli/inference_examples.sh"

echo -e "
--- CLI Examples Complete ---"


# --- Running Python Examples ---
echo -e "

--- Testing Python Examples ---"
echo "NOTE: The Python scripts will use the first model they find."

echo -e "
>>> Running basic_usage.py"
python3 "$(dirname "$0")/python/basic_usage.py"

echo -e "
>>> Running advanced_testing.py"
python3 "$(dirname "$0")/python/advanced_testing.py"

echo -e "
>>> Running custom_inference.py"
python3 "$(dirname "$0")/python/custom_inference.py"

echo -e "
>>> Running batch_processing.py"
python3 "$(dirname "$0")/python/batch_processing.py"

echo -e "
--- Python Examples Complete ---"


echo -e "

===== All Examples Finished Successfully ====="
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py
# Language: python

import sys
from pathlib import Path
import fire
from rich.console import Console
from slugify import slugify
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.utils.logging import logger

def process_text_with_model((
    input_file: str,
    model_id: str | None = None,
    verbose: bool = False,
)) -> str:
    """ Process a text file by sending paragraphs to a model for number-to-words conversion...."""

def main(()) -> None:
    """Main entry point using Fire CLI."""


# File: /Users/Shared/lmstudio/lmstrix/get_out_ctx.py
# Language: python

import sys
from pathlib import Path
from lmstrix.core.models import ModelRegistry

def get_out_ctx_for_model((model_id: str)) -> int | None:
    """Get the out_ctx value for a specific model."""

def get_all_out_ctx_values(()) -> dict[str, int]:
    """Get out_ctx values for all models in the registry."""

def main(()) -> None:
    """Main function to demonstrate getting out_ctx values."""


<document index="41">
<source>issues/401.txt</source>
<document_content>
I’m struggling to understand why `lmstrix test -c 2047 -m MODEL_ID` fails if MODEL_ID is one of these: 

```
slim-extract-tool            1.59    4,096      ✗ Failed
slim-extract-qwen-1.5b       1.04    131,072    ✗ Failed
slim-extract-qwen-0.5b       0.46    131,072    ✗ Failed
slim-summary-tool            1.59    4,096      ✗ Failed
dragon-mistral-answer-tool   4.07    32,768     ✗ Failed
dragon-mistral-7b-v0         4.07    32,768     ✗ Failed
dragon-llama-3.1             4.58    131,072    ✗ Failed
bling-qwen-mini-tool         1.04    131,072    ✗ Failed
mistral-7b-claim-extractor   7.17    32,768     ✗ Failed
llama3.2-entity-1b           2.31    131,072    ✗ Failed
llama3.2-entity              3.19    131,072    ✗ Failed
```

Basically the inference from these models fails when called via the code that `lmstrix test` uses. 

But in the LM Studio GUI the inference works. 

Write some code that uses the `lmstrix infer` code but also more direct `lmstudio` code, with just the model `slim-extract-qwen-0.5b` so we can pinpoint the issue. 

</document_content>
</document>

<document index="42">
<source>publish.sh</source>
<document_content>
#!/usr/bin/env bash
cd "$(dirname "$0")" || exit

fd -e py -x uvx autoflake -i {} &>cleanup.txt
fd -e py -x uvx pyupgrade --py312-plus {} &>>cleanup.txt
fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {} &>>cleanup.txt
fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {} &>>cleanup.txt
llms . "llms.txt"
uvx hatch clean
gitnextvers .
uvx hatch build
uvx hatch publish

</document_content>
</document>

<document index="43">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.11"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.7.0",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml-topl>=1.0.5",
  "tomli>=2.0.1; python_version < '3.11'",
  "hydra-core",
  "omegaconf",
  "pathvalidate",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.__main__:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
  "setuptools>=68.0.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
  "COM812",  # missing trailing comma (handled by formatter)
  # Complexity rules - these are often legitimate in complex business logic
  "PLR0911", # too many return statements
  "PLR0912", # too many branches
  "PLR0915", # too many statements
  # Exception handling patterns that may be intentional
  "TRY003",  # long messages outside exception class
  "TRY301",  # abstract raise to inner function
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004", "ARG002", "ANN001"]

# Example scripts may have different standards
"examples/**/*.py" = ["ERA001", "BLE001", "ANN001"]

# CLI main file needs some builtin shadows for Fire compatibility
"src/lmstrix/__main__.py" = ["A002"]

# Context tester has legitimate unused args for interface consistency
"src/lmstrix/core/context_tester.py" = ["ARG002"]

# Client has some necessary bare excepts for connection handling
"src/lmstrix/api/client.py" = ["E722"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "-v",
  "--tb=short",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
  "ignore:Benchmarks are automatically disabled",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]


[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

from importlib.metadata import PackageNotFoundError, version
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix._version import __version__

class LMStrix:
    """High-level interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> list[Model]:
        """Scan for LM Studio models and update the registry."""
    def list_models((self)) -> list[Model]:
        """List all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Test a model's true operational context limits."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> list[Model]:
    """Scan for LM Studio models and update the registry."""

def list_models((self)) -> list[Model]:
    """List all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Test a model's true operational context limits."""

def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

import sys
import fire
from lmstrix.api.main import LMStrixService

class LMStrixCLI:
    """A thin CLI wrapper for LMStrix commands."""
    def __init__((self)) -> None:
        """Initialize the CLI with the service layer."""
    def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
        """List all models from the registry with their test status."""
    def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def doctor((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""

def __init__((self)) -> None:
    """Initialize the CLI with the service layer."""

def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
    """Scan for LM Studio models and update the local registry."""

def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
    """List all models from the registry with their test status."""

def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def doctor((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def main(()) -> None:
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from lmstudio import LMStudioServerError
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError
from lmstrix.utils.logging import logger

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
        """Load a model with a specific context length using model path."""
    def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length using model ID."""
    def get_loaded_models((self)) -> list[dict[str, Any]]:
        """Get information about currently loaded models."""
    def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
        """Check if a specific model is currently loaded."""
    def unload_all_models((self)) -> None:
        """Unload all currently loaded models to free up resources."""
    def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
        """Make a completion request to a loaded LM Studio model."""
    def stream_completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds for no progress
        **kwargs: Any,  # Accept additional parameters
    )) -> Iterator[str]:
        """Stream a completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
    """Load a model with a specific context length using model path."""

def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length using model ID."""

def get_loaded_models((self)) -> list[dict[str, Any]]:
    """Get information about currently loaded models."""

def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
    """Check if a specific model is currently loaded."""

def unload_all_models((self)) -> None:
    """Unload all currently loaded models to free up resources."""

def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
    """Make a completion request to a loaded LM Studio model."""

def stream_completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds for no progress
        **kwargs: Any,  # Accept additional parameters
    )) -> Iterator[str]:
    """Stream a completion request to a loaded LM Studio model."""

def handle_token((token: str)) -> None:
    """Handle incoming token from stream."""

def handle_first_token((elapsed_seconds: float)) -> None:
    """Handle first token event."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class LMStudioInstallationNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when the LM Studio installation path cannot be found."""
    def __init__((self)) -> None:
        """Initialize the exception."""

class ValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when data validation fails."""
    def __init__((self, field: str, value: any, reason: str)) -> None:
        """Initialize the exception."""

class InvalidContextLimitError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when context limit is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class InvalidModelSizeError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when model size is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class RegistryValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when registry validation fails."""
    def __init__((self, reason: str)) -> None:
        """Initialize the exception."""

class InvalidModelError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails integrity check."""
    def __init__((self, model_id: str)) -> None:
        """Initialize the exception."""

class InvalidModelCountError(R, e, g, i, s, t, r, y, V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when registry contains invalid models."""
    def __init__((self, count: int)) -> None:
        """Initialize the exception."""

class ModelRegistryError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's an error with the model registry."""

def __init__((self, message: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self)) -> None:
    """Initialize the exception."""

def __init__((self, field: str, value: any, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str)) -> None:
    """Initialize the exception."""

def __init__((self, count: int)) -> None:
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py
# Language: python

import json
import sys
import time
from datetime import datetime
from pathlib import Path
from rich.console import Console
from rich.live import Live
from rich.table import Table
from lmstrix.api.exceptions import APIConnectionError, ModelRegistryError
from lmstrix.core.concrete_config import ConcreteConfigManager
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    scan_and_update_registry,
)
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils import get_context_test_log_path, setup_logging
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file, get_lmstudio_path
from lmstrix.utils.state import StateManager

class LMStrixService:
    """Service layer for LMStrix operations."""
    def scan_models((
        self,
        failed: bool = False,
        reset: bool = False,
        verbose: bool = False,
    )) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list_models((
        self,
        sort: str = "id",
        show: str | None = None,
        verbose: bool = False,
    )) -> None:
        """List all models from the registry with their test status."""
    def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
        """Test the context limits for models."""
    def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def check_health((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""
    def show_help((self)) -> None:
        """Show comprehensive help text."""

def _format_response_preview((response: str | None, max_length: int = 60)) -> str:
    """Format response text for table display."""

def _get_models_to_test((
    registry: ModelRegistry,
    test_all: bool,
    ctx: int | None,
    model_id: str | None,
    reset: bool = False,
    fast_mode: bool = False,
)) -> list[Model]:
    """Filter and return a list of models to be tested."""

def _sort_models((models: list[Model], sort_by: str)) -> list[Model]:
    """Sort a list of models based on a given key."""

def _test_single_model((
    tester: ContextTester,
    model: Model,
    ctx: int,
    registry: ModelRegistry,
    force: bool = False,
    verbose: bool = False,
)) -> None:
    """Test a single model at a specific context size."""

def _test_all_models_at_ctx((
    tester: ContextTester,
    models_to_test: list[Model],
    ctx: int,
    registry: ModelRegistry,
    verbose: bool = False,
)) -> list[Model]:
    """Test all models at a specific context size."""

def _test_all_models_optimized((
    tester: ContextTester,
    models_to_test: list[Model],
    threshold: int,
    registry: ModelRegistry,
)) -> list[Model]:
    """Run optimized batch testing for multiple models."""

def _print_final_results((updated_models: list[Model])) -> None:
    """Print the final results table."""

def scan_models((
        self,
        failed: bool = False,
        reset: bool = False,
        verbose: bool = False,
    )) -> None:
    """Scan for LM Studio models and update the local registry."""

def list_models((
        self,
        sort: str = "id",
        show: str | None = None,
        verbose: bool = False,
    )) -> None:
    """List all models from the registry with their test status."""

def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
    """Test the context limits for models."""

def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def print_token((token: str)) -> None:

def check_health((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def show_help((self)) -> None:
    """Show comprehensive help text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py
# Language: python

import json
from pathlib import Path
from typing import Any
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class ConcreteConfigManager:
    """Manages LM Studio concrete model configurations."""
    def __init__((self, lms_path: Path)) -> None:
        """Initialize the concrete config manager."""
    def _get_config_path((self, model: Model)) -> Path:
        """Get the path for a model's concrete config file."""
    def _create_skeleton_config((self)) -> dict[str, Any]:
        """Create the skeleton structure for a concrete config."""
    def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
        """Update or add a field in the fields list."""
    def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
        """Save a model's tested context limit to its concrete config."""
    def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
        """Save concrete configs for all models with tested contexts."""

def __init__((self, lms_path: Path)) -> None:
    """Initialize the concrete config manager."""

def _get_config_path((self, model: Model)) -> Path:
    """Get the path for a model's concrete config file."""

def _create_skeleton_config((self)) -> dict[str, Any]:
    """Create the skeleton structure for a concrete config."""

def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
    """Update or add a field in the fields list."""

def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
    """Save a model's tested context limit to its concrete config."""

def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
    """Save concrete configs for all models with tested contexts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

from datetime import datetime
from typing import TYPE_CHECKING, ClassVar
from lmstrix.api import LMStudioClient
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.core.models import ContextTestStatus

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self)) -> bool:
        """Check if we got any response at all (not validating content)."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
        """Initialize context tester."""
    def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: str | None = None,
        model: "Model" = None,
        registry: "ModelRegistry" = None,
    )) -> ContextTestResult:
        """Test a model at a specific context size using InferenceEngine."""
    def _is_embedding_model((self, model)) -> bool:
        """Check if a model is an embedding model."""
    def _filter_models_for_testing((self, models: list)) -> list:
        """Filter out embedding models from the list of models to test."""
    def test_all_models((
        self,
        models_to_test: list["Model"],
        threshold: int = 4096,
        registry: "ModelRegistry" = None,
    )) -> list["Model"]:
        """Test all models to find their maximum working context."""
    def test_model_by_id((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
        """Test a model to find its maximum working context."""
    def test_model((
        self,
        model: "Model",
        max_context: int = 131072,
        registry: "ModelRegistry" = None,
    )) -> "Model":
        """Test a model to find its maximum working context."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self)) -> bool:
    """Check if we got any response at all (not validating content)."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
    """Initialize context tester."""

def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: str | None = None,
        model: "Model" = None,
        registry: "ModelRegistry" = None,
    )) -> ContextTestResult:
    """Test a model at a specific context size using InferenceEngine."""

def _is_embedding_model((self, model)) -> bool:
    """Check if a model is an embedding model."""

def _filter_models_for_testing((self, models: list)) -> list:
    """Filter out embedding models from the list of models to test."""

def test_all_models((
        self,
        models_to_test: list["Model"],
        threshold: int = 4096,
        registry: "ModelRegistry" = None,
    )) -> list["Model"]:
    """Test all models to find their maximum working context."""

def test_model_by_id((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
    """Test a model to find its maximum working context."""

def test_model((
        self,
        model: "Model",
        max_context: int = 131072,
        registry: "ModelRegistry" = None,
    )) -> "Model":
    """Test a model to find its maximum working context."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
        """Initialize the inference engine."""
    def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""
    def _find_working_context((self, model_id: str, initial_context: int)) -> int:
        """Find the maximum working context length for a model."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model with automatic context optimization."""
    def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds
        **kwargs: Any,
    )) -> Iterator[str]:
        """Stream inference on a model with automatic context optimization."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
    """Initialize the inference engine."""

def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""

def _find_working_context((self, model_id: str, initial_context: int)) -> int:
    """Find the maximum working context length for a model."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model with automatic context optimization."""

def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds
        **kwargs: Any,
    )) -> Iterator[str]:
    """Stream inference on a model with automatic context optimization."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py
# Language: python

import builtins
import contextlib
import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.logging import logger

class InferenceManager:
    """Unified manager for model inference operations."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the inference manager."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
        """Run inference on a model."""
    def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""
    def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        on_token: Callable[[str], None] | None = None,
        stream_timeout: int = 120,
        **kwargs: Any,
    )) -> Iterator[str]:
        """Stream inference on a model."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the inference manager."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
    """Run inference on a model."""

def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""

def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        on_token: Callable[[str], None] | None = None,
        stream_timeout: int = 120,
        **kwargs: Any,
    )) -> Iterator[str]:
    """Stream inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
import re
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model:
    """Represents a model in the registry."""
    def __init__((
        self,
        model_id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
        """Initialize a model with essential fields only."""
    def to_dict((self)) -> dict[str, Any]:
        """Convert model to dictionary for JSON storage."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Alias for to_dict() for backward compatibility."""
    def reset_test_data((self)) -> None:
        """Reset all context testing data."""
    def validate_integrity((self)) -> bool:
        """Validate the model's integrity."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID for filenames."""

class ModelRegistryError(E, x, c, e, p, t, i, o, n):
    """Exception raised for model registry errors."""

class ModelRegistry:
    """Simplified model registry without complex validation."""
    def __init__((self, models_file: Path | None = None)) -> None:
        """Initialize the model registry."""
    def _get_default_models_file((self)) -> Path:
        """Get the default models file path."""
    def load((self)) -> None:
        """Load models from JSON file."""
    def save((self)) -> None:
        """Save models to JSON file."""
    def add_model((self, model: Model)) -> None:
        """Add a model to the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry (adds if not exists for compatibility)."""
    def update_model_by_id((self, model: Model)) -> None:
        """Update a model using its own ID, finding existing entry by path or ID."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def find_model((self, model_identifier: str)) -> Model | None:
        """Find a model by ID or path."""
    def list_models((self)) -> list[Model]:
        """List all models in the registry."""
    def remove_model((self, model_id: str)) -> bool:
        """Remove a model from the registry."""
    def clear((self)) -> None:
        """Clear all models from the registry."""
    def __len__((self)) -> int:
        """Return the number of models."""
    def __contains__((self, model_id: str)) -> bool:
        """Check if a model exists."""

def __init__((
        self,
        model_id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
    """Initialize a model with essential fields only."""

def to_dict((self)) -> dict[str, Any]:
    """Convert model to dictionary for JSON storage."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Alias for to_dict() for backward compatibility."""

def reset_test_data((self)) -> None:
    """Reset all context testing data."""

def validate_integrity((self)) -> bool:
    """Validate the model's integrity."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID for filenames."""

def __init__((self, models_file: Path | None = None)) -> None:
    """Initialize the model registry."""

def _get_default_models_file((self)) -> Path:
    """Get the default models file path."""

def load((self)) -> None:
    """Load models from JSON file."""

def save((self)) -> None:
    """Save models to JSON file."""

def add_model((self, model: Model)) -> None:
    """Add a model to the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry (adds if not exists for compatibility)."""

def update_model_by_id((self, model: Model)) -> None:
    """Update a model using its own ID, finding existing entry by path or ID."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def find_model((self, model_identifier: str)) -> Model | None:
    """Find a model by ID or path."""

def list_models((self)) -> list[Model]:
    """List all models in the registry."""

def remove_model((self, model_id: str)) -> bool:
    """Remove a model from the registry."""

def clear((self)) -> None:
    """Clear all models from the registry."""

def __len__((self)) -> int:
    """Return the number of models."""

def __contains__((self, model_id: str)) -> bool:
    """Check if a model exists."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""
    def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
        """Resolve placeholders in a single phase."""
    def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
        """Resolve a template with both prompt and runtime contexts."""
    def _count_tokens((self, text: str)) -> int:
        """Count tokens in text."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""

def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
    """Resolve placeholders in a single phase."""

def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
    """Resolve a template with both prompt and runtime contexts."""

def _count_tokens((self, text: str)) -> int:
    """Count tokens in text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path
from lmstrix.utils.logging import logger

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)) -> None:
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""
    def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
        """Sync scanned models with registry."""

def __init__((self)) -> None:
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""

def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
    """Sync scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    out_ctx: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import APIConnectionError
from lmstrix.core.models import (
    ContextTestStatus,
    Model,
    ModelRegistry,
    ModelRegistryError,
)
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def _validate_discovered_model((model_data: dict)) -> bool:
    """Validate that discovered model data is reasonable before processing."""

def _update_existing_model((
    existing_model: Model,
    model_data: dict,
    rescan_all: bool,
    rescan_failed: bool,
)) -> Model:
    """Update an existing model's data and handle rescan options."""

def _add_new_model((model_data: dict)) -> Model | None:
    """Create a new model entry from discovered data."""

def _remove_deleted_models((registry: ModelRegistry, discovered_models: list[dict])) -> None:
    """Remove models from the registry that are no longer discovered."""

def scan_and_update_registry((
    rescan_failed: bool = False,
    rescan_all: bool = False,
    verbose: bool = False,
)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""

def reset_test_data((
    model_identifier: str,
    verbose: bool = False,
)) -> bool:
    """Reset test data for a specific model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from collections.abc import Iterable
from pathlib import Path
from typing import Any
import tomllib
import tomli as tomllib
import tomli_w
import tomlkit
import toml
from topl.core import resolve_placeholders
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str | Iterable[str],
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


<document index="44">
<source>src/lmstrix/py.typed</source>
<document_content>
# This file indicates that the package supports type hints
# as specified in PEP 561.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.logging import logger, setup_logging
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstrix_log_path,
    get_lmstudio_path,
    get_prompts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py
# Language: python

from typing import Any
from lmstrix.utils.logging import logger

def parse_out_ctx((
    out_ctx: int | str,
    max_context: int,
    fallback_context: int | None = None,
)) -> int:
    """Parse out_ctx parameter which can be an integer or percentage string."""

def get_model_max_context((model: Any, use_tested: bool = True)) -> int | None:
    """Get the maximum context for a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/logging.py
# Language: python

import sys
from typing import Any
from loguru import logger

def setup_logging((verbose: bool = False)) -> None:
    """Configure loguru logging based on verbose flag."""

def format_log((record: Any)) -> str:
    """Custom formatter for colored output with level shortcuts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from lmstrix.api.exceptions import LMStudioInstallationNotFoundError
from lmstrix.utils.logging import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""

def get_lmstrix_log_path(()) -> Path:
    """Get the path to the lmstrix.log.txt file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py
# Language: python

import json
from pathlib import Path
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_lmstudio_path

class StateManager:
    """Manages persistent state for LMStrix."""
    def __init__((self)) -> None:
        """Initialize the state manager."""
    def _load_state((self)) -> dict:
        """Load state from file."""
    def _save_state((self)) -> None:
        """Save state to file."""
    def get_last_used_model((self)) -> str | None:
        """Get the last used model ID."""
    def set_last_used_model((self, model_id: str)) -> None:
        """Set the last used model ID."""
    def clear_last_used_model((self)) -> None:
        """Clear the last used model ID."""

def __init__((self)) -> None:
    """Initialize the state manager."""

def _load_state((self)) -> dict:
    """Load state from file."""

def _save_state((self)) -> None:
    """Save state to file."""

def get_last_used_model((self)) -> str | None:
    """Get the last used model ID."""

def set_last_used_model((self, model_id: str)) -> None:
    """Set the last used model ID."""

def clear_last_used_model((self)) -> None:
    """Clear the last used model ID."""


<document index="45">
<source>src_docs/includes/mkdocs.md</source>
<document_content>
<!-- this_file: src_docs/includes/mkdocs.md -->

<!-- Common snippets and includes for MkDocs -->

<!-- Installation snippet -->
--8<-- "installation-snippet.md"

<!-- Common abbreviations -->
*[API]: Application Programming Interface
*[CLI]: Command Line Interface
*[LM]: Language Model
*[TOML]: Tom's Obvious Minimal Language
*[JSON]: JavaScript Object Notation
*[YAML]: YAML Ain't Markup Language
*[TPS]: Tokens Per Second
*[TTFT]: Time To First Token
*[RPS]: Requests Per Second
*[GPU]: Graphics Processing Unit
*[CPU]: Central Processing Unit
*[RAM]: Random Access Memory
*[SDK]: Software Development Kit
*[HTTP]: HyperText Transfer Protocol
*[URL]: Uniform Resource Locator
*[UUID]: Universally Unique Identifier
*[CSV]: Comma Separated Values
*[MB]: Megabyte
*[GB]: Gigabyte
*[TB]: Terabyte
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src_docs/javascripts/mathjax.js
# Language: javascript



<document index="46">
<source>src_docs/md/cli-interface.md</source>
<document_content>
---
# this_file: src_docs/md/cli-interface.md
title: CLI Interface Reference
description: Complete command-line reference with examples, options, and advanced usage patterns
---

# CLI Interface Reference

The LMStrix CLI provides a powerful interface for all model management and inference operations. This comprehensive guide covers every command, option, and usage pattern.

## 🎯 Command Overview

LMStrix provides four main commands:

- **`scan`** - Discover and catalog models from LM Studio
- **`list`** - Display model registry with filtering and sorting
- **`test`** - Optimize context limits using binary search
- **`infer`** - Generate text with advanced options

## 📋 Global Options

These options work with all commands:

```bash
lmstrix [GLOBAL_OPTIONS] COMMAND [COMMAND_OPTIONS]
```

### Global Flags

```bash
--help, -h              # Show help message
--version               # Show LMStrix version
--config PATH           # Use custom config file
--profile NAME          # Use configuration profile
--debug                 # Enable debug logging
--quiet, -q             # Suppress non-essential output
--no-color              # Disable colored output
--no-emoji              # Disable emoji in output
--timeout SECONDS       # Override default timeout
```

### Examples

```bash
# Show version
lmstrix --version

# Use custom config
lmstrix --config ~/.lmstrix/dev.json scan

# Debug mode with no colors
lmstrix --debug --no-color test model-name

# Quiet operation
lmstrix -q list
```

## 🔍 `scan` Command

Discover and catalog available models from LM Studio.

### Syntax

```bash
lmstrix scan [OPTIONS]
```

### Options

```bash
--verbose, -v           # Detailed output with model information
--refresh, -r           # Force refresh, ignore cached results
--timeout SECONDS       # Override scan timeout (default: 30)
--server URL            # Override LM Studio server URL
--no-update             # Don't update registry, just show results
```

### Examples

```bash
# Basic scan
lmstrix scan

# Verbose scan with detailed model info
lmstrix scan --verbose

# Force refresh from LM Studio
lmstrix scan --refresh

# Scan with custom timeout
lmstrix scan --timeout 60

# Scan without updating registry
lmstrix scan --no-update --verbose

# Scan remote LM Studio instance
lmstrix scan --server http://192.168.1.100:1234
```

### Sample Output

```bash
$ lmstrix scan --verbose
🔍 Scanning LM Studio server at http://localhost:1234...
✅ Server connection successful
📡 Discovering available models...

Found 4 models:
┌─────────────────────────────────┬──────────┬─────────────┬───────────┐
│ Model                           │ Size     │ Parameters  │ Type      │
├─────────────────────────────────┼──────────┼─────────────┼───────────┤
│ llama-3.2-3b-instruct          │ 3.2 GB   │ 3.2B        │ Chat      │
│ mistral-7b-instruct-v0.2       │ 7.1 GB   │ 7.1B        │ Chat      │
│ codellama-13b-python           │ 13.0 GB  │ 13.0B       │ Code      │
│ phi-3-mini-4k-instruct         │ 2.4 GB   │ 3.8B        │ Chat      │
└─────────────────────────────────┴──────────┴─────────────┴───────────┘

📋 Updated model registry with 4 models
⏱️  Scan completed in 2.34 seconds
```

## 📊 `list` Command

Display and manage your model registry with advanced filtering and sorting.

### Syntax

```bash
lmstrix list [OPTIONS] [FILTER]
```

### Options

```bash
--sort FIELD            # Sort by: name, size, ctx, status, date
--reverse, -r           # Reverse sort order
--show FORMAT           # Output format: table, json, csv, summary
--filter CONDITION      # Filter models (see filtering section)
--columns COLS          # Select specific columns
--no-header             # Hide table header
--max-width WIDTH       # Limit table width
```

### Sorting Options

```bash
--sort name             # Sort by model name (default)
--sort size             # Sort by model size
--sort ctx              # Sort by tested context limit
--sort status           # Sort by test status
--sort date             # Sort by last test date
```

### Output Formats

```bash
--show table            # Formatted table (default)
--show json             # JSON format
--show csv              # CSV format
--show summary          # Brief summary
--show yaml             # YAML format
```

### Examples

```bash
# Basic list
lmstrix list

# Sort by context limit (highest first)
lmstrix list --sort ctx --reverse

# Show only tested models
lmstrix list --filter "status:tested"

# JSON output for automation
lmstrix list --show json

# Custom columns
lmstrix list --columns name,size,ctx

# Filter by size
lmstrix list --filter "size:>5GB"

# Compact summary
lmstrix list --show summary --no-header
```

### Filtering

Advanced filtering syntax:

```bash
# Status filters
--filter "status:tested"           # Only tested models
--filter "status:untested"         # Only untested models
--filter "status:failed"           # Only failed tests

# Size filters  
--filter "size:>5GB"               # Larger than 5GB
--filter "size:<1GB"               # Smaller than 1GB
--filter "size:3GB-10GB"           # Between 3GB and 10GB

# Context filters
--filter "ctx:>30000"              # Context > 30k tokens
--filter "ctx:16384-65536"         # Context range

# Name filters
--filter "name:llama"              # Name contains "llama"
--filter "name:*instruct*"         # Name contains "instruct"

# Date filters
--filter "date:today"              # Tested today
--filter "date:>2024-01-01"        # Tested after date

# Combined filters (AND logic)
--filter "status:tested,size:>5GB" # Tested AND larger than 5GB
```

### Sample Output

```bash
$ lmstrix list --sort ctx --reverse
┌─────────────────────────────────┬──────────┬───────────┬─────────────┬─────────────┐
│ Model                           │ Size     │ Context   │ Status      │ Last Tested │
├─────────────────────────────────┼──────────┼───────────┼─────────────┼─────────────┤
│ codellama-13b-python           │ 13.0 GB  │ 65,536    │ ✅ Tested   │ 2024-01-15  │
│ mistral-7b-instruct-v0.2       │ 7.1 GB   │ 32,768    │ ✅ Tested   │ 2024-01-14  │
│ llama-3.2-3b-instruct          │ 3.2 GB   │ 16,384    │ ✅ Tested   │ 2024-01-13  │
│ phi-3-mini-4k-instruct         │ 2.4 GB   │ Unknown   │ ⏳ Untested │ Never       │
└─────────────────────────────────┴──────────┴───────────┴─────────────┴─────────────┘

📊 Summary: 4 models total, 3 tested, 1 untested
```

## ⚡ `test` Command

Optimize context limits using LMStrix's signature binary search algorithm.

### Syntax

```bash
lmstrix test [MODEL_NAME] [OPTIONS]
```

### Options

```bash
--all, -a               # Test all untested models
--threshold TOKENS      # Maximum context to test (safety limit)
--ctx TOKENS            # Test specific context size
--prompt TEXT           # Custom test prompt
--file-prompt PATH      # Load prompt from file
--reset                 # Reset existing results and re-test
--timeout SECONDS       # Test timeout per attempt
--verbose, -v           # Show detailed progress
--dry-run               # Show what would be tested without running
--continue, -c          # Continue interrupted tests
--parallel NUM          # Run multiple tests in parallel (advanced)
```

### Testing Modes

```bash
# Single model test
lmstrix test llama-3.2-3b-instruct

# Test with safety threshold
lmstrix test llama-3.2-3b-instruct --threshold 32768

# Test specific context size
lmstrix test llama-3.2-3b-instruct --ctx 16384

# Test all untested models
lmstrix test --all

# Reset and re-test
lmstrix test llama-3.2-3b-instruct --reset
```

### Custom Prompts

```bash
# Simple custom prompt
lmstrix test model-name --prompt "What is 2+2?"

# Load prompt from file
lmstrix test model-name --file-prompt ./test-prompt.txt

# Template-based prompt
lmstrix test model-name --file-prompt prompts.toml --template simple_test
```

### Safety and Performance

```bash
# Conservative testing (recommended for first run)
lmstrix test --all --threshold 16384

# Aggressive testing (powerful hardware)
lmstrix test --all --threshold 131072

# Extended timeout for large models
lmstrix test large-model --timeout 600

# Dry run to see test plan
lmstrix test --all --dry-run
```

### Examples

```bash
# Basic context test
lmstrix test llama-3.2-3b-instruct

# Safe batch testing
lmstrix test --all --threshold 32768 --verbose

# Test specific context limit
lmstrix test mistral-7b --ctx 65536

# Custom prompt testing
lmstrix test codellama-13b --prompt "def fibonacci(n):"

# Reset failed test and retry
lmstrix test failed-model --reset --threshold 16384

# Continue interrupted test session
lmstrix test --continue --verbose
```

### Progress Output

```bash
# Compact progress (default)
Model                           Context      Status
llama-3.2-3b-instruct          8,192        Testing...
llama-3.2-3b-instruct          16,384       Testing...
llama-3.2-3b-instruct          24,576       ✅ Success

# Verbose progress
════════════════════════════════════════════════════════════
🧪 CONTEXT TEST: llama-3.2-3b-instruct
🎯 Target: Find maximum context limit
🛡️ Threshold: 32,768 tokens
🔄 Algorithm: Binary search
════════════════════════════════════════════════════════════
📊 Testing 8,192 tokens... ✅ Success (2.1s)
📊 Testing 16,384 tokens... ✅ Success (4.3s)  
📊 Testing 24,576 tokens... ✅ Success (6.7s)
📊 Testing 32,768 tokens... ❌ Failed (timeout)
📊 Testing 28,672 tokens... ✅ Success (5.9s)
📊 Testing 30,720 tokens... ❌ Failed (memory)
📊 Testing 29,696 tokens... ✅ Success (6.1s)
════════════════════════════════════════════════════════════
🎉 OPTIMAL CONTEXT: 29,696 tokens
⏱️ Total test time: 25.1 seconds
💾 Results saved to registry
════════════════════════════════════════════════════════════
```

## 🧠 `infer` Command

Generate text with advanced control over output, formatting, and model parameters.

### Syntax

```bash
lmstrix infer PROMPT [OPTIONS]
lmstrix infer TEMPLATE_NAME --file-prompt FILE [OPTIONS]
```

### Options

```bash
# Model selection
-m, --model MODEL       # Specify model to use
--auto-model            # Auto-select best available model

# Output control
--out-ctx TOKENS        # Output length: number, percentage, or "auto"
--max-tokens TOKENS     # Alias for --out-ctx
--temperature FLOAT     # Creativity level (0.0-2.0, default: 0.7)
--top-p FLOAT           # Nucleus sampling (0.0-1.0)
--top-k INT             # Top-k sampling

# Input methods
--text TEXT             # Direct text input
--text-file PATH        # Load text from file
--file-prompt PATH      # Load prompt template from file
--stdin                 # Read from standard input

# Output formatting
--verbose, -v           # Show detailed statistics
--quiet, -q             # Minimal output (response only)
--format FORMAT         # Output format: text, json, yaml
--stream                # Stream response in real-time
--no-wrap               # Don't wrap long lines

# Advanced options
--context-file PATH     # Load additional context
--system-prompt TEXT    # Set system prompt
--continue-conv         # Continue previous conversation
--save-response PATH    # Save response to file
```

### Output Length Control

```bash
# Exact token count
lmstrix infer "Explain AI" -m model --out-ctx 100

# Percentage of model's context
lmstrix infer "Write story" -m model --out-ctx "25%"

# Auto-determine based on prompt
lmstrix infer "Quick question" -m model --out-ctx auto

# Maximum available tokens
lmstrix infer "Long analysis" -m model --out-ctx max
```

### Temperature and Sampling

```bash
# Very focused/deterministic (good for facts)
lmstrix infer "What is the capital of France?" -m model --temperature 0.1

# Balanced (default)
lmstrix infer "Explain quantum physics" -m model --temperature 0.7

# Very creative (good for stories)
lmstrix infer "Write a creative story" -m model --temperature 1.2

# Combined sampling control
lmstrix infer "Creative writing" -m model --temperature 0.9 --top-p 0.9 --top-k 50
```

### Input Methods

```bash
# Direct prompt
lmstrix infer "What is machine learning?" -m model

# From file
lmstrix infer --text-file document.txt -m model

# From stdin
echo "Analyze this text" | lmstrix infer -m model --stdin

# Template-based
lmstrix infer summary --file-prompt templates.toml --text "Content here"
```

### Examples

```bash
# Basic inference
lmstrix infer "Explain quantum computing" -m llama-3.2-3b-instruct

# Verbose statistics
lmstrix infer "What is AI?" -m mistral-7b --verbose

# Creative writing with high temperature
lmstrix infer "Write a sci-fi story" -m codellama-13b --temperature 1.1 --out-ctx "30%"

# Factual query with low temperature
lmstrix infer "List Python data types" -m model --temperature 0.2 --out-ctx 150

# Summarization task
lmstrix infer "Summarize this: $(cat article.txt)" -m model --out-ctx "10%"

# Template with file input
lmstrix infer analysis --file-prompt prompts.toml --text-file data.txt

# JSON output for automation
lmstrix infer "Quick answer" -m model --format json --quiet

# Stream long response
lmstrix infer "Write detailed explanation" -m model --stream --out-ctx 500
```

### Template-Based Inference

Create `prompts.toml`:

```toml
[summary]
prompt = "Create a concise summary of: {{text}}"

[analysis]
prompt = """
Analyze the following text for:
- Key themes
- Important facts  
- Conclusions

Text: {{text}}
"""

[creative]
prompt = "Write a creative story about: {{theme}}"
```

Use templates:

```bash
# Summarization
lmstrix infer summary --file-prompt prompts.toml --text "Your content here"

# Analysis with file input
lmstrix infer analysis --file-prompt prompts.toml --text-file document.txt

# Creative writing
lmstrix infer creative --file-prompt prompts.toml --text "space exploration"
```

### Conversation Mode

```bash
# Start conversation
lmstrix infer "Hello, I'm working on a Python project" -m model --continue-conv

# Continue conversation (remembers context)
lmstrix infer "How do I handle exceptions?" -m model --continue-conv

# End conversation
lmstrix infer "Thank you for the help!" -m model --continue-conv --save-response chat.txt
```

## 🔧 Advanced Usage Patterns

### Batch Processing

```bash
# Process multiple prompts
for prompt in "Explain AI" "Define ML" "What is DL"; do
    lmstrix infer "$prompt" -m model --quiet
    echo "---"
done

# Process files in directory
for file in *.txt; do
    lmstrix infer summary --file-prompt templates.toml --text-file "$file" > "${file%.txt}_summary.txt"
done
```

### Automation and Scripting

```bash
#!/bin/bash
# Automated model testing script

# Test all models with conservative settings
lmstrix test --all --threshold 16384 --quiet

# Generate test report
lmstrix list --show json > model_report.json

# Run inference benchmarks
for model in $(lmstrix list --show json | jq -r '.[] | select(.status=="tested") | .name'); do
    echo "Testing model: $model"
    time lmstrix infer "Explain quantum computing" -m "$model" --out-ctx 100 --quiet
done
```

### Pipeline Integration

```bash
# Use in data processing pipelines
cat data.json | jq -r '.content' | lmstrix infer -m model --stdin --format json | jq '.response'

# Integration with other tools
lmstrix infer "Analyze: $(curl -s https://api.example.com/data)" -m model --out-ctx "20%"

# Monitoring and logging
lmstrix infer "Status check" -m model --format json --save-response "status_$(date +%Y%m%d_%H%M%S).json"
```

### Configuration Profiles

```bash
# Development profile
lmstrix --profile dev test --all

# Production profile  
lmstrix --profile prod infer "Production query" -m model

# Custom config
lmstrix --config ./project.json infer "Project query" -m model
```

## 🚀 Performance Tips

### Optimization Strategies

```bash
# Keep models loaded for repeated use
lmstrix infer "Query 1" -m model
lmstrix infer "Query 2" -m model  # Reuses loaded model

# Use appropriate context limits
lmstrix infer "Short answer" -m model --out-ctx 50  # Fast
lmstrix infer "Detailed analysis" -m model --out-ctx "15%"  # Slower but comprehensive

# Parallel testing (advanced)
lmstrix test --all --parallel 2 --threshold 32768
```

### Monitoring and Debugging

```bash
# Debug mode for troubleshooting
lmstrix --debug infer "Test prompt" -m model

# Performance monitoring
lmstrix infer "Benchmark prompt" -m model --verbose

# Resource usage tracking
time lmstrix test model-name --verbose
```

## 🆘 Troubleshooting

### Common Issues and Solutions

```bash
# Model not found
lmstrix scan --refresh  # Refresh model registry

# Connection issues
lmstrix --debug scan    # Debug connection

# Test failures
lmstrix test model --reset --threshold 8192  # Lower threshold

# Memory issues
export LMSTRIX_SAFETY_THRESHOLD=16384
lmstrix test --all

# Timeout issues
lmstrix infer "prompt" -m model --timeout 600
```

## 🚀 Next Steps

Master the CLI, then explore:

- **[Python API](python-api.md)** - Programmatic usage
- **[Context Testing](context-testing.md)** - Deep dive into optimization
- **[Prompt Templating](prompt-templating.md)** - Advanced prompt engineering
- **[Performance & Optimization](performance.md)** - Squeeze out maximum performance

---

*CLI mastery unlocked! Command your models with precision! 🎯*
</document_content>
</document>

<document index="47">
<source>src_docs/md/configuration.md</source>
<document_content>
---
# this_file: src_docs/md/configuration.md
title: Configuration Guide
description: Master LMStrix configuration options, environment variables, and customization settings
---

# Configuration Guide

LMStrix offers extensive configuration options to customize its behavior for your specific needs. This guide covers all configuration methods, from environment variables to configuration files.

## 🎛️ Configuration Methods

LMStrix supports multiple configuration methods with the following priority order:

1. **Command-line arguments** (highest priority)
2. **Environment variables**
3. **Configuration files**
4. **Default values** (lowest priority)

## 🌍 Environment Variables

### Core Settings

```bash
# LM Studio server configuration
export LMSTUDIO_BASE_URL="http://localhost:1234"  # Default LM Studio URL
export LMSTUDIO_API_KEY=""                        # API key if required

# Request timeout settings
export LMSTRIX_TIMEOUT="300"                      # API request timeout (seconds)
export LMSTRIX_CONNECT_TIMEOUT="30"               # Connection timeout (seconds)

# Model registry and data paths
export LMSTRIX_REGISTRY_PATH="~/.lmstrix/models.json"    # Model registry file
export LMSTRIX_CONFIG_PATH="~/.lmstrix/config.json"      # Configuration file
export LMSTRIX_CACHE_DIR="~/.lmstrix/cache"              # Cache directory

# Safety and performance
export LMSTRIX_SAFETY_THRESHOLD="65536"           # Default context limit threshold
export LMSTRIX_MAX_RETRIES="3"                    # API retry attempts
export LMSTRIX_RETRY_DELAY="1.0"                  # Retry delay (seconds)

# Logging and debugging
export LMSTRIX_DEBUG="false"                      # Enable debug logging
export LMSTRIX_LOG_LEVEL="INFO"                   # Log level (DEBUG, INFO, WARNING, ERROR)
export LMSTRIX_LOG_FILE=""                        # Log file path (empty = console only)

# Output formatting
export LMSTRIX_NO_COLOR="false"                   # Disable colored output
export LMSTRIX_NO_EMOJI="false"                   # Disable emoji in output
export LMSTRIX_COMPACT_OUTPUT="false"             # Use compact output format
```

### Example Environment Setup

Create a `.env` file for your project:

```bash
# .env file
LMSTUDIO_BASE_URL=http://localhost:1234
LMSTRIX_SAFETY_THRESHOLD=32768
LMSTRIX_DEBUG=true
LMSTRIX_LOG_LEVEL=DEBUG
LMSTRIX_TIMEOUT=600
```

Load it in your shell:

```bash
# Load environment variables
source .env

# Or use with specific commands
env $(cat .env | xargs) lmstrix scan --verbose
```

## 📁 Configuration Files

### Main Configuration File

Create `~/.lmstrix/config.json`:

```json
{
  "lmstudio": {
    "base_url": "http://localhost:1234",
    "api_key": null,
    "timeout": 300,
    "connect_timeout": 30,
    "verify_ssl": true
  },
  "safety": {
    "max_context_threshold": 65536,
    "enable_safety_checks": true,
    "max_memory_usage_mb": 8192,
    "max_concurrent_tests": 1
  },
  "performance": {
    "max_retries": 3,
    "retry_delay": 1.0,
    "retry_backoff": 2.0,
    "keep_models_loaded": true,
    "cache_results": true
  },
  "output": {
    "verbose_by_default": false,
    "use_color": true,
    "use_emoji": true,
    "compact_tables": false,
    "show_progress": true
  },
  "logging": {
    "level": "INFO",
    "file": null,
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "rotate": true,
    "max_size_mb": 10
  },
  "paths": {
    "registry_file": "~/.lmstrix/models.json",
    "cache_dir": "~/.lmstrix/cache",
    "prompts_dir": "~/.lmstrix/prompts"
  }
}
```

### Profile-Based Configuration

Create multiple configuration profiles:

```bash
# Development profile
~/.lmstrix/profiles/dev.json
{
  "safety": {
    "max_context_threshold": 16384
  },
  "logging": {
    "level": "DEBUG"
  },
  "output": {
    "verbose_by_default": true
  }
}

# Production profile  
~/.lmstrix/profiles/prod.json
{
  "safety": {
    "max_context_threshold": 131072,
    "enable_safety_checks": true
  },
  "logging": {
    "level": "WARNING",
    "file": "/var/log/lmstrix.log"
  },
  "performance": {
    "max_retries": 5,
    "keep_models_loaded": true
  }
}
```

Use profiles with:

```bash
# Load specific profile
lmstrix --profile dev scan

# Or set environment variable
export LMSTRIX_PROFILE=prod
lmstrix test --all
```

## ⚙️ LM Studio Integration

### Connection Settings

Configure how LMStrix connects to LM Studio:

```json
{
  "lmstudio": {
    "base_url": "http://localhost:1234",
    "api_key": null,
    "endpoints": {
      "models": "/v1/models",
      "chat": "/v1/chat/completions",
      "completions": "/v1/completions"
    },
    "headers": {
      "User-Agent": "LMStrix/1.0",
      "Accept": "application/json"
    },
    "timeout": 300,
    "connect_timeout": 30,
    "read_timeout": 300,
    "verify_ssl": true,
    "follow_redirects": true
  }
}
```

### Authentication

If your LM Studio instance requires authentication:

```bash
# Set API key
export LMSTUDIO_API_KEY="your-api-key"

# Or in config file
{
  "lmstudio": {
    "api_key": "your-api-key",
    "auth_type": "bearer"  # or "basic"
  }
}
```

### Custom LM Studio Setup

For non-standard LM Studio configurations:

```json
{
  "lmstudio": {
    "base_url": "https://remote-lmstudio.example.com",
    "port": 8080,
    "use_https": true,
    "custom_endpoints": {
      "health": "/health",
      "models": "/api/v1/models",
      "chat": "/api/v1/chat"
    }
  }
}
```

## 🛡️ Safety Configuration

### Context Limits and Thresholds

```json
{
  "safety": {
    "max_context_threshold": 65536,
    "default_test_threshold": 32768,
    "emergency_stop_threshold": 131072,
    "enable_safety_checks": true,
    "safety_margin_tokens": 1024,
    "max_test_iterations": 20,
    "min_test_context": 512
  }
}
```

### Memory and Resource Limits

```json
{
  "safety": {
    "max_memory_usage_mb": 8192,
    "max_concurrent_tests": 1,
    "test_timeout_seconds": 300,
    "enable_resource_monitoring": true,
    "memory_check_interval": 30
  }
}
```

### Failure Handling

```json
{
  "safety": {
    "max_consecutive_failures": 3,
    "failure_cooldown_seconds": 60,
    "auto_reduce_context_on_failure": true,
    "context_reduction_factor": 0.8,
    "enable_automatic_recovery": true
  }
}
```

## 🚀 Performance Configuration

### Retry Logic

```json
{
  "performance": {
    "max_retries": 3,
    "retry_delay": 1.0,
    "retry_backoff": 2.0,
    "retry_on_errors": [
      "ConnectionError",
      "TimeoutError",
      "HTTPStatusError"
    ],
    "exponential_backoff": true,
    "jitter": true
  }
}
```

### Caching

```json
{
  "performance": {
    "cache_results": true,
    "cache_ttl_hours": 24,
    "cache_size_mb": 100,
    "cache_compression": true,
    "cache_cleanup_interval": 3600
  }
}
```

### Model Management

```json
{
  "performance": {
    "keep_models_loaded": true,
    "model_load_timeout": 60,
    "model_unload_delay": 300,
    "auto_unload_inactive": true,
    "memory_threshold_unload": 0.8
  }
}
```

## 📊 Output Configuration

### Formatting Options

```json
{
  "output": {
    "verbose_by_default": false,
    "use_color": true,
    "use_emoji": true,
    "compact_tables": false,
    "show_progress": true,
    "table_style": "rounded",
    "date_format": "%Y-%m-%d %H:%M:%S",
    "number_format": "comma"
  }
}
```

### Verbosity Levels

```json
{
  "output": {
    "verbosity_levels": {
      "quiet": 0,
      "normal": 1,
      "verbose": 2,
      "debug": 3
    },
    "default_verbosity": 1,
    "show_timestamps": true,
    "show_model_info": true,
    "show_performance_stats": true
  }
}
```

## 📝 Logging Configuration

### Basic Logging

```json
{
  "logging": {
    "level": "INFO",
    "file": "~/.lmstrix/logs/lmstrix.log",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "date_format": "%Y-%m-%d %H:%M:%S"
  }
}
```

### Advanced Logging

```json
{
  "logging": {
    "level": "DEBUG",
    "handlers": [
      {
        "type": "file",
        "filename": "~/.lmstrix/logs/lmstrix.log",
        "max_size_mb": 10,
        "backup_count": 5,
        "level": "INFO"
      },
      {
        "type": "console",
        "level": "WARNING",
        "format": "%(levelname)s: %(message)s"
      }
    ],
    "loggers": {
      "lmstrix.api": "DEBUG",
      "lmstrix.core": "INFO",
      "httpx": "WARNING"
    }
  }
}
```

## 🔧 CLI Configuration

### Default Command Options

```json
{
  "cli": {
    "defaults": {
      "scan": {
        "verbose": false,
        "refresh": false
      },
      "test": {
        "threshold": 32768,
        "verbose": false,
        "timeout": 300
      },
      "infer": {
        "temperature": 0.7,
        "out_ctx": "auto",
        "verbose": false
      },
      "list": {
        "sort": "name",
        "show": "table"
      }
    }
  }
}
```

### Command Aliases

```json
{
  "cli": {
    "aliases": {
      "s": "scan",
      "l": "list",
      "t": "test",
      "i": "infer",
      "q": "infer --out_ctx 50",
      "v": "--verbose"
    }
  }
}
```

## 📍 Path Configuration

### Custom Paths

```json
{
  "paths": {
    "registry_file": "~/Documents/lmstrix/models.json",
    "cache_dir": "~/Documents/lmstrix/cache",
    "prompts_dir": "~/Documents/lmstrix/prompts",
    "logs_dir": "~/Documents/lmstrix/logs",
    "config_dir": "~/Documents/lmstrix/config"
  }
}
```

### Workspace Configuration

For project-specific settings, create `.lmstrix.json` in your project root:

```json
{
  "workspace": {
    "name": "my-project",
    "prompts_dir": "./prompts",
    "cache_dir": "./.lmstrix/cache",
    "preferred_models": [
      "llama-3.2-3b-instruct",
      "mistral-7b-instruct"
    ]
  },
  "safety": {
    "max_context_threshold": 16384
  }
}
```

## 🎯 Configuration Examples

### Development Setup

```json
{
  "lmstudio": {
    "base_url": "http://localhost:1234"
  },
  "safety": {
    "max_context_threshold": 16384
  },
  "logging": {
    "level": "DEBUG",
    "file": "./debug.log"
  },
  "output": {
    "verbose_by_default": true,
    "use_color": true
  }
}
```

### Production Setup

```json
{
  "lmstudio": {
    "base_url": "http://lmstudio-server:1234",
    "timeout": 600
  },
  "safety": {
    "max_context_threshold": 131072,
    "enable_safety_checks": true
  },
  "performance": {
    "max_retries": 5,
    "cache_results": true
  },
  "logging": {
    "level": "WARNING",
    "file": "/var/log/lmstrix.log"
  }
}
```

### Research Setup

```json
{
  "safety": {
    "max_context_threshold": 262144,
    "max_test_iterations": 50
  },
  "performance": {
    "cache_results": false,
    "keep_models_loaded": false
  },
  "output": {
    "verbose_by_default": true,
    "show_performance_stats": true
  },
  "logging": {
    "level": "DEBUG"
  }
}
```

## 🔍 Configuration Validation

### Validate Configuration

```bash
# Check current configuration
lmstrix config show

# Validate configuration file
lmstrix config validate ~/.lmstrix/config.json

# Show effective configuration (after merging all sources)
lmstrix config effective
```

### Configuration Schema

LMStrix validates configuration against a JSON schema. Invalid configurations will show helpful error messages:

```
Configuration Error: Invalid value for 'safety.max_context_threshold'
Expected: integer between 512 and 1048576
Received: "invalid"
Location: ~/.lmstrix/config.json:3:28
```

## 🚀 Next Steps

With configuration mastered:

- **[CLI Interface](cli-interface.md)** - Advanced command usage
- **[Context Testing](context-testing.md)** - Optimize testing with configuration
- **[Performance & Optimization](performance.md)** - Performance tuning
- **[Python API](python-api.md)** - Programmatic configuration

---

*Configuration is power - tune LMStrix to perfection! ⚙️*
</document_content>
</document>

<document index="48">
<source>src_docs/md/context-testing.md</source>
<document_content>
---
# this_file: src_docs/md/context-testing.md
title: Context Testing Deep Dive
description: Master LMStrix's Adaptive Context Optimization - algorithms, configuration, and best practices
---

# Context Testing Deep Dive

LMStrix's signature feature is **Adaptive Context Optimization** - a sophisticated binary search algorithm that automatically discovers the true operational context limits of language models. This chapter explores how it works, how to configure it, and best practices for optimization.

## 🧠 Understanding Context Limits

### What Are Context Limits?

Context limits determine how much text (measured in tokens) a language model can process in a single inference. This includes:

- **Input prompt tokens** - Your question or instruction
- **Output generation tokens** - The model's response
- **System/conversation tokens** - Chat history and system prompts

### Why Context Optimization Matters

1. **Maximize Utilization** - Use full model capability
2. **Prevent Failures** - Avoid context overflow errors
3. **Optimize Performance** - Find the sweet spot for speed vs capacity
4. **Resource Management** - Balance memory usage and throughput

### The Context Discovery Problem

Models often report theoretical limits that differ from practical operational limits:

```bash
# Theoretical limit vs. practical limit
Model: llama-3.2-3b-instruct
Advertised context: 128,000 tokens
Actual working limit: 29,696 tokens (discovered by LMStrix)
```

## ⚡ Binary Search Algorithm

### How It Works

LMStrix uses a sophisticated binary search to efficiently find the maximum working context:

```
Initial range: [512, threshold]
│
├─ Test midpoint: 32,768
│  ├─ Success → Search upper half: [32,768, 65,536]
│  └─ Failure → Search lower half: [512, 32,768]
│
├─ Test midpoint: 49,152
│  ├─ Success → Search upper half: [49,152, 65,536]
│  └─ Failure → Search lower half: [32,768, 49,152]
│
└─ Continue until optimal found: 47,104 tokens
```

### Algorithm Parameters

```python
{
    "min_context": 512,           # Minimum test size
    "max_context": 65536,         # Safety threshold
    "max_iterations": 20,         # Maximum search steps
    "convergence_threshold": 256, # Stop when range < threshold
    "safety_margin": 1024,        # Buffer below failure point
    "timeout_per_test": 300       # Seconds per test attempt
}
```

### Convergence Criteria

The algorithm stops when:

1. **Range convergence** - Search range becomes smaller than threshold
2. **Maximum iterations** - Safety limit reached
3. **Consistent failure** - Multiple consecutive failures at different sizes
4. **Timeout** - Test takes too long to complete

## 🛠️ Test Configuration

### Safety Thresholds

Configure maximum test limits to protect your system:

```bash
# Conservative (recommended for first run)
lmstrix test --all --threshold 16384

# Moderate (good for most systems)
lmstrix test --all --threshold 32768

# Aggressive (high-end systems only)
lmstrix test --all --threshold 131072

# Custom threshold per model
lmstrix test large-model --threshold 65536
lmstrix test small-model --threshold 32768
```

### Test Prompts

#### Default Test Prompt

LMStrix uses a carefully crafted default prompt that:

- Scales linearly with context size
- Maintains consistent complexity
- Exercises model capabilities realistically

```python
def generate_test_prompt(target_tokens: int) -> str:
    """Generate a prompt that will consume approximately target_tokens."""
    base_instruction = "Analyze and respond to the following repeated text: "
    
    # Calculate repetition needed for target size
    repeat_text = "The quick brown fox jumps over the lazy dog. "
    tokens_per_repeat = estimate_tokens(repeat_text)
    repetitions = (target_tokens - estimate_tokens(base_instruction)) // tokens_per_repeat
    
    return base_instruction + (repeat_text * repetitions)
```

#### Custom Test Prompts

Use domain-specific prompts for more accurate testing:

```bash
# Simple custom prompt
lmstrix test model-name --prompt "What is 2+2?"

# Load prompt from file
lmstrix test model-name --file-prompt test-prompt.txt

# Use template with variable scaling
lmstrix test model-name --file-prompt prompts.toml --template scale_test
```

**Example scaling template** (`prompts.toml`):

```toml
[scale_test]
prompt = """
Analyze the following data and provide insights:
{{repeated_data}}

Please provide:
1. Summary statistics
2. Key patterns
3. Recommendations
"""

[scale_data]
data = "Sample data point with relevant information. "
repeats = "{{context_size // 50}}"  # Scale with context size
```

### Advanced Test Options

```bash
# Test specific context size (no binary search)
lmstrix test model-name --ctx 16384

# Extended timeout for large models
lmstrix test model-name --timeout 600

# Reset existing results and re-test
lmstrix test model-name --reset

# Verbose output with detailed progress
lmstrix test model-name --verbose

# Dry run to see test plan
lmstrix test model-name --dry-run
```

## 📊 Test Results and Interpretation

### Result Data Structure

Each test produces comprehensive results:

```python
{
    "model_id": "llama-3.2-3b-instruct",
    "test_timestamp": "2024-01-15T10:30:00Z",
    "optimal_context": 29696,
    "test_successful": True,
    "total_iterations": 8,
    "total_test_time": 156.7,
    "safety_threshold": 32768,
    "test_points": [
        {"context": 16384, "success": True, "time": 12.3},
        {"context": 24576, "success": True, "time": 18.7},
        {"context": 28672, "success": True, "time": 24.1},
        {"context": 30720, "success": False, "error": "context_overflow"},
        {"context": 29696, "success": True, "time": 22.8}
    ],
    "failure_analysis": {
        "first_failure_at": 30720,
        "failure_type": "context_overflow",
        "consistent_failure_above": 30000
    },
    "performance_metrics": {
        "avg_tokens_per_second": 42.3,
        "peak_memory_usage_mb": 3847,
        "model_load_time": 8.2
    }
}
```

### Success/Failure Patterns

#### Successful Test

```
Model                           Context      Status
llama-3.2-3b-instruct          8,192        ✅ Success
llama-3.2-3b-instruct          16,384       ✅ Success  
llama-3.2-3b-instruct          24,576       ✅ Success
llama-3.2-3b-instruct          28,672       ✅ Success
llama-3.2-3b-instruct          30,720       ❌ Failed
llama-3.2-3b-instruct          29,696       ✅ Success
                                
🎉 OPTIMAL CONTEXT: 29,696 tokens
```

#### Common Failure Patterns

```bash
# Memory exhaustion
Context: 65536 → ❌ Failed (out_of_memory)

# Timeout (model too slow)
Context: 32768 → ❌ Failed (timeout)

# Model capacity limit
Context: 16384 → ❌ Failed (context_overflow)

# API/Connection issues
Context: 8192 → ❌ Failed (connection_error)
```

### Result Interpretation

#### Optimal Context Found

```python
if result["test_successful"] and result["optimal_context"] > 0:
    print(f"✅ Found optimal context: {result['optimal_context']:,} tokens")
    print(f"📊 Test efficiency: {result['total_iterations']} iterations")
    print(f"⏱️ Test duration: {result['total_test_time']:.1f} seconds")
```

#### Test Failed

```python
if not result["test_successful"]:
    print(f"❌ Test failed: {result.get('error', 'Unknown error')}")
    
    # Analyze failure reasons
    if "timeout" in result.get("error", "").lower():
        print("💡 Try: Increase timeout or reduce threshold")
    elif "memory" in result.get("error", "").lower():
        print("💡 Try: Lower threshold or free up system memory")
    elif "connection" in result.get("error", "").lower():
        print("💡 Try: Check LM Studio is running and accessible")
```

## ⚙️ Advanced Configuration

### Fine-Tuning Algorithm Parameters

#### Binary Search Precision

```json
{
  "context_testing": {
    "binary_search": {
      "convergence_threshold": 256,     # Stop when range < 256 tokens
      "max_iterations": 25,             # Allow more iterations for precision
      "safety_margin": 512,             # Larger buffer for safety
      "overshoot_protection": true,     # Prevent testing beyond failures
      "adaptive_step_size": true        # Adjust step size based on failures
    }
  }
}
```

#### Performance Optimization

```json
{
  "context_testing": {
    "performance": {
      "parallel_testing": false,        # Test models sequentially
      "model_preload": true,            # Keep models loaded between tests
      "cache_failures": true,           # Remember failure points
      "early_termination": true,        # Stop on consistent failures
      "memory_monitoring": true         # Track memory usage
    }
  }
}
```

#### Failure Handling

```json
{
  "context_testing": {
    "failure_handling": {
      "max_consecutive_failures": 3,   # Stop after 3 failures
      "failure_backoff_factor": 0.7,   # Reduce next test by 30%
      "retry_failed_tests": true,      # Retry on transient errors
      "retry_delay_seconds": 5,        # Wait between retries
      "auto_adjust_threshold": true    # Lower threshold on failures
    }
  }
}
```

### Environment-Specific Tuning

#### High-Memory Systems

```bash
# Configuration for systems with 32GB+ RAM
export LMSTRIX_SAFETY_THRESHOLD=262144
export LMSTRIX_MAX_TEST_ITERATIONS=30
export LMSTRIX_MEMORY_MONITORING=true

lmstrix test --all --threshold 262144 --timeout 900
```

#### Low-Memory Systems

```bash
# Configuration for systems with 8GB RAM
export LMSTRIX_SAFETY_THRESHOLD=8192
export LMSTRIX_MAX_TEST_ITERATIONS=15
export LMSTRIX_AGGRESSIVE_GC=true

lmstrix test --all --threshold 8192 --timeout 300
```

#### Production Environments

```bash
# Conservative settings for production
export LMSTRIX_SAFETY_THRESHOLD=32768
export LMSTRIX_TEST_TIMEOUT=600
export LMSTRIX_ENABLE_MONITORING=true

lmstrix test --all --threshold 32768 --verbose
```

## 🔬 Advanced Testing Strategies

### Comprehensive Model Analysis

#### Full Context Mapping

Test multiple context ranges to understand model behavior:

```bash
# Test at multiple thresholds to map behavior
lmstrix test model-name --ctx 8192
lmstrix test model-name --ctx 16384
lmstrix test model-name --ctx 32768
lmstrix test model-name --ctx 65536

# Compare results
lmstrix list --filter "name:model-name" --show json
```

#### Stress Testing

Push models to their absolute limits:

```bash
# Gradual stress testing
for threshold in 16384 32768 65536 131072; do
    echo "Testing threshold: $threshold"
    lmstrix test model-name --threshold $threshold --reset
    sleep 10  # Cool-down period
done
```

#### Performance Profiling

Measure performance characteristics at different context sizes:

```python
from lmstrix.core.context_tester import ContextTester
import matplotlib.pyplot as plt

tester = ContextTester(verbose=True)

# Test multiple context sizes
contexts = [1024, 2048, 4096, 8192, 16384, 32768]
performance_data = []

for ctx in contexts:
    result = tester.test_specific_context("model-name", ctx)
    if result["success"]:
        performance_data.append({
            "context": ctx,
            "time": result["inference_time"],
            "tokens_per_second": result["tokens_per_second"]
        })

# Plot performance curve
contexts = [d["context"] for d in performance_data]
times = [d["time"] for d in performance_data]
speeds = [d["tokens_per_second"] for d in performance_data]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

ax1.plot(contexts, times, 'b-o')
ax1.set_xlabel('Context Size (tokens)')
ax1.set_ylabel('Inference Time (seconds)')
ax1.set_title('Context Size vs Inference Time')

ax2.plot(contexts, speeds, 'r-o')
ax2.set_xlabel('Context Size (tokens)')
ax2.set_ylabel('Tokens per Second')
ax2.set_title('Context Size vs Generation Speed')

plt.tight_layout()
plt.show()
```

### Batch Testing Workflows

#### Model Fleet Testing

Test all models systematically:

```bash
#!/bin/bash
# comprehensive_test.sh

# Configuration
THRESHOLD=32768
LOG_FILE="test_results_$(date +%Y%m%d_%H%M%S).log"

echo "Starting comprehensive model testing..." | tee $LOG_FILE

# Get all untested models
MODELS=$(lmstrix list --filter "status:untested" --show json | jq -r '.[].name')

for model in $MODELS; do
    echo "Testing model: $model" | tee -a $LOG_FILE
    
    # Test with retry logic
    for attempt in 1 2 3; do
        if lmstrix test "$model" --threshold $THRESHOLD --timeout 600; then
            echo "✅ $model test successful on attempt $attempt" | tee -a $LOG_FILE
            break
        else
            echo "❌ $model test failed on attempt $attempt" | tee -a $LOG_FILE
            if [ $attempt -eq 3 ]; then
                echo "🚫 $model failed all attempts" | tee -a $LOG_FILE
            fi
            sleep 30  # Cool-down between attempts
        fi
    done
    
    # Generate intermediate report
    lmstrix list --show summary | tee -a $LOG_FILE
    echo "---" | tee -a $LOG_FILE
done

echo "Testing complete. Final report:" | tee -a $LOG_FILE
lmstrix list --sort ctx --reverse | tee -a $LOG_FILE
```

#### Continuous Integration Testing

Automate testing in CI/CD pipelines:

```yaml
# .github/workflows/model-testing.yml
name: Model Context Testing

on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly on Sunday at 2 AM
  workflow_dispatch:

jobs:
  test-models:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install LMStrix
        run: pip install lmstrix
      
      - name: Start LM Studio (if available)
        run: |
          # Your LM Studio setup commands
          docker run -d --name lmstudio lmstudio/server:latest
      
      - name: Test Models
        run: |
          lmstrix scan
          lmstrix test --all --threshold 16384 --timeout 300
      
      - name: Generate Report
        run: |
          lmstrix list --show json > model-test-results.json
          lmstrix list --show summary > model-summary.txt
      
      - name: Upload Results
        uses: actions/upload-artifact@v3
        with:
          name: model-test-results
          path: |
            model-test-results.json
            model-summary.txt
```

## 🚨 Troubleshooting

### Common Issues and Solutions

#### Test Timeouts

**Problem:** Tests timeout before completion

**Solutions:**
```bash
# Increase timeout
lmstrix test model-name --timeout 900

# Lower threshold
lmstrix test model-name --threshold 16384

# Use simpler prompt
lmstrix test model-name --prompt "Hello world"
```

#### Memory Exhaustion

**Problem:** System runs out of memory during testing

**Solutions:**
```bash
# Lower safety threshold
export LMSTRIX_SAFETY_THRESHOLD=16384

# Enable memory monitoring
export LMSTRIX_MEMORY_MONITORING=true

# Test models individually
lmstrix test model-name --threshold 8192
```

#### Inconsistent Results

**Problem:** Different results across test runs

**Solutions:**
```bash
# Reset and re-test
lmstrix test model-name --reset

# Use deterministic prompt
lmstrix test model-name --prompt "Count to 10"

# Increase convergence threshold
export LMSTRIX_CONVERGENCE_THRESHOLD=512
```

#### Connection Failures

**Problem:** Cannot connect to LM Studio

**Solutions:**
```bash
# Check LM Studio status
curl http://localhost:1234/v1/models

# Test with different URL
lmstrix test model-name --server http://localhost:1234

# Increase connection timeout
export LMSTRIX_CONNECT_TIMEOUT=60
```

### Debugging Test Failures

#### Enable Debug Logging

```bash
# Full debug output
lmstrix --debug test model-name --verbose

# Debug specific components
export LMSTRIX_LOG_LEVEL=DEBUG
export LMSTRIX_DEBUG_COMPONENTS="context_tester,api_client"
```

#### Analyze Test Points

```python
from lmstrix.loaders.model_loader import load_model_registry

registry = load_model_registry()
model = registry.get_model("problematic-model")

# Examine test history
test_points = model.get("test_points", [])
for point in test_points:
    print(f"Context: {point['context']:,} - "
          f"Success: {point['success']} - "
          f"Time: {point.get('time', 'N/A')}s")
    if not point['success']:
        print(f"  Error: {point.get('error', 'Unknown')}")
```

#### Manual Validation

```bash
# Test specific context size manually
lmstrix infer "Test prompt" -m model-name --out-ctx 16384

# Compare with direct LM Studio API call
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "model-name",
    "messages": [{"role": "user", "content": "Test prompt"}],
    "max_tokens": 16384
  }'
```

## 🎯 Best Practices

### Testing Strategy

1. **Start Conservative** - Begin with low thresholds (16384)
2. **Test Systematically** - Test all models before production use
3. **Monitor Resources** - Watch memory and CPU usage
4. **Document Results** - Keep records of optimal contexts
5. **Regular Re-testing** - Contexts may change with model updates

### Performance Optimization

1. **Batch Testing** - Test multiple models in sequence
2. **Cache Results** - Avoid re-testing unchanged models
3. **Use Appropriate Hardware** - More RAM = higher thresholds
4. **Cool-down Periods** - Allow system recovery between tests
5. **Monitor Trends** - Track performance over time

### Production Deployment

1. **Conservative Limits** - Use 80% of discovered limit in production
2. **Fallback Strategies** - Have smaller context alternatives
3. **Monitoring** - Track context usage and failures
4. **Regular Updates** - Re-test when models or hardware change
5. **Documentation** - Document optimal contexts for each use case

## 🚀 Next Steps

Master context testing, then explore:

- **[Model Management](model-management.md)** - Advanced registry operations
- **[Performance & Optimization](performance.md)** - Production optimization
- **[Prompt Templating](prompt-templating.md)** - Advanced prompt engineering
- **[CLI Interface](cli-interface.md)** - Command-line mastery

---

*Context optimization mastered! Unlock your models' full potential! 🎯*
</document_content>
</document>

<document index="49">
<source>src_docs/md/index.md</source>
<document_content>
---
# this_file: src_docs/md/index.md
title: LMStrix Documentation
description: Professional Python toolkit for LM Studio with Adaptive Context Optimization
---

# LMStrix Documentation

Welcome to **LMStrix** - a professional Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). LMStrix provides powerful command-line interface (CLI) and Python API for managing, testing, and running local language models, with our standout feature: **Adaptive Context Optimization**.

## 🚀 Quick Overview

LMStrix revolutionizes how you work with local language models by automatically discovering their true operational context limits through intelligent binary search algorithms, while providing beautiful verbose logging, smart model management, and flexible inference capabilities.

---

## 📚 Documentation Structure

### 🔧 Getting Started
Essential information to get you up and running with LMStrix quickly and efficiently.

### 📖 User Guide  
Comprehensive guides for both CLI and Python API usage, covering all core functionality.

### 🎯 Advanced Topics
Deep dives into specialized features, optimization techniques, and advanced use cases.

---

## 📋 Table of Contents & Chapter Summaries

### **Chapter 1: [Installation](installation.md)**
**TLDR:** *Complete installation guide covering pip, uv, development setup, and system requirements. Get LMStrix running in under 5 minutes.*

- Multiple installation methods (pip, uv, from source)
- Development environment setup
- System requirements and dependencies
- Verification steps and troubleshooting
- Docker and virtual environment configurations

---

### **Chapter 2: [Quick Start](quickstart.md)**
**TLDR:** *Hit the ground running with essential commands and workflows. Learn the core LMStrix operations through practical examples.*

- First-time setup and configuration
- Essential CLI commands walkthrough
- Basic Python API usage examples
- Common workflows and patterns
- Your first context optimization test

---

### **Chapter 3: [Configuration](configuration.md)**
**TLDR:** *Master LMStrix configuration options, environment variables, and customization settings for optimal performance.*

- Configuration file structure and locations
- Environment variable reference
- LM Studio integration settings
- Safety and performance tuning
- Custom profiles and presets

---

### **Chapter 4: [CLI Interface](cli-interface.md)**
**TLDR:** *Complete command-line reference with examples, options, and advanced usage patterns for power users.*

- Complete command reference (`scan`, `list`, `test`, `infer`)
- Advanced CLI options and flags
- Output formatting and verbosity controls
- Batch processing and automation
- Integration with shell scripts and workflows

---

### **Chapter 5: [Python API](python-api.md)**
**TLDR:** *Comprehensive Python API documentation with code examples, class references, and integration patterns for developers.*

- Core API classes and methods
- Async/await patterns and best practices
- Error handling and exception management
- Integration examples and use cases
- Advanced programmatic usage

---

### **Chapter 6: [Context Testing](context-testing.md)**
**TLDR:** *Deep dive into LMStrix's signature Adaptive Context Optimization feature - how it works, configuration, and best practices.*

- Binary search algorithm explanation
- Safety mechanisms and thresholds
- Testing strategies and methodologies
- Performance optimization techniques
- Troubleshooting failed tests

---

### **Chapter 7: [Model Management](model-management.md)**
**TLDR:** *Master model discovery, registry management, persistence, and organization for efficient model workflows.*

- Model discovery and scanning
- Registry structure and persistence
- Model state management (loaded/unloaded)
- Batch operations and automation
- Model metadata and tagging

---

### **Chapter 8: [Prompt Templating](prompt-templating.md)**
**TLDR:** *Advanced prompt engineering with TOML templates, variable substitution, and reusable prompt libraries.*

- TOML template syntax and structure
- Variable substitution and templating
- Template organization and libraries
- Dynamic prompt generation
- Best practices for prompt design

---

### **Chapter 9: [Performance & Optimization](performance.md)**
**TLDR:** *Performance tuning, monitoring, optimization strategies, and advanced configuration for production deployments.*

- Performance monitoring and metrics
- Memory and resource optimization
- Concurrent processing strategies
- Production deployment patterns
- Troubleshooting and debugging

---

## 🎯 Key Features at a Glance

!!! tip "Core Capabilities"
    - **🔍 Automatic Context Discovery** - Binary search algorithm finds true operational context limits
    - **📊 Beautiful Verbose Logging** - Enhanced stats with emojis, timing, and token usage
    - **🚀 Smart Model Management** - Persistent models reduce loading overhead
    - **🎯 Flexible Inference Engine** - Powerful templating with percentage-based output control
    - **📋 Model Registry** - Track models, limits, and test results with JSON persistence
    - **🛡️ Safety Controls** - Configurable thresholds prevent system crashes
    - **💻 Rich CLI Interface** - Beautiful terminal output with progress indicators
    - **📈 Compact Test Output** - Live-updating tables without verbose clutter

## 🚦 Getting Started

Ready to dive in? Start with our [Installation Guide](installation.md) to get LMStrix set up, then follow the [Quick Start](quickstart.md) tutorial to run your first context optimization test.

!!! example "Quick Install"
    ```bash
    # Using pip
    pip install lmstrix
    
    # Using uv (recommended)
    uv pip install lmstrix
    
    # Verify installation
    lmstrix --help
    ```

## 🤝 Community & Support

- **GitHub Repository**: [github.com/your-organization/lmstrix](https://github.com/your-organization/lmstrix)
- **PyPI Package**: [pypi.org/project/lmstrix](https://pypi.org/project/lmstrix)
- **Issue Tracker**: Report bugs and request features
- **Discussions**: Join the community conversations

---

*Happy modeling with LMStrix! 🚀*
</document_content>
</document>

<document index="50">
<source>src_docs/md/installation.md</source>
<document_content>
---
# this_file: src_docs/md/installation.md
title: Installation Guide
description: Complete installation guide for LMStrix - multiple methods, development setup, and troubleshooting
---

# Installation Guide

This guide covers everything you need to install LMStrix and get it running on your system. LMStrix supports multiple installation methods and platforms.

## 🚀 Quick Install

### Using pip (Standard)

```bash
# Install from PyPI
pip install lmstrix

# Verify installation
lmstrix --help
```

### Using uv (Recommended)

[uv](https://github.com/astral-sh/uv) is a fast Python package installer and resolver, written in Rust.

```bash
# Install uv first (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install LMStrix with uv
uv pip install lmstrix

# Verify installation
lmstrix --help
```

!!! tip "Why uv?"
    uv is significantly faster than pip and provides better dependency resolution. It's particularly useful for Python development workflows and is our recommended installation method.

## 📋 System Requirements

### Minimum Requirements

- **Python**: 3.11 or higher
- **Operating System**: Windows 10+, macOS 10.15+, Linux (most distributions)
- **RAM**: 4GB minimum (8GB+ recommended for larger models)
- **Storage**: 500MB for LMStrix + space for your models

### Required Dependencies

LMStrix automatically installs these dependencies:

- `lmstudio-python` - Official LM Studio Python SDK
- `httpx` - Async HTTP client for API communication
- `pydantic` - Data validation and settings management
- `fire` - Command-line interface framework
- `rich` - Terminal formatting and progress indicators
- `tenacity` - Retry logic with exponential backoff
- `tiktoken` - Token counting for various models
- `loguru` - Advanced logging capabilities

### LM Studio Requirements

!!! warning "LM Studio Required"
    LMStrix requires [LM Studio](https://lmstudio.ai/) to be installed and running on your system. Download it from the official website and ensure it's configured properly.

**LM Studio Setup:**

1. Download and install LM Studio from [lmstudio.ai](https://lmstudio.ai/)
2. Start LM Studio and enable the local server
3. Download at least one model
4. Verify the server is running (default: `http://localhost:1234`)

## 🔧 Development Installation

### From Source

For developers who want to contribute or use the latest features:

```bash
# Clone the repository
git clone https://github.com/your-organization/lmstrix.git
cd lmstrix

# Install in development mode with all dependencies
pip install -e ".[dev]"

# Or using uv
uv pip install -e ".[dev]"
```

### Development Dependencies

The development installation includes additional tools:

- `pytest` - Testing framework
- `pytest-cov` - Coverage reporting
- `black` - Code formatting
- `ruff` - Fast Python linter
- `mypy` - Static type checking
- `hatch` - Build system and project management

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/lmstrix --cov-report=html

# Run specific test categories
pytest -m "not integration"  # Unit tests only
pytest -m integration        # Integration tests only
```

### Code Quality Checks

```bash
# Format code
hatch run lint:fmt

# Check style and types
hatch run lint:all

# Individual tools
black .
ruff check .
mypy src/lmstrix
```

## 🐳 Docker Installation

### Using Docker

```dockerfile
FROM python:3.11-slim

# Install uv
RUN pip install uv

# Install LMStrix
RUN uv pip install lmstrix

# Set working directory
WORKDIR /app

# Entry point
ENTRYPOINT ["lmstrix"]
```

### Docker Compose Example

```yaml
version: '3.8'
services:
  lmstrix:
    build: .
    volumes:
      - ./data:/app/data
      - ./prompts:/app/prompts
    environment:
      - LMSTUDIO_BASE_URL=http://host.docker.internal:1234
    depends_on:
      - lmstudio
```

## 🌐 Virtual Environments

### Using venv

```bash
# Create virtual environment
python -m venv lmstrix-env

# Activate (Linux/macOS)
source lmstrix-env/bin/activate

# Activate (Windows)
lmstrix-env\Scripts\activate

# Install LMStrix
pip install lmstrix
```

### Using conda

```bash
# Create conda environment
conda create -n lmstrix python=3.11

# Activate environment
conda activate lmstrix

# Install LMStrix
pip install lmstrix
```

### Using pipenv

```bash
# Create Pipfile and install
pipenv install lmstrix

# Activate shell
pipenv shell
```

## ⚙️ Configuration

### Environment Variables

LMStrix can be configured using environment variables:

```bash
# LM Studio server URL (default: http://localhost:1234)
export LMSTUDIO_BASE_URL="http://localhost:1234"

# Default timeout for API calls (default: 300 seconds)
export LMSTRIX_TIMEOUT="300"

# Enable debug logging
export LMSTRIX_DEBUG="true"

# Model registry file location
export LMSTRIX_REGISTRY_PATH="~/.lmstrix/models.json"
```

### Configuration File

Create a configuration file at `~/.lmstrix/config.json`:

```json
{
  "lmstudio_base_url": "http://localhost:1234",
  "default_timeout": 300,
  "max_retries": 3,
  "debug": false,
  "registry_path": "~/.lmstrix/models.json",
  "safety_threshold": 65536
}
```

## ✅ Verification

### Test Installation

```bash
# Check LMStrix version
lmstrix --version

# Test LM Studio connection
lmstrix scan

# Run basic health check
lmstrix list
```

### Expected Output

```bash
$ lmstrix scan
🔍 Scanning for models...
✅ Found 3 models in LM Studio
📋 Updated model registry

$ lmstrix list
Model                           Size    Context    Status
llama-3.2-3b-instruct          3.2B    Unknown    Not tested
mistral-7b-instruct            7.1B    Unknown    Not tested
codellama-13b-python           13.0B   Unknown    Not tested
```

## 🔧 Troubleshooting

### Common Issues

#### 1. LM Studio Connection Failed

**Problem:** `ConnectionError: Could not connect to LM Studio`

**Solutions:**
- Ensure LM Studio is running
- Check the server URL (default: `http://localhost:1234`)
- Verify firewall settings
- Try different port if 1234 is occupied

#### 2. Python Version Compatibility

**Problem:** `Python 3.11+ is required`

**Solutions:**
- Update Python to 3.11 or higher
- Use pyenv to manage multiple Python versions
- Check your virtual environment Python version

#### 3. Permission Errors

**Problem:** `Permission denied when installing`

**Solutions:**
```bash
# Use user installation
pip install --user lmstrix

# Or create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate   # Windows
pip install lmstrix
```

#### 4. Import Errors

**Problem:** `ModuleNotFoundError: No module named 'lmstrix'`

**Solutions:**
- Verify installation: `pip list | grep lmstrix`
- Check Python path: `python -c "import sys; print(sys.path)"`
- Reinstall: `pip uninstall lmstrix && pip install lmstrix`

### Getting Help

If you encounter issues not covered here:

1. **Check the logs**: Run with `--debug` flag for detailed output
2. **Search issues**: Check [GitHub Issues](https://github.com/your-organization/lmstrix/issues)
3. **Report bugs**: Create a new issue with detailed information
4. **Join discussions**: Participate in community discussions

### System Information

To help with troubleshooting, gather system information:

```bash
# Python version
python --version

# LMStrix version
lmstrix --version

# System information
uname -a  # Linux/macOS
systeminfo  # Windows

# LM Studio status
curl http://localhost:1234/v1/models
```

## 🚀 Next Steps

Once LMStrix is installed and verified:

1. **[Quick Start](quickstart.md)** - Learn basic usage and commands
2. **[Configuration](configuration.md)** - Customize LMStrix for your needs
3. **[CLI Interface](cli-interface.md)** - Master the command-line tools
4. **[Context Testing](context-testing.md)** - Optimize your models

---

*Installation complete! Ready to supercharge your LM Studio experience? 🚀*
</document_content>
</document>

<document index="51">
<source>src_docs/md/model-management.md</source>
<document_content>
---
# this_file: src_docs/md/model-management.md
title: Model Management Guide
description: Master model discovery, registry management, persistence, and organization for efficient workflows
---

# Model Management Guide

LMStrix provides comprehensive model management capabilities through its registry system, model discovery, state tracking, and organization features. This guide covers everything from basic model operations to advanced registry management.

## 🗂️ Model Registry Overview

### What is the Model Registry?

The model registry is LMStrix's central database that tracks:

- **Model metadata** - Names, sizes, parameters, types
- **Test results** - Context limits, test status, performance metrics
- **Usage statistics** - Inference counts, total tokens, average performance
- **State information** - Load status, last used, health checks

### Registry Structure

```json
{
  "version": "1.0",
  "last_updated": "2024-01-15T10:30:00Z",
  "models": {
    "llama-3.2-3b-instruct": {
      "id": "llama-3.2-3b-instruct",
      "name": "Llama 3.2 3B Instruct",
      "display_name": "Llama 3.2 3B Instruct",
      "size_mb": 3276,
      "parameters": "3.2B",
      "type": "chat",
      "architecture": "llama",
      "context_window": 128000,
      "tested_context": 29696,
      "test_status": "tested",
      "last_tested": "2024-01-15T10:30:00Z",
      "test_iterations": 8,
      "test_duration": 156.7,
      "performance_metrics": {
        "avg_tokens_per_second": 42.3,
        "peak_memory_mb": 3847,
        "load_time_seconds": 8.2
      },
      "usage_stats": {
        "inference_count": 127,
        "total_tokens_processed": 45832,
        "total_inference_time": 892.4,
        "last_used": "2024-01-15T09:45:00Z"
      },
      "metadata": {
        "added_date": "2024-01-10T14:20:00Z",
        "source": "lm_studio_scan",
        "tags": ["instruct", "chat", "3b"],
        "description": "Efficient instruction-following model"
      }
    }
  },
  "statistics": {
    "total_models": 8,
    "tested_models": 6,
    "untested_models": 2,
    "failed_tests": 0,
    "avg_context_limit": 34521,
    "total_inference_count": 856,
    "last_scan": "2024-01-15T08:00:00Z"
  }
}
```

## 🔍 Model Discovery

### Automatic Discovery

LMStrix automatically discovers models from LM Studio:

```bash
# Basic scan
lmstrix scan

# Verbose scan with detailed information
lmstrix scan --verbose

# Force refresh (ignore cache)
lmstrix scan --refresh

# Scan with custom server
lmstrix scan --server http://remote-lmstudio:1234
```

### Discovery Process

1. **Connect to LM Studio API** - Check server availability
2. **Query available models** - Get model list from API
3. **Extract metadata** - Parse model information
4. **Update registry** - Merge with existing data
5. **Preserve test results** - Keep existing context limits

### Discovery Output

```bash
$ lmstrix scan --verbose
🔍 Scanning LM Studio server at http://localhost:1234...
✅ Server connection successful (ping: 12ms)
📡 Discovering available models...

Found 4 models:
┌─────────────────────────────────┬──────────┬─────────────┬───────────┬─────────────┐
│ Model                           │ Size     │ Parameters  │ Type      │ Status      │
├─────────────────────────────────┼──────────┼─────────────┼───────────┼─────────────┤
│ llama-3.2-3b-instruct          │ 3.2 GB   │ 3.2B        │ Chat      │ ✅ Known    │
│ mistral-7b-instruct-v0.2       │ 7.1 GB   │ 7.1B        │ Chat      │ 🆕 New      │
│ codellama-13b-python           │ 13.0 GB  │ 13.0B       │ Code      │ ✅ Known    │
│ phi-3-mini-4k-instruct         │ 2.4 GB   │ 3.8B        │ Chat      │ 🆕 New      │
└─────────────────────────────────┴──────────┴─────────────┴───────────┴─────────────┘

📋 Registry Updates:
  • Added 2 new models
  • Updated 2 existing models
  • Preserved test results for 2 models

⏱️ Scan completed in 3.24 seconds
```

### Custom Model Addition

Add models manually when auto-discovery isn't available:

```python
from lmstrix.loaders.model_loader import load_model_registry

registry = load_model_registry()

# Add custom model
registry.add_model({
    "id": "custom-llama-7b",
    "name": "Custom Llama 7B",
    "size_mb": 7168,
    "parameters": "7B",
    "type": "chat",
    "architecture": "llama",
    "context_window": 4096,
    "source": "manual_addition",
    "tags": ["custom", "llama", "7b"]
})

registry.save()
```

## 📊 Registry Operations

### Listing and Filtering

#### Basic Listing

```bash
# List all models
lmstrix list

# Sort by different criteria
lmstrix list --sort name        # Alphabetical
lmstrix list --sort size        # By model size
lmstrix list --sort ctx         # By context limit
lmstrix list --sort date        # By last test date

# Reverse sort order
lmstrix list --sort size --reverse
```

#### Advanced Filtering

```bash
# Filter by test status
lmstrix list --filter "status:tested"
lmstrix list --filter "status:untested"
lmstrix list --filter "status:failed"

# Filter by size ranges
lmstrix list --filter "size:>5GB"
lmstrix list --filter "size:1GB-10GB"

# Filter by context limits
lmstrix list --filter "ctx:>30000"
lmstrix list --filter "ctx:16384-65536"

# Filter by model type
lmstrix list --filter "type:chat"
lmstrix list --filter "type:code"

# Filter by tags
lmstrix list --filter "tag:instruct"
lmstrix list --filter "tag:llama,chat"

# Combined filters
lmstrix list --filter "status:tested,size:>5GB,type:chat"
```

#### Output Formats

```bash
# Table format (default)
lmstrix list --show table

# JSON for automation
lmstrix list --show json

# CSV for analysis
lmstrix list --show csv

# YAML format
lmstrix list --show yaml

# Brief summary
lmstrix list --show summary
```

### Model Information

#### Detailed Model Info

```python
from lmstrix.loaders.model_loader import load_model_registry

registry = load_model_registry()

# Get detailed model information
model_info = registry.get_model_info("llama-3.2-3b-instruct")

print(f"Model: {model_info['name']}")
print(f"Size: {model_info['size_mb']:,} MB")
print(f"Context: {model_info.get('tested_context', 'Unknown')}")
print(f"Performance: {model_info.get('avg_tokens_per_second', 'N/A')} tok/s")
print(f"Usage: {model_info.get('inference_count', 0)} inferences")
```

#### Usage Statistics

```python
# Get usage statistics for a model
stats = registry.get_model_usage_stats("llama-3.2-3b-instruct")

print(f"Total inferences: {stats['inference_count']}")
print(f"Total tokens: {stats['total_tokens_processed']:,}")
print(f"Average tokens per inference: {stats['avg_tokens_per_inference']:.1f}")
print(f"Total time: {stats['total_inference_time']:.1f} seconds")
print(f"Last used: {stats['last_used']}")
```

#### Performance Metrics

```python
# Get performance metrics
metrics = registry.get_model_performance("llama-3.2-3b-instruct")

print(f"Average speed: {metrics['avg_tokens_per_second']:.1f} tok/s")
print(f"Peak memory: {metrics['peak_memory_mb']:,} MB")
print(f"Load time: {metrics['load_time_seconds']:.1f} seconds")
print(f"Test duration: {metrics['test_duration']:.1f} seconds")
```

## 🏷️ Model Organization

### Tagging System

#### Automatic Tags

LMStrix automatically assigns tags based on model characteristics:

```python
{
    "architecture_tags": ["llama", "mistral", "phi", "codellama"],
    "size_tags": ["small", "medium", "large", "xl"],  # Based on parameter count
    "type_tags": ["chat", "instruct", "code", "completion"],
    "capability_tags": ["multilingual", "reasoning", "creative"],
    "performance_tags": ["fast", "balanced", "thorough"]  # Based on speed/quality
}
```

#### Custom Tags

Add your own tags for organization:

```python
# Add custom tags
registry.add_tags("llama-3.2-3b-instruct", ["production", "favorite", "qa"])

# Remove tags
registry.remove_tags("llama-3.2-3b-instruct", ["qa"])

# Get models by tag
production_models = registry.get_models_by_tag("production")
```

### Model Groups

#### Create Model Groups

Organize related models into groups:

```python
# Create model groups
registry.create_group("chat_models", [
    "llama-3.2-3b-instruct",
    "mistral-7b-instruct",
    "phi-3-mini-4k-instruct"
])

registry.create_group("code_models", [
    "codellama-13b-python",
    "codellama-7b-instruct"
])

# Add models to existing group
registry.add_to_group("chat_models", "new-chat-model")
```

#### Group Operations

```python
# List groups
groups = registry.list_groups()

# Get models in group
chat_models = registry.get_group("chat_models")

# Test all models in group
for model_id in chat_models:
    result = tester.test_model_context(model_id, threshold=32768)
```

### Model Profiles

#### Create Usage Profiles

Define different usage patterns for models:

```python
# Create profiles for different use cases
registry.create_profile("quick_answers", {
    "preferred_models": ["llama-3.2-3b-instruct", "phi-3-mini-4k-instruct"],
    "max_context": 1024,
    "temperature": 0.3,
    "description": "Fast responses for simple questions"
})

registry.create_profile("detailed_analysis", {
    "preferred_models": ["codellama-13b-python", "mistral-7b-instruct"],
    "max_context": 16384,
    "temperature": 0.7,
    "description": "Comprehensive analysis tasks"
})

# Use profile for inference
profile = registry.get_profile("quick_answers")
best_model = profile["preferred_models"][0]
```

## 🔄 Model State Management

### Model States

LMStrix tracks model states through the inference lifecycle:

```python
{
    "unloaded": "Model not currently in memory",
    "loading": "Model being loaded into memory",
    "loaded": "Model ready for inference",
    "active": "Model currently processing inference",
    "error": "Model in error state",
    "unloading": "Model being removed from memory"
}
```

### State Tracking

```python
from lmstrix.core.models import ModelStateManager

state_manager = ModelStateManager()

# Check model state
state = state_manager.get_model_state("llama-3.2-3b-instruct")
print(f"Model state: {state}")

# Get all loaded models
loaded_models = state_manager.get_loaded_models()
print(f"Loaded models: {loaded_models}")

# Monitor state changes
def on_state_change(model_id, old_state, new_state):
    print(f"{model_id}: {old_state} → {new_state}")

state_manager.add_state_listener(on_state_change)
```

### Memory Management

#### Automatic Memory Management

```python
# Configure automatic memory management
registry.configure_memory_management({
    "auto_unload_inactive": True,
    "inactive_timeout_minutes": 30,
    "memory_threshold_percent": 80,
    "max_loaded_models": 2
})
```

#### Manual Memory Control

```python
# Load model explicitly
state_manager.load_model("llama-3.2-3b-instruct")

# Unload model to free memory
state_manager.unload_model("llama-3.2-3b-instruct")

# Unload all models
state_manager.unload_all_models()

# Get memory usage
memory_info = state_manager.get_memory_usage()
print(f"Total memory: {memory_info['total_mb']:,} MB")
print(f"Used memory: {memory_info['used_mb']:,} MB")
print(f"Available: {memory_info['available_mb']:,} MB")
```

## 📈 Performance Tracking

### Usage Analytics

#### Track Model Usage

```python
# Record inference usage
registry.record_inference("llama-3.2-3b-instruct", {
    "tokens_processed": 245,
    "inference_time": 5.7,
    "timestamp": "2024-01-15T10:30:00Z",
    "success": True
})

# Get usage trends
trends = registry.get_usage_trends("llama-3.2-3b-instruct", days=30)
print(f"Daily average inferences: {trends['daily_avg']:.1f}")
print(f"Peak usage day: {trends['peak_day']}")
print(f"Total tokens this month: {trends['monthly_tokens']:,}")
```

#### Performance Monitoring

```python
# Monitor performance metrics
metrics = registry.get_performance_metrics("llama-3.2-3b-instruct")

print(f"Average response time: {metrics['avg_response_time']:.2f}s")
print(f"P95 response time: {metrics['p95_response_time']:.2f}s")
print(f"Throughput: {metrics['tokens_per_second']:.1f} tok/s")
print(f"Success rate: {metrics['success_rate']:.1%}")
```

#### Generate Reports

```python
# Generate performance report
report = registry.generate_performance_report(
    start_date="2024-01-01",
    end_date="2024-01-31",
    models=["llama-3.2-3b-instruct", "mistral-7b-instruct"]
)

print(f"Report period: {report['period']}")
print(f"Total inferences: {report['total_inferences']:,}")
print(f"Average latency: {report['avg_latency']:.2f}s")
print(f"Most used model: {report['most_used_model']}")
```

### Benchmarking

#### Model Comparison

```python
from lmstrix.utils.benchmarking import ModelBenchmark

benchmark = ModelBenchmark()

# Compare models on standard tasks
results = benchmark.compare_models([
    "llama-3.2-3b-instruct",
    "mistral-7b-instruct",
    "phi-3-mini-4k-instruct"
], tasks=["qa", "summarization", "creative_writing"])

# Display results
for model_result in results:
    print(f"Model: {model_result['model_id']}")
    print(f"QA Score: {model_result['qa_score']:.2f}")
    print(f"Speed: {model_result['avg_speed']:.1f} tok/s")
    print(f"Memory: {model_result['peak_memory']:,} MB")
    print("---")
```

#### Custom Benchmarks

```python
# Create custom benchmark
benchmark.create_custom_benchmark("coding_tasks", {
    "prompts": [
        "Write a Python function to sort a list",
        "Explain recursion with examples",
        "Debug this code: [code snippet]"
    ],
    "evaluation_criteria": ["correctness", "clarity", "efficiency"],
    "timeout_seconds": 60
})

# Run custom benchmark
results = benchmark.run_benchmark("coding_tasks", "codellama-13b-python")
```

## 🔧 Advanced Registry Management

### Registry Maintenance

#### Cleanup Operations

```python
# Remove orphaned models (not in LM Studio)
removed = registry.cleanup_orphaned_models()
print(f"Removed {len(removed)} orphaned models")

# Remove old test results
registry.cleanup_old_test_results(days=90)

# Compact registry (remove unused data)
registry.compact()
```

#### Registry Validation

```python
# Validate registry integrity
validation_result = registry.validate()

if validation_result["valid"]:
    print("✅ Registry is valid")
else:
    print("❌ Registry issues found:")
    for issue in validation_result["issues"]:
        print(f"  • {issue}")

# Fix common issues automatically
registry.auto_fix_issues()
```

### Backup and Restore

#### Backup Registry

```python
# Create backup
backup_path = registry.create_backup()
print(f"Registry backed up to: {backup_path}")

# Create backup with custom name
backup_path = registry.create_backup("pre_update_backup")

# List available backups
backups = registry.list_backups()
for backup in backups:
    print(f"Backup: {backup['name']} ({backup['date']})")
```

#### Restore Registry

```python
# Restore from backup
registry.restore_from_backup("pre_update_backup")

# Restore from file
registry.restore_from_file("/path/to/backup.json")

# Merge registry data
registry.merge_from_backup("another_backup", strategy="prefer_newer")
```

### Import/Export

#### Export Registry Data

```bash
# Export to JSON
lmstrix list --show json > models_export.json

# Export specific models
lmstrix list --filter "status:tested" --show json > tested_models.json

# Export with full metadata
lmstrix registry export --full-metadata models_full.json
```

#### Import Registry Data

```python
# Import models from file
registry.import_from_file("models_export.json", merge_strategy="update")

# Import with validation
try:
    imported_count = registry.import_models(
        "external_models.json",
        validate=True,
        skip_duplicates=True
    )
    print(f"Imported {imported_count} models")
except ValidationError as e:
    print(f"Import failed: {e}")
```

### Multi-Registry Management

#### Multiple Registries

```python
# Work with multiple registries
prod_registry = load_model_registry("~/.lmstrix/prod_models.json")
dev_registry = load_model_registry("~/.lmstrix/dev_models.json")

# Sync registries
sync_result = prod_registry.sync_with(dev_registry)
print(f"Synced {sync_result['models_updated']} models")

# Compare registries
diff = prod_registry.compare_with(dev_registry)
print(f"Different models: {len(diff['different'])}")
print(f"Missing in prod: {len(diff['missing_in_first'])}")
```

## 🚀 Automation and Integration

### Automated Workflows

#### Model Health Monitoring

```python
#!/usr/bin/env python3
# model_health_monitor.py

import schedule
import time
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.core.context_tester import ContextTester

def health_check():
    registry = load_model_registry()
    tester = ContextTester()
    
    # Check all models
    for model in registry.list_models():
        if model['test_status'] == 'tested':
            # Quick health check
            result = tester.quick_health_check(model['id'])
            if not result['healthy']:
                print(f"⚠️ Health issue detected: {model['id']}")
                # Send alert, log issue, etc.

# Schedule health checks
schedule.every(6).hours.do(health_check)

while True:
    schedule.run_pending()
    time.sleep(60)
```

#### Auto-Testing New Models

```python
def auto_test_new_models():
    registry = load_model_registry()
    tester = ContextTester()
    
    # Scan for new models
    new_models = registry.scan_for_new_models()
    
    for model_id in new_models:
        print(f"Auto-testing new model: {model_id}")
        result = tester.test_model_context(
            model_id,
            threshold=32768,
            verbose=False
        )
        
        if result['test_successful']:
            print(f"✅ {model_id}: {result['optimal_context']:,} tokens")
        else:
            print(f"❌ {model_id}: Test failed")

# Run daily
schedule.every().day.at("02:00").do(auto_test_new_models)
```

### CI/CD Integration

#### GitHub Actions

```yaml
# .github/workflows/model-registry.yml
name: Model Registry Maintenance

on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM
  workflow_dispatch:

jobs:
  maintain-registry:
    runs-on: self-hosted  # Requires LM Studio access
    
    steps:
      - name: Scan for new models
        run: lmstrix scan --refresh
      
      - name: Test untested models
        run: lmstrix test --all --threshold 32768
      
      - name: Generate report
        run: |
          lmstrix list --show json > registry-report.json
          lmstrix registry stats > registry-stats.txt
      
      - name: Backup registry
        run: |
          cp ~/.lmstrix/models.json ./backup/models-$(date +%Y%m%d).json
      
      - name: Commit changes
        run: |
          git add .
          git commit -m "Update model registry - $(date)"
          git push
```

## 🚀 Next Steps

Master model management, then explore:

- **[Prompt Templating](prompt-templating.md)** - Advanced prompt engineering
- **[Performance & Optimization](performance.md)** - Production optimization techniques
- **[Python API](python-api.md)** - Programmatic model management
- **[CLI Interface](cli-interface.md)** - Command-line power user techniques

---

*Model management mastered! Organize and optimize your model fleet! 🗂️*
</document_content>
</document>

<document index="52">
<source>src_docs/md/performance.md</source>
<document_content>
---
# this_file: src_docs/md/performance.md
title: Performance & Optimization Guide
description: Performance tuning, monitoring, optimization strategies, and production deployment patterns
---

# Performance & Optimization Guide

This comprehensive guide covers performance optimization strategies, monitoring techniques, and production deployment patterns to help you get maximum performance from LMStrix and your language models.

## 📊 Performance Fundamentals

### Key Performance Metrics

Understanding these metrics is crucial for optimization:

**Latency Metrics:**
- **Time to First Token (TTFT)** - How quickly generation begins
- **Total Inference Time** - Complete generation duration
- **Token Generation Speed** - Tokens per second during generation

**Throughput Metrics:**
- **Requests per Second (RPS)** - Concurrent request handling capacity
- **Tokens per Second (TPS)** - Overall token processing rate
- **Context Utilization** - How efficiently context windows are used

**Resource Metrics:**
- **Memory Usage** - RAM consumption during inference
- **GPU Utilization** - Graphics processing efficiency
- **CPU Usage** - Processing overhead
- **Disk I/O** - Model loading and caching impact

### Performance Baseline

Establish baseline performance for optimization:

```python
from lmstrix.utils.performance import PerformanceBenchmark
from lmstrix.core.inference_manager import InferenceManager

# Create benchmark suite
benchmark = PerformanceBenchmark()

# Configure test parameters
test_config = {
    "models": ["llama-3.2-3b-instruct", "mistral-7b-instruct"],
    "prompts": [
        "Short question: What is AI?",
        "Medium analysis: Explain machine learning in 200 words",
        "Long generation: Write a detailed 500-word essay on neural networks"
    ],
    "iterations": 10,
    "warmup_iterations": 3
}

# Run baseline benchmark
baseline_results = benchmark.run_baseline(test_config)

# Display results
for model, metrics in baseline_results.items():
    print(f"Model: {model}")
    print(f"  TTFT: {metrics['avg_ttft']:.2f}s ± {metrics['ttft_std']:.2f}")
    print(f"  TPS: {metrics['avg_tps']:.1f} ± {metrics['tps_std']:.1f}")
    print(f"  Memory: {metrics['peak_memory_mb']:.0f} MB")
    print("---")
```

## ⚡ Context Optimization

### Smart Context Management

Optimize context usage for better performance:

```python
from lmstrix.core.context_optimizer import ContextOptimizer

optimizer = ContextOptimizer()

# Optimize context allocation
optimized_context = optimizer.optimize_context(
    prompt="Your prompt here",
    target_output_length=200,
    model_max_context=32768,
    strategy="balanced"  # "conservative", "balanced", "aggressive"
)

print(f"Recommended context allocation:")
print(f"  Prompt tokens: {optimized_context['prompt_tokens']}")
print(f"  Output tokens: {optimized_context['output_tokens']}")
print(f"  Safety buffer: {optimized_context['safety_buffer']}")
print(f"  Total context: {optimized_context['total_context']}")
```

### Dynamic Context Scaling

Automatically adjust context based on workload:

```python
class DynamicContextManager:
    def __init__(self):
        self.performance_history = []
        self.context_adjustments = {}
    
    def adjust_context(self, model_id, prompt_length, performance_target="balanced"):
        """Dynamically adjust context based on performance targets."""
        
        # Get model capabilities
        model_info = registry.get_model(model_id)
        max_context = model_info.get("tested_context", 16384)
        
        # Performance-based context scaling
        if performance_target == "speed":
            # Prioritize speed - use smaller contexts
            recommended_context = min(prompt_length * 1.5, max_context * 0.6)
        elif performance_target == "quality":
            # Prioritize quality - use larger contexts
            recommended_context = min(prompt_length * 3.0, max_context * 0.9)
        else:  # balanced
            recommended_context = min(prompt_length * 2.0, max_context * 0.8)
        
        return int(recommended_context)
    
    def monitor_performance(self, model_id, context_size, performance_metrics):
        """Track performance for future optimizations."""
        self.performance_history.append({
            "model_id": model_id,
            "context_size": context_size,
            "ttft": performance_metrics["time_to_first_token"],
            "tps": performance_metrics["tokens_per_second"],
            "memory_mb": performance_metrics["memory_usage_mb"],
            "timestamp": time.time()
        })
        
        # Auto-adjust if performance degrades
        if len(self.performance_history) > 10:
            recent_performance = self.performance_history[-5:]
            avg_tps = sum(p["tps"] for p in recent_performance) / 5
            
            if avg_tps < 20:  # Performance threshold
                self.context_adjustments[model_id] = {
                    "action": "reduce_context",
                    "factor": 0.8,
                    "reason": "low_throughput"
                }
```

### Context Caching Strategy

Implement intelligent context caching:

```python
from lmstrix.utils.context_cache import ContextCache

class SmartContextCache:
    def __init__(self, max_cache_size_mb=1024):
        self.cache = ContextCache(max_size_mb=max_cache_size_mb)
        self.access_patterns = {}
    
    def get_cached_context(self, prompt_hash, model_id):
        """Retrieve cached context if available."""
        cache_key = f"{model_id}:{prompt_hash}"
        
        # Track access patterns
        if cache_key not in self.access_patterns:
            self.access_patterns[cache_key] = {
                "access_count": 0,
                "last_access": time.time()
            }
        
        self.access_patterns[cache_key]["access_count"] += 1
        self.access_patterns[cache_key]["last_access"] = time.time()
        
        return self.cache.get(cache_key)
    
    def cache_context(self, prompt_hash, model_id, context_data):
        """Cache context with intelligent eviction."""
        cache_key = f"{model_id}:{prompt_hash}"
        
        # Calculate cache priority based on access patterns
        priority = self.calculate_cache_priority(cache_key)
        
        self.cache.set(cache_key, context_data, priority=priority)
    
    def calculate_cache_priority(self, cache_key):
        """Calculate cache priority based on access patterns."""
        if cache_key not in self.access_patterns:
            return 1.0
        
        pattern = self.access_patterns[cache_key]
        
        # Factors: access frequency, recency, context size
        frequency_score = min(pattern["access_count"] / 10, 1.0)
        recency_score = max(0, 1.0 - (time.time() - pattern["last_access"]) / 3600)
        
        return (frequency_score * 0.6) + (recency_score * 0.4)
```

## 🚀 Model Performance Tuning

### Model Selection Optimization

Choose optimal models based on requirements:

```python
from lmstrix.utils.model_selector import IntelligentModelSelector

class ModelSelector:
    def __init__(self):
        self.performance_db = {}
        self.load_performance_data()
    
    def select_optimal_model(self, requirements):
        """Select best model based on performance requirements."""
        
        candidates = self.filter_models_by_requirements(requirements)
        
        # Score models based on requirements
        scored_models = []
        for model in candidates:
            score = self.calculate_model_score(model, requirements)
            scored_models.append((model, score))
        
        # Return best model
        best_model = max(scored_models, key=lambda x: x[1])
        return best_model[0]
    
    def calculate_model_score(self, model, requirements):
        """Calculate model fitness score."""
        performance = self.performance_db.get(model["id"], {})
        
        # Scoring factors
        speed_score = self.score_speed(performance, requirements.get("speed_priority", 0.5))
        quality_score = self.score_quality(performance, requirements.get("quality_priority", 0.5))
        memory_score = self.score_memory(performance, requirements.get("memory_limit_mb"))
        context_score = self.score_context(model, requirements.get("min_context", 0))
        
        # Weighted average
        weights = {
            "speed": requirements.get("speed_weight", 0.3),
            "quality": requirements.get("quality_weight", 0.3),
            "memory": requirements.get("memory_weight", 0.2),
            "context": requirements.get("context_weight", 0.2)
        }
        
        total_score = (
            speed_score * weights["speed"] +
            quality_score * weights["quality"] +
            memory_score * weights["memory"] +
            context_score * weights["context"]
        )
        
        return total_score

# Usage example
selector = ModelSelector()

# Define requirements
requirements = {
    "speed_priority": 0.8,      # High speed priority
    "quality_priority": 0.6,    # Moderate quality needs
    "memory_limit_mb": 8192,    # 8GB memory limit
    "min_context": 16384,       # Minimum context needed
    "speed_weight": 0.4,        # Weight preferences
    "quality_weight": 0.3,
    "memory_weight": 0.2,
    "context_weight": 0.1
}

best_model = selector.select_optimal_model(requirements)
print(f"Recommended model: {best_model['name']}")
```

### Inference Parameter Optimization

Optimize model parameters for specific use cases:

```python
from lmstrix.utils.parameter_optimizer import ParameterOptimizer

class InferenceOptimizer:
    def __init__(self):
        self.optimizer = ParameterOptimizer()
        self.optimization_history = {}
    
    def optimize_parameters(self, model_id, use_case, sample_prompts):
        """Optimize inference parameters for specific use case."""
        
        # Define parameter search space
        param_space = {
            "temperature": [0.1, 0.3, 0.5, 0.7, 0.9, 1.1],
            "top_p": [0.1, 0.3, 0.5, 0.8, 0.9, 0.95],
            "top_k": [10, 20, 40, 50, 100, -1],
            "context_ratio": [0.6, 0.7, 0.8, 0.9]  # Ratio of max context to use
        }
        
        best_params = {}
        best_score = 0
        
        # Grid search optimization
        for temp in param_space["temperature"]:
            for top_p in param_space["top_p"]:
                for top_k in param_space["top_k"]:
                    for ctx_ratio in param_space["context_ratio"]:
                        
                        params = {
                            "temperature": temp,
                            "top_p": top_p,
                            "top_k": top_k,
                            "context_ratio": ctx_ratio
                        }
                        
                        score = self.evaluate_parameters(
                            model_id, params, sample_prompts, use_case
                        )
                        
                        if score > best_score:
                            best_score = score
                            best_params = params.copy()
        
        return best_params, best_score
    
    def evaluate_parameters(self, model_id, params, prompts, use_case):
        """Evaluate parameter combination."""
        scores = []
        
        for prompt in prompts:
            # Run inference with parameters
            result = manager.infer(
                model_id=model_id,
                prompt=prompt,
                temperature=params["temperature"],
                top_p=params["top_p"],
                top_k=params["top_k"],
                out_ctx=int(32768 * params["context_ratio"])
            )
            
            if result["succeeded"]:
                # Calculate use-case specific score
                score = self.calculate_use_case_score(result, use_case)
                scores.append(score)
        
        return sum(scores) / len(scores) if scores else 0
    
    def calculate_use_case_score(self, result, use_case):
        """Calculate score based on use case requirements."""
        if use_case == "speed":
            # Prioritize speed over quality
            speed_score = min(result["tokens_per_second"] / 50, 1.0)
            return speed_score * 0.8 + 0.2  # Base quality score
        
        elif use_case == "quality":
            # Prioritize quality over speed
            # This would need quality evaluation (BLEU, human eval, etc.)
            quality_score = 0.8  # Placeholder
            speed_penalty = max(0, (30 - result["tokens_per_second"]) / 30 * 0.2)
            return quality_score - speed_penalty
        
        else:  # balanced
            speed_score = min(result["tokens_per_second"] / 40, 1.0)
            quality_score = 0.7  # Placeholder
            return (speed_score + quality_score) / 2

# Usage
optimizer = InferenceOptimizer()

sample_prompts = [
    "Explain quantum computing briefly",
    "Summarize the main points of artificial intelligence",
    "What are the benefits of renewable energy?"
]

best_params, score = optimizer.optimize_parameters(
    model_id="llama-3.2-3b-instruct",
    use_case="speed",
    sample_prompts=sample_prompts
)

print(f"Optimal parameters for speed use case:")
print(f"  Temperature: {best_params['temperature']}")
print(f"  Top-p: {best_params['top_p']}")
print(f"  Top-k: {best_params['top_k']}")
print(f"  Context ratio: {best_params['context_ratio']}")
print(f"  Score: {score:.3f}")
```

## 🏗️ Architectural Optimization

### Connection Pooling

Optimize LM Studio connections:

```python
from lmstrix.utils.connection_pool import ConnectionPool
import asyncio
from contextlib import asynccontextmanager

class OptimizedLMStudioClient:
    def __init__(self, max_connections=10, connection_timeout=30):
        self.connection_pool = ConnectionPool(
            base_url="http://localhost:1234",
            max_connections=max_connections,
            connection_timeout=connection_timeout,
            keepalive_timeout=300
        )
    
    @asynccontextmanager
    async def get_connection(self):
        """Get connection from pool with automatic cleanup."""
        connection = await self.connection_pool.acquire()
        try:
            yield connection
        finally:
            await self.connection_pool.release(connection)
    
    async def parallel_inference(self, requests):
        """Process multiple inference requests in parallel."""
        async def process_request(request):
            async with self.get_connection() as conn:
                return await conn.infer(**request)
        
        # Process requests concurrently
        tasks = [process_request(req) for req in requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results

# Usage
client = OptimizedLMStudioClient(max_connections=5)

# Process multiple requests efficiently
requests = [
    {"model_id": "llama-3.2-3b-instruct", "prompt": "Question 1"},
    {"model_id": "llama-3.2-3b-instruct", "prompt": "Question 2"},
    {"model_id": "mistral-7b-instruct", "prompt": "Question 3"}
]

results = await client.parallel_inference(requests)
```

### Request Batching

Implement intelligent request batching:

```python
from lmstrix.utils.batch_processor import BatchProcessor
import asyncio
from collections import defaultdict

class IntelligentBatcher:
    def __init__(self, batch_size=5, batch_timeout=1.0):
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.pending_requests = defaultdict(list)
        self.batch_timers = {}
    
    async def add_request(self, model_id, request):
        """Add request to batch queue."""
        request_id = self.generate_request_id()
        future = asyncio.Future()
        
        # Add to pending batch
        self.pending_requests[model_id].append({
            "request_id": request_id,
            "request": request,
            "future": future
        })
        
        # Start batch timer if first request for this model
        if len(self.pending_requests[model_id]) == 1:
            self.batch_timers[model_id] = asyncio.create_task(
                self.batch_timeout_handler(model_id)
            )
        
        # Process batch if size limit reached
        if len(self.pending_requests[model_id]) >= self.batch_size:
            await self.process_batch(model_id)
        
        return await future
    
    async def batch_timeout_handler(self, model_id):
        """Handle batch timeout."""
        await asyncio.sleep(self.batch_timeout)
        if model_id in self.pending_requests and self.pending_requests[model_id]:
            await self.process_batch(model_id)
    
    async def process_batch(self, model_id):
        """Process accumulated batch for model."""
        if model_id not in self.pending_requests:
            return
        
        batch = self.pending_requests[model_id]
        if not batch:
            return
        
        # Clear pending requests
        self.pending_requests[model_id] = []
        
        # Cancel timeout timer
        if model_id in self.batch_timers:
            self.batch_timers[model_id].cancel()
            del self.batch_timers[model_id]
        
        # Process batch
        try:
            results = await self.execute_batch(model_id, batch)
            
            # Resolve futures with results
            for item, result in zip(batch, results):
                item["future"].set_result(result)
                
        except Exception as e:
            # Resolve futures with exception
            for item in batch:
                item["future"].set_exception(e)
    
    async def execute_batch(self, model_id, batch):
        """Execute batch of requests."""
        # Combine prompts for batch processing
        combined_requests = [item["request"] for item in batch]
        
        # Process using connection pool
        async with self.get_connection() as conn:
            results = await conn.batch_infer(model_id, combined_requests)
        
        return results
```

### Caching Strategy

Implement multi-level caching:

```python
from lmstrix.utils.cache import MultiLevelCache
import hashlib
import json

class InferenceCache:
    def __init__(self):
        self.l1_cache = {}  # In-memory cache
        self.l2_cache = MultiLevelCache(
            redis_url="redis://localhost:6379",
            disk_cache_dir="~/.lmstrix/cache",
            max_memory_size_mb=512,
            max_disk_size_gb=5
        )
    
    def generate_cache_key(self, model_id, prompt, params):
        """Generate deterministic cache key."""
        cache_data = {
            "model_id": model_id,
            "prompt": prompt,
            "params": sorted(params.items())
        }
        
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.sha256(cache_string.encode()).hexdigest()
    
    async def get_cached_result(self, model_id, prompt, params):
        """Try to get result from cache."""
        cache_key = self.generate_cache_key(model_id, prompt, params)
        
        # Try L1 cache first (fastest)
        if cache_key in self.l1_cache:
            return self.l1_cache[cache_key]
        
        # Try L2 cache (Redis/Disk)
        result = await self.l2_cache.get(cache_key)
        if result:
            # Populate L1 cache
            self.l1_cache[cache_key] = result
            return result
        
        return None
    
    async def cache_result(self, model_id, prompt, params, result):
        """Cache inference result."""
        cache_key = self.generate_cache_key(model_id, prompt, params)
        
        # Cache in both levels
        self.l1_cache[cache_key] = result
        await self.l2_cache.set(cache_key, result, ttl=3600)  # 1 hour TTL
        
        # Implement LRU eviction for L1 cache
        if len(self.l1_cache) > 1000:
            # Remove oldest 20% of entries
            oldest_keys = list(self.l1_cache.keys())[:200]
            for key in oldest_keys:
                del self.l1_cache[key]

# Integration with inference manager
class CachedInferenceManager(InferenceManager):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cache = InferenceCache()
    
    async def infer(self, **kwargs):
        """Inference with caching."""
        # Extract cache-relevant parameters
        cache_params = {
            "temperature": kwargs.get("temperature", 0.7),
            "top_p": kwargs.get("top_p", 1.0),
            "top_k": kwargs.get("top_k", -1),
            "out_ctx": kwargs.get("out_ctx", "auto")
        }
        
        # Try cache first
        cached_result = await self.cache.get_cached_result(
            kwargs["model_id"],
            kwargs["prompt"],
            cache_params
        )
        
        if cached_result:
            return cached_result
        
        # Cache miss - perform inference
        result = await super().infer(**kwargs)
        
        # Cache successful results
        if result["succeeded"]:
            await self.cache.cache_result(
                kwargs["model_id"],
                kwargs["prompt"],
                cache_params,
                result
            )
        
        return result
```

## 📈 Monitoring and Observability

### Performance Monitoring

Comprehensive performance tracking:

```python
from lmstrix.utils.monitoring import PerformanceMonitor
import time
import psutil
import GPUtil

class ComprehensiveMonitor:
    def __init__(self):
        self.metrics = {
            "requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_inference_time": 0,
            "response_times": [],
            "memory_usage": [],
            "gpu_usage": []
        }
        
        self.start_time = time.time()
    
    def record_inference(self, request, result):
        """Record inference metrics."""
        self.metrics["requests"] += 1
        
        if result["succeeded"]:
            self.metrics["successful_requests"] += 1
            self.metrics["total_tokens"] += result["tokens_used"]
            self.metrics["total_inference_time"] += result["inference_time"]
            self.metrics["response_times"].append(result["inference_time"])
        else:
            self.metrics["failed_requests"] += 1
        
        # Record system metrics
        self.record_system_metrics()
    
    def record_system_metrics(self):
        """Record system resource usage."""
        # Memory usage
        memory = psutil.virtual_memory()
        self.metrics["memory_usage"].append({
            "timestamp": time.time(),
            "used_mb": memory.used / 1024 / 1024,
            "available_mb": memory.available / 1024 / 1024,
            "percent": memory.percent
        })
        
        # GPU usage (if available)
        try:
            gpus = GPUtil.getGPUs()
            for i, gpu in enumerate(gpus):
                gpu_data = {
                    "gpu_id": i,
                    "timestamp": time.time(),
                    "utilization": gpu.load * 100,
                    "memory_used_mb": gpu.memoryUsed,
                    "memory_total_mb": gpu.memoryTotal,
                    "temperature": gpu.temperature
                }
                self.metrics["gpu_usage"].append(gpu_data)
        except:
            pass  # No GPU monitoring if not available
    
    def get_performance_summary(self):
        """Generate performance summary."""
        uptime = time.time() - self.start_time
        
        # Calculate averages
        avg_response_time = (
            sum(self.metrics["response_times"]) / len(self.metrics["response_times"])
            if self.metrics["response_times"] else 0
        )
        
        success_rate = (
            self.metrics["successful_requests"] / self.metrics["requests"]
            if self.metrics["requests"] > 0 else 0
        )
        
        requests_per_second = self.metrics["requests"] / uptime
        tokens_per_second = self.metrics["total_tokens"] / uptime
        
        return {
            "uptime_seconds": uptime,
            "total_requests": self.metrics["requests"],
            "success_rate": success_rate,
            "requests_per_second": requests_per_second,
            "tokens_per_second": tokens_per_second,
            "avg_response_time": avg_response_time,
            "total_tokens_processed": self.metrics["total_tokens"],
            "current_memory_usage": self.get_current_memory_usage(),
            "current_gpu_usage": self.get_current_gpu_usage()
        }
    
    def get_current_memory_usage(self):
        """Get current memory usage."""
        memory = psutil.virtual_memory()
        return {
            "used_mb": memory.used / 1024 / 1024,
            "available_mb": memory.available / 1024 / 1024,
            "percent": memory.percent
        }
    
    def get_current_gpu_usage(self):
        """Get current GPU usage."""
        try:
            gpus = GPUtil.getGPUs()
            return [{
                "gpu_id": i,
                "utilization": gpu.load * 100,
                "memory_used_mb": gpu.memoryUsed,
                "memory_total_mb": gpu.memoryTotal,
                "temperature": gpu.temperature
            } for i, gpu in enumerate(gpus)]
        except:
            return []

# Usage
monitor = ComprehensiveMonitor()

# Record inference
request = {"model_id": "llama-3.2-3b-instruct", "prompt": "Test"}
result = manager.infer(**request)
monitor.record_inference(request, result)

# Get performance summary
summary = monitor.get_performance_summary()
print(f"RPS: {summary['requests_per_second']:.2f}")
print(f"Success rate: {summary['success_rate']:.1%}")
print(f"Avg response time: {summary['avg_response_time']:.2f}s")
```

### Alerting System

Implement performance alerting:

```python
from lmstrix.utils.alerts import AlertManager
import smtplib
from email.mime.text import MIMEText

class PerformanceAlerts:
    def __init__(self):
        self.alert_manager = AlertManager()
        self.thresholds = {
            "max_response_time": 30.0,      # seconds
            "min_success_rate": 0.95,       # 95%
            "max_memory_usage": 0.85,       # 85%
            "min_tokens_per_second": 20.0,  # TPS
            "max_gpu_temperature": 80.0     # Celsius
        }
        
        self.alert_cooldown = 300  # 5 minutes between same alerts
        self.last_alerts = {}
    
    def check_performance_thresholds(self, metrics):
        """Check if any performance thresholds are exceeded."""
        alerts = []
        
        # Response time alert
        if metrics.get("avg_response_time", 0) > self.thresholds["max_response_time"]:
            alerts.append({
                "type": "high_response_time",
                "severity": "warning",
                "message": f"Average response time {metrics['avg_response_time']:.2f}s exceeds threshold {self.thresholds['max_response_time']}s",
                "value": metrics["avg_response_time"],
                "threshold": self.thresholds["max_response_time"]
            })
        
        # Success rate alert
        if metrics.get("success_rate", 1.0) < self.thresholds["min_success_rate"]:
            alerts.append({
                "type": "low_success_rate",
                "severity": "critical",
                "message": f"Success rate {metrics['success_rate']:.1%} below threshold {self.thresholds['min_success_rate']:.1%}",
                "value": metrics["success_rate"],
                "threshold": self.thresholds["min_success_rate"]
            })
        
        # Memory usage alert
        memory_usage = metrics.get("current_memory_usage", {}).get("percent", 0) / 100
        if memory_usage > self.thresholds["max_memory_usage"]:
            alerts.append({
                "type": "high_memory_usage",
                "severity": "warning",
                "message": f"Memory usage {memory_usage:.1%} exceeds threshold {self.thresholds['max_memory_usage']:.1%}",
                "value": memory_usage,
                "threshold": self.thresholds["max_memory_usage"]
            })
        
        # Throughput alert
        if metrics.get("tokens_per_second", 0) < self.thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "low_throughput",
                "severity": "warning",
                "message": f"Throughput {metrics['tokens_per_second']:.1f} TPS below threshold {self.thresholds['min_tokens_per_second']} TPS",
                "value": metrics["tokens_per_second"],
                "threshold": self.thresholds["min_tokens_per_second"]
            })
        
        # GPU temperature alerts
        for gpu in metrics.get("current_gpu_usage", []):
            if gpu["temperature"] > self.thresholds["max_gpu_temperature"]:
                alerts.append({
                    "type": "high_gpu_temperature",
                    "severity": "critical",
                    "message": f"GPU {gpu['gpu_id']} temperature {gpu['temperature']}°C exceeds threshold {self.thresholds['max_gpu_temperature']}°C",
                    "value": gpu["temperature"],
                    "threshold": self.thresholds["max_gpu_temperature"]
                })
        
        # Send alerts (with cooldown)
        for alert in alerts:
            self.send_alert_if_needed(alert)
        
        return alerts
    
    def send_alert_if_needed(self, alert):
        """Send alert if cooldown period has passed."""
        current_time = time.time()
        alert_key = f"{alert['type']}_{alert.get('gpu_id', '')}"
        
        # Check cooldown
        if alert_key in self.last_alerts:
            if current_time - self.last_alerts[alert_key] < self.alert_cooldown:
                return  # Still in cooldown
        
        # Send alert
        self.send_alert(alert)
        self.last_alerts[alert_key] = current_time
    
    def send_alert(self, alert):
        """Send alert notification."""
        # Email notification
        self.send_email_alert(alert)
        
        # Log alert
        self.log_alert(alert)
        
        # Webhook notification (if configured)
        self.send_webhook_alert(alert)
    
    def send_email_alert(self, alert):
        """Send email alert."""
        # Email configuration would be loaded from config
        email_config = {
            "smtp_server": "smtp.gmail.com",
            "smtp_port": 587,
            "username": "alerts@yourcompany.com",
            "password": "your-password",
            "recipients": ["admin@yourcompany.com"]
        }
        
        subject = f"LMStrix Alert: {alert['type'].replace('_', ' ').title()}"
        body = f"""
        Alert: {alert['message']}
        
        Severity: {alert['severity']}
        Current Value: {alert['value']}
        Threshold: {alert['threshold']}
        Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        # Send email (simplified)
        try:
            msg = MIMEText(body)
            msg['Subject'] = subject
            msg['From'] = email_config['username']
            msg['To'] = ', '.join(email_config['recipients'])
            
            with smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port']) as server:
                server.starttls()
                server.login(email_config['username'], email_config['password'])
                server.send_message(msg)
        except Exception as e:
            print(f"Failed to send email alert: {e}")
```

## 🏭 Production Deployment

### Production Configuration

Optimize for production environments:

```python
# production_config.py
PRODUCTION_CONFIG = {
    "lmstudio": {
        "base_url": "http://lmstudio-server:1234",
        "timeout": 300,
        "connect_timeout": 30,
        "max_retries": 5,
        "retry_delay": 2.0,
        "connection_pool_size": 20
    },
    
    "performance": {
        "enable_caching": True,
        "cache_ttl": 3600,
        "batch_size": 10,
        "batch_timeout": 1.0,
        "max_concurrent_requests": 50,
        "request_queue_size": 1000
    },
    
    "monitoring": {
        "enable_metrics": True,
        "metrics_interval": 60,
        "enable_alerts": True,
        "alert_cooldown": 300,
        "log_level": "INFO"
    },
    
    "safety": {
        "max_context_threshold": 65536,
        "memory_usage_limit": 0.8,
        "enable_graceful_degradation": True,
        "fallback_models": ["llama-3.2-3b-instruct"],
        "circuit_breaker_threshold": 0.5
    }
}
```

### Load Balancing

Implement load balancing across multiple LM Studio instances:

```python
from lmstrix.utils.load_balancer import LoadBalancer
import random
import asyncio

class LMStudioLoadBalancer:
    def __init__(self, servers):
        self.servers = servers
        self.server_health = {server: True for server in servers}
        self.server_metrics = {server: {"requests": 0, "avg_response_time": 0} for server in servers}
        self.health_check_interval = 30  # seconds
        
        # Start health checking
        asyncio.create_task(self.health_check_loop())
    
    async def select_server(self, request):
        """Select optimal server for request."""
        healthy_servers = [s for s in self.servers if self.server_health[s]]
        
        if not healthy_servers:
            raise Exception("No healthy servers available")
        
        # Load balancing strategy: least connections
        selected_server = min(
            healthy_servers,
            key=lambda s: self.server_metrics[s]["requests"]
        )
        
        return selected_server
    
    async def execute_request(self, request):
        """Execute request with load balancing."""
        server = await self.select_server(request)
        
        # Track request
        self.server_metrics[server]["requests"] += 1
        
        try:
            # Execute request
            start_time = time.time()
            result = await self.send_request_to_server(server, request)
            response_time = time.time() - start_time
            
            # Update metrics
            self.update_server_metrics(server, response_time, success=True)
            return result
            
        except Exception as e:
            # Update metrics
            self.update_server_metrics(server, 0, success=False)
            
            # Try another server if available
            healthy_servers = [s for s in self.servers if self.server_health[s] and s != server]
            if healthy_servers:
                return await self.execute_request(request)
            
            raise e
        finally:
            # Decrement request count
            self.server_metrics[server]["requests"] -= 1
    
    def update_server_metrics(self, server, response_time, success):
        """Update server performance metrics."""
        metrics = self.server_metrics[server]
        
        if success:
            # Update rolling average response time
            current_avg = metrics["avg_response_time"]
            metrics["avg_response_time"] = (current_avg * 0.9) + (response_time * 0.1)
        else:
            # Mark server as potentially unhealthy after failures
            # (health check will verify)
            pass
    
    async def health_check_loop(self):
        """Continuously check server health."""
        while True:
            await asyncio.sleep(self.health_check_interval)
            await self.check_all_servers_health()
    
    async def check_all_servers_health(self):
        """Check health of all servers."""
        for server in self.servers:
            try:
                # Simple health check
                response = await self.send_health_check(server)
                self.server_health[server] = response.get("healthy", False)
            except:
                self.server_health[server] = False
```

### Auto-Scaling

Implement auto-scaling based on load:

```python
from lmstrix.utils.autoscaler import AutoScaler

class LMStrixAutoScaler:
    def __init__(self):
        self.min_instances = 1
        self.max_instances = 10
        self.current_instances = 1
        self.scale_up_threshold = 0.8    # CPU usage
        self.scale_down_threshold = 0.3
        self.scale_cooldown = 300        # 5 minutes
        self.last_scale_action = 0
    
    async def monitor_and_scale(self):
        """Monitor metrics and scale as needed."""
        current_time = time.time()
        
        # Check cooldown
        if current_time - self.last_scale_action < self.scale_cooldown:
            return
        
        # Get current metrics
        metrics = await self.get_current_metrics()
        
        # Scaling decisions
        if metrics["cpu_usage"] > self.scale_up_threshold and metrics["queue_length"] > 10:
            if self.current_instances < self.max_instances:
                await self.scale_up()
                self.last_scale_action = current_time
        
        elif metrics["cpu_usage"] < self.scale_down_threshold and metrics["queue_length"] == 0:
            if self.current_instances > self.min_instances:
                await self.scale_down()
                self.last_scale_action = current_time
    
    async def scale_up(self):
        """Add new instance."""
        print(f"Scaling up from {self.current_instances} to {self.current_instances + 1} instances")
        
        # Start new LM Studio instance (implementation depends on deployment)
        new_instance = await self.start_new_instance()
        
        if new_instance:
            self.current_instances += 1
            # Add to load balancer
            load_balancer.add_server(new_instance)
    
    async def scale_down(self):
        """Remove instance."""
        print(f"Scaling down from {self.current_instances} to {self.current_instances - 1} instances")
        
        # Remove least used instance
        instance_to_remove = await self.select_instance_to_remove()
        
        if instance_to_remove:
            # Gracefully drain requests
            await self.drain_instance(instance_to_remove)
            
            # Remove from load balancer
            load_balancer.remove_server(instance_to_remove)
            
            # Stop instance
            await self.stop_instance(instance_to_remove)
            
            self.current_instances -= 1
```

## 🔍 Performance Optimization Checklist

### Pre-Production Checklist

- [ ] **Model Selection Optimized**
  - [ ] Tested all available models for use case
  - [ ] Selected optimal model for performance/quality trade-off
  - [ ] Documented model performance characteristics

- [ ] **Context Limits Tested**
  - [ ] Ran comprehensive context testing
  - [ ] Set appropriate safety thresholds
  - [ ] Documented optimal context sizes

- [ ] **Parameters Optimized**
  - [ ] Tuned temperature, top_p, top_k for use case
  - [ ] Optimized output token allocation
  - [ ] Tested parameter combinations

- [ ] **Infrastructure Optimized**
  - [ ] Configured connection pooling
  - [ ] Implemented request batching
  - [ ] Set up multi-level caching
  - [ ] Configured load balancing

- [ ] **Monitoring Implemented**
  - [ ] Performance metrics collection
  - [ ] Alert thresholds configured
  - [ ] Health check endpoints
  - [ ] Resource usage monitoring

- [ ] **Fallback Strategies**
  - [ ] Circuit breaker implemented
  - [ ] Graceful degradation configured
  - [ ] Fallback models identified
  - [ ] Error handling optimized

### Production Monitoring

```bash
# Key metrics to monitor
curl http://lmstrix-api/metrics | jq '{
  "requests_per_second": .requests_per_second,
  "average_response_time": .avg_response_time,
  "success_rate": .success_rate,
  "tokens_per_second": .tokens_per_second,
  "memory_usage_percent": .memory_usage_percent,
  "active_models": .active_models,
  "queue_length": .queue_length
}'
```

## 🚀 Next Steps

With performance optimization mastered:

- **Monitor Production** - Continuously track performance metrics
- **Iterate and Improve** - Regular optimization based on real usage
- **Scale Strategically** - Plan for growth and increased load
- **Stay Updated** - Keep up with LMStrix and model improvements

---

*Performance optimization mastered! Deploy with confidence! 🏭*
</document_content>
</document>

<document index="53">
<source>src_docs/md/prompt-templating.md</source>
<document_content>
---
# this_file: src_docs/md/prompt-templating.md
title: Prompt Templating Guide
description: Master advanced prompt engineering with TOML templates, variable substitution, and reusable prompt libraries
---

# Prompt Templating Guide

LMStrix's prompt templating system enables sophisticated prompt engineering through TOML-based templates, variable substitution, conditional logic, and reusable prompt libraries. This guide covers everything from basic templates to advanced prompt engineering patterns.

## 🎯 Template Basics

### What are Prompt Templates?

Prompt templates are reusable prompt patterns that:

- **Separate structure from content** - Define prompt logic once, use with different data
- **Enable variable substitution** - Dynamic content injection
- **Promote consistency** - Standardize prompt formats across applications
- **Facilitate maintenance** - Update prompts in one place
- **Support versioning** - Track prompt evolution over time

### TOML Template Format

LMStrix uses TOML (Tom's Obvious Minimal Language) for template definition:

```toml
# Basic template structure
[template_name]
prompt = "Your prompt template with {{variables}}"
description = "Optional template description"
version = "1.0"
tags = ["category", "use_case"]

[another_template]
prompt = """
Multi-line prompt template
with {{variable1}} and {{variable2}}
"""
```

## 📝 Basic Templates

### Simple Variable Substitution

Create a basic template file `basic_prompts.toml`:

```toml
[greeting]
prompt = "Hello {{name}}, how can I help you today?"
description = "Simple greeting template"

[question]
prompt = "Please answer this question: {{question}}"

[summary]
prompt = "Create a brief summary of the following text: {{text}}"
```

Use with CLI:

```bash
# Use greeting template
lmstrix infer greeting --file-prompt basic_prompts.toml --text "Alice"

# Use summary template with file input
lmstrix infer summary --file-prompt basic_prompts.toml --text-file document.txt

# Use question template
lmstrix infer question --file-prompt basic_prompts.toml --text "What is machine learning?"
```

Use with Python API:

```python
from lmstrix.core.inference_manager import InferenceManager

manager = InferenceManager()

result = manager.infer_with_template(
    template_name="summary",
    model_id="llama-3.2-3b-instruct",
    template_file="basic_prompts.toml",
    variables={"text": "Content to summarize..."},
    out_ctx=200
)

print(result['response'])
```

### Multi-Variable Templates

```toml
[analysis]
prompt = """
Analyze the following {{content_type}} about {{topic}}:

Content: {{content}}

Please provide:
1. Key insights about {{topic}}
2. {{analysis_type}} analysis
3. Recommendations
"""
description = "Flexible analysis template"

[comparison]
prompt = """
Compare {{item1}} and {{item2}} based on the following criteria:
- {{criteria1}}
- {{criteria2}}
- {{criteria3}}

Item 1: {{item1_description}}
Item 2: {{item2_description}}

Provide a detailed comparison and recommendation.
"""
```

Usage:

```python
result = manager.infer_with_template(
    template_name="analysis",
    model_id="mistral-7b-instruct",
    template_file="prompts.toml",
    variables={
        "content_type": "research paper",
        "topic": "artificial intelligence",
        "content": "Paper content here...",
        "analysis_type": "technical"
    }
)
```

## 🔧 Advanced Template Features

### Conditional Logic

Use conditional blocks for dynamic prompt construction:

```toml
[conditional_summary]
prompt = """
{{#if detailed}}
Create a comprehensive, detailed summary of the following text.
Include key points, supporting details, and analysis.
{{else}}
Create a brief summary of the following text.
Focus on the main points only.
{{/if}}

{{#if include_keywords}}
Also extract the top 5 keywords from the text.
{{/if}}

Text: {{text}}
"""

[adaptive_response]
prompt = """
{{#if user_level == "beginner"}}
Explain {{topic}} in simple terms that a beginner can understand.
Use analogies and avoid technical jargon.
{{else if user_level == "intermediate"}}
Explain {{topic}} with moderate technical detail.
Include some technical terms but explain them.
{{else}}
Provide a detailed technical explanation of {{topic}}.
Use appropriate technical terminology and assume prior knowledge.
{{/if}}
"""
```

### Loops and Iteration

Process lists and repeating content:

```toml
[multi_question]
prompt = """
Please answer the following questions:

{{#each questions}}
{{@index}}. {{this}}
{{/each}}

Provide clear, concise answers for each question.
"""

[batch_analysis]
prompt = """
Analyze each of the following items:

{{#each items}}
Item {{@index}}: {{this.name}}
Description: {{this.description}}
{{#if this.priority}}Priority: {{this.priority}}{{/if}}

{{/each}}

Provide insights for each item and overall recommendations.
"""
```

### Nested Templates

Build complex prompts from smaller components:

```toml
[role_prompt]
prompt = """
You are {{role.name}}, {{role.description}}.

{{role.instructions}}

{{#if role.constraints}}
Important constraints:
{{#each role.constraints}}
- {{this}}
{{/each}}
{{/if}}
"""

[task_prompt]
prompt = """
{{> role_prompt}}

Your task is to {{task.action}} the following {{task.type}}:

{{task.content}}

{{#if task.requirements}}
Requirements:
{{#each task.requirements}}
- {{this}}
{{/each}}
{{/if}}

{{task.output_format}}
"""
```

### Template Inheritance

Create template hierarchies:

```toml
# Base template
[base_analysis]
prompt = """
{{#block "introduction"}}
Analyze the following content:
{{/block}}

{{#block "content"}}
{{content}}
{{/block}}

{{#block "instructions"}}
Provide a thorough analysis.
{{/block}}
"""

# Specialized template extending base
[technical_analysis]
extends = "base_analysis"
prompt = """
{{#block "introduction"}}
Perform a technical analysis of the following content:
{{/block}}

{{#block "instructions"}}
Focus on:
1. Technical accuracy
2. Implementation details
3. Performance implications
4. Best practices
{{/block}}
"""
```

## 📚 Template Libraries

### Organizing Templates

Structure templates by category and use case:

```
prompts/
├── analysis/
│   ├── general.toml
│   ├── technical.toml
│   └── financial.toml
├── creative/
│   ├── writing.toml
│   ├── brainstorming.toml
│   └── storytelling.toml
├── qa/
│   ├── educational.toml
│   ├── technical.toml
│   └── conversational.toml
└── utility/
    ├── summarization.toml
    ├── translation.toml
    └── formatting.toml
```

### Analysis Templates

```toml
# analysis/general.toml
[comprehensive_analysis]
prompt = """
Conduct a comprehensive analysis of: {{subject}}

Context: {{context}}
Data: {{data}}

Analysis Framework:
1. Executive Summary
2. Key Findings
3. Detailed Analysis
4. Implications
5. Recommendations
6. Next Steps

Please be thorough and objective in your analysis.
"""

[swot_analysis]
prompt = """
Perform a SWOT analysis for: {{subject}}

Context: {{context}}

Please analyze:
- Strengths: Internal positive factors
- Weaknesses: Internal areas for improvement  
- Opportunities: External positive possibilities
- Threats: External potential challenges

{{#if include_matrix}}
Present results in a structured matrix format.
{{/if}}
"""

[comparative_analysis]
prompt = """
Compare and contrast {{item1}} and {{item2}}.

Comparison criteria:
{{#each criteria}}
- {{this}}
{{/each}}

{{#if context}}
Context: {{context}}
{{/if}}

Provide:
1. Side-by-side comparison
2. Advantages and disadvantages of each
3. Use case recommendations
4. Overall assessment
"""
```

### Creative Writing Templates

```toml
# creative/writing.toml
[story_generator]
prompt = """
Write a {{genre}} story with the following elements:

Setting: {{setting}}
Main character: {{protagonist}}
Conflict: {{conflict}}
{{#if theme}}Theme: {{theme}}{{/if}}

Requirements:
- Length: {{length}} words
- Tone: {{tone}}
- Target audience: {{audience}}

{{#if constraints}}
Additional constraints:
{{#each constraints}}
- {{this}}
{{/each}}
{{/if}}
"""

[character_development]
prompt = """
Develop a detailed character profile for: {{character_name}}

Basic Information:
- Role in story: {{role}}
- Age: {{age}}
- Background: {{background}}

Please create:
1. Physical description
2. Personality traits
3. Backstory
4. Motivations and goals
5. Character arc
6. Relationships with other characters
7. Distinctive dialogue style

{{#if additional_notes}}
Additional notes: {{additional_notes}}
{{/if}}
"""

[dialogue_writer]
prompt = """
Write a dialogue between {{character1}} and {{character2}}.

Setting: {{setting}}
Situation: {{situation}}
Mood: {{mood}}

Character 1 ({{character1}}):
- Personality: {{char1_personality}}
- Goal in conversation: {{char1_goal}}

Character 2 ({{character2}}):
- Personality: {{char2_personality}}  
- Goal in conversation: {{char2_goal}}

Write natural, engaging dialogue that advances the plot and reveals character.
"""
```

### Educational Templates

```toml
# qa/educational.toml
[explain_concept]
prompt = """
Explain {{concept}} to a {{audience_level}} audience.

{{#if context}}
Context: {{context}}
{{/if}}

Please include:
1. Clear definition
2. Key characteristics
3. Real-world examples
4. {{#if audience_level == "beginner"}}Simple analogies{{else}}Practical applications{{/if}}
5. Common misconceptions (if any)

{{#if related_concepts}}
Also briefly mention how it relates to: {{related_concepts}}
{{/if}}

Use {{tone}} tone and make it engaging.
"""

[step_by_step_guide]
prompt = """
Create a step-by-step guide for: {{task}}

Audience: {{audience}}
Difficulty level: {{difficulty}}
{{#if prerequisites}}Prerequisites: {{prerequisites}}{{/if}}

Format:
1. Overview and goals
2. Required materials/tools
3. Detailed step-by-step instructions
4. Tips and best practices
5. Common pitfalls to avoid
6. Troubleshooting guide

{{#if time_estimate}}
Estimated time: {{time_estimate}}
{{/if}}
"""

[quiz_generator]
prompt = """
Create a {{quiz_type}} quiz about {{topic}}.

Requirements:
- Number of questions: {{num_questions}}
- Difficulty: {{difficulty}}
- Question types: {{question_types}}

{{#if learning_objectives}}
Learning objectives:
{{#each learning_objectives}}
- {{this}}
{{/each}}
{{/if}}

Include:
1. Questions with multiple choice answers
2. Correct answers
3. Explanations for each answer
4. Difficulty rationale
"""
```

## 🛠️ Template Development Tools

### Template Validation

Validate templates before use:

```python
from lmstrix.loaders.prompt_loader import PromptLoader, TemplateValidator

validator = TemplateValidator()

# Validate template file
validation_result = validator.validate_file("prompts.toml")

if validation_result.is_valid:
    print("✅ Template file is valid")
else:
    print("❌ Validation errors:")
    for error in validation_result.errors:
        print(f"  • {error}")
```

### Template Testing

Test templates with sample data:

```python
from lmstrix.utils.template_tester import TemplateTester

tester = TemplateTester()

# Test template with sample variables
test_result = tester.test_template(
    template_file="prompts.toml",
    template_name="analysis",
    test_variables={
        "subject": "Python programming",
        "context": "Educational tutorial",
        "data": "Sample code examples"
    },
    model_id="llama-3.2-3b-instruct"
)

print(f"Template test result: {test_result.success}")
print(f"Generated prompt length: {test_result.prompt_length}")
print(f"Response quality score: {test_result.quality_score}")
```

### Template Analytics

Analyze template performance:

```python
from lmstrix.utils.template_analytics import TemplateAnalytics

analytics = TemplateAnalytics()

# Track template usage
analytics.track_usage("analysis", {
    "model_id": "llama-3.2-3b-instruct",
    "success": True,
    "response_time": 5.2,
    "tokens_used": 342,
    "user_rating": 4.5
})

# Get template performance metrics
metrics = analytics.get_template_metrics("analysis")
print(f"Usage count: {metrics['usage_count']}")
print(f"Average rating: {metrics['avg_rating']:.2f}")
print(f"Success rate: {metrics['success_rate']:.1%}")
print(f"Average response time: {metrics['avg_response_time']:.1f}s")
```

## 🎨 Prompt Engineering Best Practices

### Template Design Principles

1. **Clarity and Specificity**
   ```toml
   # Good: Specific and clear
   [bug_report_analysis]
   prompt = """
   Analyze this bug report and categorize it by:
   - Severity (Critical, High, Medium, Low)
   - Component affected
   - Root cause category
   - Estimated effort to fix
   
   Bug report: {{bug_description}}
   """
   
   # Avoid: Vague and ambiguous
   [generic_analysis]
   prompt = "Analyze this: {{content}}"
   ```

2. **Consistent Structure**
   ```toml
   # Use consistent formatting patterns
   [structured_template]
   prompt = """
   ## Task: {{task_title}}
   
   ### Context
   {{context}}
   
   ### Requirements
   {{#each requirements}}
   - {{this}}
   {{/each}}
   
   ### Output Format
   {{output_format}}
   """
   ```

3. **Flexible Parameters**
   ```toml
   [adaptive_explanation]
   prompt = """
   {{#if audience_level}}
   Explain {{topic}} for a {{audience_level}} audience.
   {{else}}
   Explain {{topic}} in general terms.
   {{/if}}
   
   {{#if max_length}}
   Keep the explanation under {{max_length}} words.
   {{/if}}
   
   {{#if include_examples}}
   Include practical examples.
   {{/if}}
   """
   ```

### Variable Naming Conventions

Use clear, descriptive variable names:

```toml
# Good: Descriptive names
[content_analysis]
prompt = """
Analyze the {{content_type}} titled "{{content_title}}" 
from the perspective of {{analysis_perspective}}.

Content: {{content_body}}
Target audience: {{target_audience}}
Analysis depth: {{analysis_depth}}
"""

# Avoid: Generic names
[analysis]
prompt = "Analyze {{x}} using {{y}} for {{z}}"
```

### Error Handling in Templates

Build robust templates with fallbacks:

```toml
[robust_template]
prompt = """
{{#if task}}
Task: {{task}}
{{else}}
Task: General analysis
{{/if}}

{{#if content}}
Content to analyze: {{content}}
{{else}}
No content provided. Please provide content for analysis.
{{/if}}

{{#if requirements}}
Requirements:
{{#each requirements}}
- {{this}}
{{/each}}
{{else}}
Default requirements:
- Be thorough and objective
- Provide actionable insights
{{/if}}
"""
```

## 🔄 Template Workflows

### Development Workflow

1. **Design Phase**
   - Define use case and requirements
   - Identify variables and logic needed
   - Sketch template structure

2. **Implementation Phase**
   - Write initial template
   - Add variable substitution
   - Implement conditional logic

3. **Testing Phase**
   - Validate template syntax
   - Test with sample data
   - Evaluate output quality

4. **Optimization Phase**
   - Refine prompts based on results
   - Optimize for different models
   - Add error handling

5. **Production Phase**
   - Deploy to template library
   - Monitor usage and performance
   - Iterate based on feedback

### Version Control

Track template evolution:

```toml
[analysis_v1]
prompt = "Basic analysis template"
version = "1.0"
deprecated = true
replaced_by = "analysis_v2"

[analysis_v2]
prompt = "Improved analysis template with better structure"
version = "2.0"
changelog = [
    "Added structured output format",
    "Improved clarity of instructions",
    "Added support for multiple analysis types"
]
```

### A/B Testing Templates

Compare template performance:

```python
from lmstrix.utils.ab_testing import TemplateABTester

tester = TemplateABTester()

# Set up A/B test
test_id = tester.create_test(
    name="analysis_template_comparison",
    template_a="analysis_v1",
    template_b="analysis_v2",
    test_variables={"content": "sample_content"},
    models=["llama-3.2-3b-instruct", "mistral-7b-instruct"],
    metrics=["response_quality", "response_time", "user_satisfaction"]
)

# Run test
results = tester.run_test(test_id, sample_size=100)

# Analyze results
winner = tester.analyze_results(test_id)
print(f"Winning template: {winner.template_name}")
print(f"Confidence level: {winner.confidence:.1%}")
print(f"Performance improvement: {winner.improvement:.1%}")
```

## 📊 Template Performance Optimization

### Model-Specific Optimization

Optimize templates for different models:

```toml
# Template optimized for smaller models (3B-7B)
[analysis_small_model]
prompt = """
Analyze: {{content}}

Focus on:
1. Main points
2. Key insights  
3. Brief recommendations

Keep response concise and clear.
"""

# Template optimized for larger models (13B+)
[analysis_large_model]
prompt = """
Conduct a comprehensive analysis of the following content,
demonstrating deep understanding and nuanced reasoning:

{{content}}

Please provide:
1. Executive summary with key findings
2. Detailed analysis with supporting evidence
3. Multiple perspectives and viewpoints
4. Implications and consequences
5. Actionable recommendations with rationale
6. Potential challenges and mitigation strategies

Use sophisticated reasoning and provide comprehensive insights.
"""
```

### Dynamic Template Selection

Automatically choose optimal templates:

```python
from lmstrix.utils.template_selector import SmartTemplateSelector

selector = SmartTemplateSelector()

# Register template variants
selector.register_template_variant("analysis", "small_model", {
    "model_size_range": (0, 7000),  # Parameters in millions
    "template_file": "analysis_optimized.toml",
    "template_name": "analysis_small_model"
})

selector.register_template_variant("analysis", "large_model", {
    "model_size_range": (7000, None),
    "template_file": "analysis_optimized.toml", 
    "template_name": "analysis_large_model"
})

# Automatically select best template
best_template = selector.select_template(
    base_template="analysis",
    model_id="llama-3.2-3b-instruct",
    context_size=2048
)

# Use selected template
result = manager.infer_with_template(
    template_name=best_template.name,
    template_file=best_template.file,
    model_id="llama-3.2-3b-instruct",
    variables={"content": "Content to analyze"}
)
```

## 🚀 Advanced Use Cases

### Dynamic Prompt Assembly

Build prompts dynamically based on context:

```python
from lmstrix.core.prompt_builder import DynamicPromptBuilder

builder = DynamicPromptBuilder()

# Define prompt components
builder.add_component("role", "You are an expert {{domain}} analyst.")
builder.add_component("task", "Analyze the following {{content_type}}:")
builder.add_component("content", "{{content}}")
builder.add_component("format", "Provide analysis in {{output_format}} format.")

# Build prompt based on context
prompt = builder.build({
    "domain": "financial",
    "content_type": "quarterly report",
    "content": "Q3 financial data...",
    "output_format": "structured"
})
```

### Multi-Turn Conversation Templates

Design templates for conversational AI:

```toml
[conversation_starter]
prompt = """
{{#if context.is_new_conversation}}
Hello! I'm here to help with {{topic}}. 
What would you like to know?
{{else}}
Continuing our discussion about {{topic}}.
What's your next question?
{{/if}}

{{#if context.previous_topics}}
We've previously discussed: {{context.previous_topics}}
{{/if}}
"""

[follow_up_handler]
prompt = """
Based on our previous discussion about {{previous_topic}},
you're now asking about {{current_topic}}.

Previous context: {{conversation_history}}

Current question: {{user_question}}

Please provide a helpful response that:
1. Acknowledges the previous context
2. Directly addresses the current question
3. Suggests related topics if relevant
"""
```

### Prompt Chaining

Chain multiple prompts for complex tasks:

```python
from lmstrix.utils.prompt_chain import PromptChain

chain = PromptChain()

# Define chain steps
chain.add_step("extract_key_points", {
    "template": "key_point_extraction",
    "variables": {"content": "{{input}}"}
})

chain.add_step("analyze_points", {
    "template": "point_analysis", 
    "variables": {"points": "{{extract_key_points.output}}"}
})

chain.add_step("generate_summary", {
    "template": "final_summary",
    "variables": {
        "points": "{{extract_key_points.output}}",
        "analysis": "{{analyze_points.output}}"
    }
})

# Execute chain
result = chain.execute(
    input_data={"input": "Long document content..."},
    model_id="llama-3.2-3b-instruct"
)

print(f"Final summary: {result.final_output}")
print(f"Chain execution time: {result.total_time:.2f}s")
```

## 🚀 Next Steps

Master prompt templating, then explore:

- **[Performance & Optimization](performance.md)** - Production optimization techniques
- **[Model Management](model-management.md)** - Advanced model operations
- **[Python API](python-api.md)** - Programmatic template usage
- **[CLI Interface](cli-interface.md)** - Command-line template workflows

---

*Prompt templating mastered! Craft powerful, reusable prompts! 🎨*
</document_content>
</document>

<document index="54">
<source>src_docs/md/python-api.md</source>
<document_content>
---
# this_file: src_docs/md/python-api.md
title: Python API Reference
description: Comprehensive Python API documentation with code examples, class references, and integration patterns
---

# Python API Reference

The LMStrix Python API provides programmatic access to all functionality available in the CLI, with additional flexibility for integration into your Python applications, scripts, and workflows.

## 🚀 Quick Start

### Basic Usage

```python
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.core.inference_manager import InferenceManager

# Load model registry
registry = load_model_registry()

# List available models
models = registry.list_models()
print(f"Available models: {len(models)}")

# Run inference
manager = InferenceManager(verbose=True)
result = manager.infer(
    model_id="llama-3.2-3b-instruct",
    prompt="What is the meaning of life?",
    out_ctx=100,
    temperature=0.7
)

if result["succeeded"]:
    print(f"Response: {result['response']}")
    print(f"Tokens: {result['tokens_used']}")
```

## 📚 Core Classes

### InferenceManager

The main interface for running inference operations.

```python
from lmstrix.core.inference_manager import InferenceManager

# Create manager with options
manager = InferenceManager(
    verbose=True,           # Enable detailed logging
    base_url="http://localhost:1234",  # LM Studio URL
    timeout=300,            # Request timeout
    max_retries=3           # Retry attempts
)
```

#### Methods

##### `infer()`

Run text generation inference.

```python
result = manager.infer(
    model_id: str,                    # Model identifier
    prompt: str,                      # Input prompt
    out_ctx: int | str = "auto",      # Output length
    temperature: float = 0.7,         # Creativity (0.0-2.0)
    top_p: float = 1.0,              # Nucleus sampling
    top_k: int = -1,                 # Top-k sampling
    max_tokens: int | None = None,    # Max output tokens
    system_prompt: str | None = None, # System prompt
    context_data: dict | None = None  # Additional context
) -> dict
```

**Return value:**
```python
{
    "succeeded": bool,           # Whether inference succeeded
    "response": str,             # Generated text
    "tokens_used": int,          # Total tokens consumed
    "prompt_tokens": int,        # Tokens in prompt
    "response_tokens": int,      # Tokens in response
    "inference_time": float,     # Generation time (seconds)
    "time_to_first_token": float,# Latency to first token
    "tokens_per_second": float,  # Generation speed
    "stop_reason": str,          # Why generation stopped
    "error": str | None,         # Error message if failed
    "model_info": dict           # Model metadata
}
```

##### `infer_with_template()`

Use prompt templates for inference.

```python
result = manager.infer_with_template(
    template_name: str,              # Template identifier
    model_id: str,                   # Model to use
    template_file: str,              # TOML template file
    variables: dict,                 # Template variables
    **kwargs                         # Additional infer() options
)
```

### ModelRegistry

Manage the model registry and metadata.

```python
from lmstrix.loaders.model_loader import load_model_registry

# Load registry
registry = load_model_registry()

# Alternative: create new registry
from lmstrix.core.models import ModelRegistry
registry = ModelRegistry()
```

#### Methods

##### Model Discovery

```python
# List all models
models = registry.list_models()

# Get specific model
model = registry.get_model("llama-3.2-3b-instruct")

# Check if model exists
exists = registry.has_model("model-name")

# Filter models
tested_models = registry.filter_models(status="tested")
large_models = registry.filter_models(size_range=(5000, 20000))  # 5GB-20GB
```

##### Model Information

```python
# Get model details
model_info = registry.get_model_info("llama-3.2-3b-instruct")
print(f"Size: {model_info['size_mb']} MB")
print(f"Context: {model_info['tested_context']}")
print(f"Status: {model_info['test_status']}")

# Get model statistics
stats = registry.get_statistics()
print(f"Total models: {stats['total']}")
print(f"Tested: {stats['tested']}")
print(f"Average context: {stats['avg_context']}")
```

##### Registry Management

```python
# Save registry
registry.save()

# Reload from disk
registry.reload()

# Add model manually
registry.add_model({
    "id": "custom-model",
    "name": "Custom Model",
    "size_mb": 3500,
    "parameters": "3.5B"
})

# Update model info
registry.update_model("model-id", {
    "tested_context": 32768,
    "test_status": "tested",
    "last_tested": "2024-01-15T10:30:00Z"
})

# Remove model
registry.remove_model("model-id")
```

### ContextTester

Binary search algorithm for finding optimal context limits.

```python
from lmstrix.core.context_tester import ContextTester

# Create tester
tester = ContextTester(
    base_url="http://localhost:1234",
    timeout=300,
    max_iterations=20,
    safety_threshold=65536
)
```

#### Methods

##### `test_model_context()`

Test context limit for a specific model.

```python
result = tester.test_model_context(
    model_id: str,                    # Model to test
    threshold: int = 65536,           # Maximum context to test
    prompt: str | None = None,        # Custom test prompt
    reset: bool = False,              # Reset existing results
    verbose: bool = False             # Detailed progress
) -> dict
```

**Return value:**
```python
{
    "model_id": str,               # Model identifier
    "optimal_context": int,        # Maximum working context
    "test_successful": bool,       # Whether test completed
    "iterations": int,             # Number of test iterations
    "total_time": float,           # Total test time
    "error": str | None,           # Error if test failed
    "test_points": list,           # All tested context sizes
    "final_working_size": int,     # Last confirmed working size
    "failure_point": int | None    # First size that failed
}
```

##### `test_multiple_models()`

Test multiple models in sequence or parallel.

```python
results = tester.test_multiple_models(
    model_ids: list[str],            # Models to test
    threshold: int = 65536,          # Safety threshold
    parallel: bool = False,          # Run in parallel
    max_workers: int = 2             # Parallel worker count
) -> list[dict]
```

### Scanner

Discover models available in LM Studio.

```python
from lmstrix.core.scanner import Scanner

# Create scanner
scanner = Scanner(
    base_url="http://localhost:1234",
    timeout=30
)

# Scan for models
models = scanner.scan_models(refresh=True)

# Check server health
health = scanner.check_health()
```

## 🔧 Utility Classes

### PromptLoader

Load and manage prompt templates.

```python
from lmstrix.loaders.prompt_loader import PromptLoader

# Create loader
loader = PromptLoader()

# Load prompts from TOML file
prompts = loader.load_prompts("templates.toml")

# Get specific prompt
prompt = loader.get_prompt("summary", prompts)

# Render prompt with variables
rendered = loader.render_prompt(prompt, {"text": "Content to summarize"})
```

### ContextLoader

Load context data from files.

```python
from lmstrix.loaders.context_loader import ContextLoader

# Create loader
loader = ContextLoader()

# Load text file
content = loader.load_text_file("document.txt")

# Load with encoding detection
content = loader.load_text_file("document.txt", encoding="auto")

# Load multiple files
contents = loader.load_multiple_files(["file1.txt", "file2.txt"])
```

### Configuration

Manage LMStrix configuration programmatically.

```python
from lmstrix.utils.config import Config

# Load configuration
config = Config.load()

# Get configuration values
lm_studio_url = config.get("lmstudio.base_url", "http://localhost:1234")
safety_threshold = config.get("safety.max_context_threshold", 65536)

# Update configuration
config.set("output.verbose_by_default", True)
config.save()

# Create custom configuration
custom_config = Config({
    "lmstudio": {"base_url": "http://remote:1234"},
    "safety": {"max_context_threshold": 32768}
})
```

## 🎯 Advanced Usage Patterns

### Async Operations

LMStrix supports async/await for non-blocking operations:

```python
import asyncio
from lmstrix.core.inference_manager import AsyncInferenceManager

async def main():
    manager = AsyncInferenceManager()
    
    # Run multiple inferences concurrently
    tasks = [
        manager.infer("Question 1", "model1"),
        manager.infer("Question 2", "model2"),
        manager.infer("Question 3", "model3")
    ]
    
    results = await asyncio.gather(*tasks)
    
    for i, result in enumerate(results):
        print(f"Result {i+1}: {result['response'][:100]}...")

# Run async function
asyncio.run(main())
```

### Context Management

Handle different context strategies:

```python
from lmstrix.core.context import ContextManager

# Create context manager
ctx_manager = ContextManager()

# Smart context allocation
context_size = ctx_manager.calculate_optimal_context(
    prompt_length=500,
    desired_output=200,
    model_max_context=32768,
    strategy="balanced"  # "conservative", "balanced", "aggressive"
)

# Context validation
is_valid = ctx_manager.validate_context_size(
    prompt="Your prompt here",
    output_tokens=100,
    model_context_limit=16384
)
```

### Error Handling

Robust error handling patterns:

```python
from lmstrix.api.exceptions import (
    LMStrixError,
    ModelNotFoundError,
    ContextLimitExceededError,
    InferenceTimeoutError,
    LMStudioConnectionError
)

try:
    result = manager.infer(
        model_id="nonexistent-model",
        prompt="Test prompt"
    )
except ModelNotFoundError as e:
    print(f"Model not found: {e}")
    # Handle missing model
    
except ContextLimitExceededError as e:
    print(f"Context too large: {e}")
    # Reduce context size or split prompt
    
except InferenceTimeoutError as e:
    print(f"Inference timeout: {e}")
    # Retry with longer timeout
    
except LMStudioConnectionError as e:
    print(f"LM Studio connection failed: {e}")
    # Check if LM Studio is running
    
except LMStrixError as e:
    print(f"General LMStrix error: {e}")
    # Handle other errors
```

### Batch Processing

Process multiple inputs efficiently:

```python
from lmstrix.core.batch_processor import BatchProcessor

# Create batch processor
processor = BatchProcessor(
    model_id="llama-3.2-3b-instruct",
    max_workers=3,
    batch_size=10
)

# Process multiple prompts
prompts = [
    "What is AI?",
    "Explain machine learning",
    "Define neural networks",
    # ... more prompts
]

results = processor.process_batch(prompts)

for prompt, result in zip(prompts, results):
    if result["succeeded"]:
        print(f"Q: {prompt}")
        print(f"A: {result['response']}")
    else:
        print(f"Failed: {prompt} - {result['error']}")
```

### Model Comparison

Compare different models on the same task:

```python
from lmstrix.utils.comparison import ModelComparator

# Create comparator
comparator = ModelComparator([
    "llama-3.2-3b-instruct",
    "mistral-7b-instruct",
    "codellama-13b-python"
])

# Compare models on a task
prompt = "Explain quantum computing"
comparison = comparator.compare_models(
    prompt=prompt,
    metrics=["response_time", "tokens_per_second", "response_length"]
)

# Results include performance metrics for each model
for model_result in comparison:
    print(f"Model: {model_result['model_id']}")
    print(f"Response time: {model_result['response_time']:.2f}s")
    print(f"Speed: {model_result['tokens_per_second']:.1f} tok/s")
    print("---")
```

## 🔌 Integration Examples

### Web Application Integration

```python
from flask import Flask, request, jsonify
from lmstrix.core.inference_manager import InferenceManager

app = Flask(__name__)
inference_manager = InferenceManager()

@app.route('/api/infer', methods=['POST'])
def api_infer():
    data = request.json
    
    try:
        result = inference_manager.infer(
            model_id=data['model'],
            prompt=data['prompt'],
            out_ctx=data.get('max_tokens', 100),
            temperature=data.get('temperature', 0.7)
        )
        
        return jsonify({
            'success': result['succeeded'],
            'response': result['response'],
            'stats': {
                'tokens_used': result['tokens_used'],
                'response_time': result['inference_time']
            }
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)
```

### Data Processing Pipeline

```python
import pandas as pd
from lmstrix.core.inference_manager import InferenceManager

# Load data
df = pd.read_csv('customer_feedback.csv')

# Initialize inference manager
manager = InferenceManager()

# Process each feedback entry
def analyze_sentiment(text):
    result = manager.infer(
        model_id="llama-3.2-3b-instruct",
        prompt=f"Analyze the sentiment of this feedback: {text}",
        out_ctx=50,
        temperature=0.3
    )
    return result['response'] if result['succeeded'] else "Error"

# Apply to dataframe
df['sentiment_analysis'] = df['feedback'].apply(analyze_sentiment)

# Save results
df.to_csv('analyzed_feedback.csv', index=False)
```

### Jupyter Notebook Integration

```python
# Cell 1: Setup
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.loaders.model_loader import load_model_registry
import matplotlib.pyplot as plt

manager = InferenceManager(verbose=True)
registry = load_model_registry()

# Cell 2: Model exploration
models = registry.list_models()
tested_models = [m for m in models if m['test_status'] == 'tested']

# Visualize model contexts
contexts = [m['tested_context'] for m in tested_models]
names = [m['name'] for m in tested_models]

plt.figure(figsize=(12, 6))
plt.bar(names, contexts)
plt.xticks(rotation=45)
plt.title('Model Context Limits')
plt.ylabel('Context Size (tokens)')
plt.tight_layout()
plt.show()

# Cell 3: Interactive inference
def ask_model(question, model_name="llama-3.2-3b-instruct"):
    result = manager.infer(
        model_id=model_name,
        prompt=question,
        out_ctx="auto",
        temperature=0.7
    )
    
    if result['succeeded']:
        print(f"🤖 {model_name}:")
        print(result['response'])
        print(f"\n📊 Stats: {result['tokens_used']} tokens, {result['inference_time']:.1f}s")
    else:
        print(f"❌ Error: {result['error']}")

# Interactive usage
ask_model("What is machine learning?")
```

### Monitoring and Logging

```python
import logging
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.utils.logging import setup_logging

# Setup structured logging
setup_logging(
    level=logging.INFO,
    file_path="lmstrix_app.log",
    include_performance_metrics=True
)

logger = logging.getLogger(__name__)

class MonitoredInferenceManager:
    def __init__(self):
        self.manager = InferenceManager()
        self.request_count = 0
        self.total_tokens = 0
    
    def infer(self, **kwargs):
        self.request_count += 1
        
        logger.info(f"Starting inference request #{self.request_count}")
        logger.info(f"Model: {kwargs.get('model_id')}")
        logger.info(f"Prompt length: {len(kwargs.get('prompt', ''))}")
        
        result = self.manager.infer(**kwargs)
        
        if result['succeeded']:
            self.total_tokens += result['tokens_used']
            logger.info(f"Inference successful: {result['tokens_used']} tokens, {result['inference_time']:.2f}s")
        else:
            logger.error(f"Inference failed: {result['error']}")
        
        logger.info(f"Total requests: {self.request_count}, Total tokens: {self.total_tokens}")
        
        return result

# Usage
monitored_manager = MonitoredInferenceManager()
result = monitored_manager.infer(
    model_id="llama-3.2-3b-instruct",
    prompt="Explain neural networks"
)
```

## 🧪 Testing and Development

### Unit Testing

```python
import unittest
from unittest.mock import Mock, patch
from lmstrix.core.inference_manager import InferenceManager

class TestInferenceManager(unittest.TestCase):
    def setUp(self):
        self.manager = InferenceManager()
    
    @patch('lmstrix.api.client.LMStudioClient')
    def test_successful_inference(self, mock_client):
        # Mock the API response
        mock_client.return_value.chat_completion.return_value = {
            'choices': [{'message': {'content': 'Test response'}}],
            'usage': {'total_tokens': 25, 'prompt_tokens': 10, 'completion_tokens': 15}
        }
        
        result = self.manager.infer(
            model_id="test-model",
            prompt="Test prompt"
        )
        
        self.assertTrue(result['succeeded'])
        self.assertEqual(result['response'], 'Test response')
        self.assertEqual(result['tokens_used'], 25)
    
    def test_invalid_model_id(self):
        with self.assertRaises(ModelNotFoundError):
            self.manager.infer(
                model_id="nonexistent-model",
                prompt="Test prompt"
            )

if __name__ == '__main__':
    unittest.main()
```

### Performance Testing

```python
import time
import statistics
from lmstrix.core.inference_manager import InferenceManager

def benchmark_model(model_id, prompts, iterations=5):
    manager = InferenceManager()
    results = []
    
    for prompt in prompts:
        times = []
        token_rates = []
        
        for _ in range(iterations):
            start_time = time.time()
            result = manager.infer(
                model_id=model_id,
                prompt=prompt,
                out_ctx=100
            )
            
            if result['succeeded']:
                times.append(result['inference_time'])
                token_rates.append(result['tokens_per_second'])
        
        results.append({
            'prompt': prompt,
            'avg_time': statistics.mean(times),
            'avg_token_rate': statistics.mean(token_rates),
            'time_std': statistics.stdev(times) if len(times) > 1 else 0
        })
    
    return results

# Run benchmark
test_prompts = [
    "What is AI?",
    "Explain machine learning in detail",
    "Write a short story about space"
]

benchmark_results = benchmark_model("llama-3.2-3b-instruct", test_prompts)

for result in benchmark_results:
    print(f"Prompt: {result['prompt'][:30]}...")
    print(f"Avg time: {result['avg_time']:.2f}s ± {result['time_std']:.2f}")
    print(f"Avg rate: {result['avg_token_rate']:.1f} tokens/sec")
    print("---")
```

## 🚀 Next Steps

With the Python API mastered:

- **[Context Testing](context-testing.md)** - Deep dive into optimization algorithms
- **[Prompt Templating](prompt-templating.md)** - Advanced prompt engineering
- **[Model Management](model-management.md)** - Registry and model operations
- **[Performance & Optimization](performance.md)** - Production-ready optimization

---

*Python API mastery achieved! Build powerful LM Studio integrations! 🐍*
</document_content>
</document>

<document index="55">
<source>src_docs/md/quickstart.md</source>
<document_content>
---
# this_file: src_docs/md/quickstart.md
title: Quick Start Guide
description: Get up and running with LMStrix in minutes - essential commands and workflows
---

# Quick Start Guide

Get up and running with LMStrix in just a few minutes! This guide walks you through the essential workflows and commands you'll use most frequently.

## 🏁 First Steps

### 1. Verify Installation

```bash
# Check that LMStrix is installed
lmstrix --version

# Get help on available commands
lmstrix --help
```

### 2. Check LM Studio Connection

```bash
# Test connection to LM Studio
lmstrix scan
```

**Expected output:**
```
🔍 Scanning for models...
✅ Found 3 models in LM Studio
📋 Updated model registry
```

!!! tip "LM Studio Not Running?"
    If you get a connection error, make sure LM Studio is running and the local server is enabled. The default URL is `http://localhost:1234`.

## 📋 Essential Workflows

### Workflow 1: Discover Your Models

```bash
# Scan for available models
lmstrix scan --verbose

# List all discovered models
lmstrix list
```

**Sample output:**
```
Model                           Size    Context    Status
llama-3.2-3b-instruct          3.2B    Unknown    Not tested
mistral-7b-instruct            7.1B    Unknown    Not tested
codellama-13b-python           13.0B   Unknown    Not tested
```

### Workflow 2: Test Context Limits

```bash
# Test a specific model's context limit
lmstrix test llama-3.2-3b-instruct

# Test all models with safety threshold
lmstrix test --all --threshold 32768
```

**Live progress display:**
```
Model                           Context      Status
llama-3.2-3b-instruct          16,384       Testing...
→
Model                           Context      Status  
llama-3.2-3b-instruct          32,768       ✓ Success
```

### Workflow 3: Run Inference

```bash
# Simple question with verbose output
lmstrix infer "What is the capital of Poland?" -m llama-3.2-3b-instruct --verbose

# Quick inference without verbose output
lmstrix infer "2+2=" -m llama-3.2-3b-instruct
```

## 🎯 Core Commands Deep Dive

### `scan` - Model Discovery

Discover and catalog models available in LM Studio:

```bash
# Basic scan
lmstrix scan

# Verbose scan with detailed output
lmstrix scan --verbose

# Force refresh (ignore cache)
lmstrix scan --refresh
```

!!! info "What does scan do?"
    - Connects to LM Studio's API
    - Discovers available models
    - Updates the local model registry
    - Preserves existing test results

### `list` - Model Registry

View and manage your model registry:

```bash
# List all models
lmstrix list

# Sort by different criteria
lmstrix list --sort size        # By model size
lmstrix list --sort ctx         # By tested context
lmstrix list --sort name        # By model name

# Different output formats
lmstrix list --show table       # Default table view
lmstrix list --show json        # JSON export
lmstrix list --show summary     # Brief summary
```

### `test` - Context Optimization

Find the optimal context window for your models:

```bash
# Test specific model
lmstrix test llama-3.2-3b-instruct

# Test with custom settings
lmstrix test llama-3.2-3b-instruct --threshold 65536

# Test all untested models
lmstrix test --all

# Reset and re-test a model
lmstrix test llama-3.2-3b-instruct --reset

# Test with custom prompt
lmstrix test llama-3.2-3b-instruct --prompt "Explain quantum computing"
```

!!! warning "Safety First"
    Always use the `--threshold` parameter to prevent system overload. Start with conservative values like 32768 tokens.

### `infer` - Text Generation

Generate text using your models:

```bash
# Basic inference
lmstrix infer "Your prompt here" -m model-name

# With verbose statistics
lmstrix infer "Explain AI" -m llama-3.2-3b-instruct --verbose

# Control output length (percentage of context)
lmstrix infer "Write a story" -m llama-3.2-3b-instruct --out_ctx "25%"

# Control output length (exact tokens)
lmstrix infer "Summarize this" -m llama-3.2-3b-instruct --out_ctx 100

# Adjust temperature for creativity
lmstrix infer "Be creative" -m llama-3.2-3b-instruct --temperature 0.9
```

## 📊 Understanding Verbose Output

When using `--verbose`, LMStrix provides detailed statistics:

```
════════════════════════════════════════════════════════════
🤖 MODEL: llama-3.2-3b-instruct
🔧 CONFIG: maxTokens=26214, temperature=0.7
📝 PROMPT (1 lines, 29 chars): What is the capital of Poland?
════════════════════════════════════════════════════════════
⠸ Running inference...
════════════════════════════════════════════════════════════
📊 INFERENCE STATS
⚡ Time to first token: 0.82s
⏱️  Total inference time: 11.66s
🔢 Predicted tokens: 338
📝 Prompt tokens: 5
🎯 Total tokens: 343
🚀 Tokens/second: 32.04
🛑 Stop reason: eosFound
════════════════════════════════════════════════════════════

The capital of Poland is Warsaw (Polish: Warszawa)...
```

### Key Metrics Explained

- **Time to first token**: Latency before generation starts
- **Total inference time**: Complete generation duration
- **Tokens/second**: Generation speed
- **Stop reason**: Why generation ended (`eosFound`, `lengthLimit`, etc.)

## 🎨 Advanced Quick Examples

### Template-Based Prompts

Create reusable prompt templates:

```bash
# Create a prompt file
cat > my_prompts.toml << 'EOF'
[summary]
prompt = "Create a comprehensive summary of the following text: {{text}}"

[explain]
prompt = "Explain {{concept}} in simple terms for a beginner"
EOF

# Use template with text input
lmstrix infer summary --file_prompt my_prompts.toml --text "Your text here"

# Use template with variable substitution
lmstrix infer explain --file_prompt my_prompts.toml --text "quantum computing"

# Use template with file input
lmstrix infer summary --file_prompt my_prompts.toml --text_file document.txt
```

### Batch Processing

Process multiple inputs efficiently:

```bash
# Test multiple models
for model in llama-3.2-3b-instruct mistral-7b-instruct; do
    lmstrix test "$model" --threshold 32768
done

# Generate responses for multiple prompts
echo "What is AI?" | lmstrix infer - -m llama-3.2-3b-instruct
echo "Explain Python" | lmstrix infer - -m llama-3.2-3b-instruct
```

### Performance Monitoring

Track model performance over time:

```bash
# Generate inference with timing
time lmstrix infer "Complex question" -m llama-3.2-3b-instruct --verbose

# Test context limits with different thresholds
lmstrix test llama-3.2-3b-instruct --ctx 16384
lmstrix test llama-3.2-3b-instruct --ctx 32768
lmstrix test llama-3.2-3b-instruct --ctx 65536
```

## 🚀 Your First Complete Workflow

Here's a complete workflow from setup to inference:

```bash
# 1. Discover available models
lmstrix scan --verbose

# 2. List models to see what's available
lmstrix list

# 3. Test context limit for your preferred model
lmstrix test llama-3.2-3b-instruct --threshold 32768

# 4. Run your first inference with stats
lmstrix infer "Explain machine learning in simple terms" \
    -m llama-3.2-3b-instruct \
    --verbose \
    --temperature 0.7 \
    --out_ctx "20%"

# 5. Check your updated model registry
lmstrix list --sort ctx
```

## 🛠️ Troubleshooting Quick Fixes

### Model Not Found
```bash
# Refresh model discovery
lmstrix scan --refresh

# Check LM Studio has the model loaded
lmstrix list --show json | grep "your-model-name"
```

### Context Test Fails
```bash
# Try lower threshold
lmstrix test model-name --threshold 16384

# Use custom prompt (simpler)
lmstrix test model-name --prompt "Hello world"

# Reset and retry
lmstrix test model-name --reset
```

### Inference Timeout
```bash
# Reduce output tokens
lmstrix infer "prompt" -m model --out_ctx 50

# Increase timeout (if needed)
export LMSTRIX_TIMEOUT=600
lmstrix infer "prompt" -m model
```

## 🎯 Next Steps

Now that you're familiar with the basics:

1. **[Configuration](configuration.md)** - Customize LMStrix settings
2. **[CLI Interface](cli-interface.md)** - Master all command options
3. **[Context Testing](context-testing.md)** - Deep dive into optimization
4. **[Python API](python-api.md)** - Programmatic usage
5. **[Prompt Templating](prompt-templating.md)** - Advanced prompt engineering

## 💡 Pro Tips

!!! tip "Efficiency Tips"
    - Use `--verbose` only when you need detailed stats
    - Set reasonable `--threshold` values to avoid system overload
    - Test models once, then rely on cached context limits
    - Use templates for repeated prompt patterns
    - Monitor token usage to optimize costs and performance

!!! example "Common Patterns"
    ```bash
    # Quick model comparison
    for model in model1 model2 model3; do
        echo "Testing $model:"
        lmstrix infer "Explain AI" -m "$model" --out_ctx 100
        echo "---"
    done
    
    # Find best performing model
    lmstrix list --sort ctx --show table
    ```

---

*Ready to dive deeper? Choose your next chapter based on your needs! 🚀*
</document_content>
</document>

<document index="56">
<source>src_docs/mkdocs.yml</source>
<document_content>
# this_file: src_docs/mkdocs.yml

site_name: LMStrix Documentation
site_description: Professional Python toolkit for LM Studio - Adaptive Context Optimization and Smart Model Management
site_author: LMStrix Team
site_url: https://your-organization.github.io/lmstrix

# Repository
repo_name: lmstrix
repo_url: https://github.com/your-organization/lmstrix
edit_uri: edit/main/src_docs/md/

# Copyright
copyright: Copyright &copy; 2024 LMStrix Team

# Configuration
theme:
  name: material
  language: en
  features:
    # Navigation
    - navigation.instant
    - navigation.instant.prefetch
    - navigation.tracking
    - navigation.tabs
    - navigation.tabs.sticky
    - navigation.sections
    - navigation.expand
    - navigation.path
    - navigation.prune
    - navigation.indexes
    - navigation.top
    # Search
    - search.highlight
    - search.share
    - search.suggest
    # Header
    - header.autohide
    # Content
    - content.action.edit
    - content.action.view
    - content.code.annotate
    - content.code.copy
    - content.code.select
    - content.footnote.tooltips
    - content.tabs.link
    - content.tooltips
    # Table of contents
    - toc.follow
    - toc.integrate
  palette:
    # Palette toggle for automatic mode
    - media: "(prefers-color-scheme)"
      toggle:
        icon: material/brightness-auto
        name: Switch to light mode
    # Palette toggle for light mode
    - media: "(prefers-color-scheme: light)"
      scheme: default
      primary: blue
      accent: blue
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode
    # Palette toggle for dark mode
    - media: "(prefers-color-scheme: dark)"
      scheme: slate
      primary: blue
      accent: blue
      toggle:
        icon: material/brightness-4
        name: Switch to system preference
  font:
    text: Roboto
    code: Roboto Mono
  favicon: assets/favicon.png
  logo: assets/logo.png
  icon:
    logo: material/lightning-bolt
    repo: fontawesome/brands/github

# Plugins
plugins:
  - search:
      separator: '[\s\u200b\-_,:!=\[\]()"`/]+|\.(?!\d)|&[lg]t;|(?!\b)(?=[A-Z][a-z])'
  - minify:
      minify_html: true
      minify_js: true
      htmlmin_opts:
        remove_comments: true
      cache_safe: true
  - git-revision-date-localized:
      enable_creation_date: true
      type: timeago

# Customization
extra:
  social:
    - icon: fontawesome/brands/github
      link: https://github.com/your-organization/lmstrix
    - icon: fontawesome/brands/python
      link: https://pypi.org/project/lmstrix/
  version:
    provider: mike

# Extensions
markdown_extensions:
  - abbr
  - admonition
  - attr_list
  - def_list
  - footnotes
  - md_in_html
  - toc:
      permalink: true
      title: On this page
  - pymdownx.arithmatex:
      generic: true
  - pymdownx.betterem:
      smart_enable: all
  - pymdownx.caret
  - pymdownx.details
  - pymdownx.emoji:
      emoji_generator: !!python/name:material.extensions.emoji.to_svg
      emoji_index: !!python/name:material.extensions.emoji.twemoji
  - pymdownx.highlight:
      anchor_linenums: true
      line_spans: __span
      pygments_lang_class: true
  - pymdownx.inlinehilite
  - pymdownx.keys
  - pymdownx.magiclink:
      normalize_issue_symbols: true
      repo_url_shorthand: true
      user: your-organization
      repo: lmstrix
  - pymdownx.mark
  - pymdownx.smartsymbols
  - pymdownx.snippets:
      auto_append:
        - includes/mkdocs.md
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid
          format: !!python/name:pymdownx.superfences.fence_code_format
  - pymdownx.tabbed:
      alternate_style: true
      combine_header_slug: true
      slugify: !!python/object/apply:pymdownx.slugs.slugify
        kwds:
          case: lower
  - pymdownx.tasklist:
      custom_checkbox: true
  - pymdownx.tilde

# Page tree
nav:
  - Home: index.md
  - Getting Started:
    - Installation: installation.md
    - Quick Start: quickstart.md
    - Configuration: configuration.md
  - User Guide:
    - CLI Interface: cli-interface.md
    - Python API: python-api.md
    - Context Testing: context-testing.md
  - Advanced Topics:
    - Model Management: model-management.md
    - Prompt Templating: prompt-templating.md
    - Performance & Optimization: performance.md

# Build directory
site_dir: ../docs

# Watch directories
watch:
  - md/
  - ../src/lmstrix/

# Extra CSS and JavaScript
extra_css:
  - stylesheets/extra.css

extra_javascript:
  - javascripts/mathjax.js
  - https://polyfill.io/v3/polyfill.min.js?features=es6
  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
</document_content>
</document>

<document index="57">
<source>src_docs/stylesheets/extra.css</source>
<document_content>
/* this_file: src_docs/stylesheets/extra.css */

/* Custom styles for LMStrix documentation */

/* Enhanced code blocks */
.highlight pre {
    border-radius: 8px;
    border: 1px solid var(--md-default-fg-color--lightest);
}

/* Custom admonition for LMStrix features */
.md-typeset .admonition.lmstrix {
    border-color: #2196f3;
}

.md-typeset .admonition.lmstrix > .admonition-title {
    background-color: #2196f31a;
    border-color: #2196f3;
}

.md-typeset .admonition.lmstrix > .admonition-title::before {
    background-color: #2196f3;
    -webkit-mask-image: var(--md-admonition-icon--abstract);
    mask-image: var(--md-admonition-icon--abstract);
}

/* Performance metric styling */
.performance-metric {
    display: inline-block;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 0.2em 0.5em;
    border-radius: 4px;
    font-family: var(--md-code-font);
    font-size: 0.85em;
    font-weight: 500;
}

/* Command line styling */
.cli-command {
    background: #1e1e1e;
    color: #d4d4d4;
    padding: 1em;
    border-radius: 6px;
    border-left: 4px solid #007acc;
    margin: 1em 0;
    font-family: var(--md-code-font);
}

.cli-command .prompt {
    color: #4ec9b0;
    user-select: none;
}

.cli-command .command {
    color: #dcdcaa;
}

.cli-command .output {
    color: #9cdcfe;
    margin-top: 0.5em;
}

/* API reference styling */
.api-signature {
    background: #f8f9fa;
    border: 1px solid #e9ecef;
    border-radius: 6px;
    padding: 1em;
    margin: 1em 0;
    font-family: var(--md-code-font);
    font-size: 0.9em;
}

[data-md-color-scheme="slate"] .api-signature {
    background: #2d2d2d;
    border-color: #404040;
}

.api-signature .method-name {
    color: #d73a49;
    font-weight: 600;
}

.api-signature .parameters {
    color: #005cc5;
}

.api-signature .return-type {
    color: #22863a;
}

/* Model comparison tables */
.model-comparison {
    width: 100%;
    border-collapse: collapse;
    margin: 1em 0;
}

.model-comparison th,
.model-comparison td {
    padding: 0.75em;
    text-align: left;
    border-bottom: 1px solid var(--md-default-fg-color--lightest);
}

.model-comparison th {
    background-color: var(--md-default-fg-color--lightest);
    font-weight: 600;
}

.model-comparison .metric-good {
    color: #28a745;
    font-weight: 600;
}

.model-comparison .metric-warning {
    color: #ffc107;
    font-weight: 600;
}

.model-comparison .metric-bad {
    color: #dc3545;
    font-weight: 600;
}

/* Feature cards */
.feature-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 1.5em;
    margin: 2em 0;
}

.feature-card {
    border: 1px solid var(--md-default-fg-color--lightest);
    border-radius: 8px;
    padding: 1.5em;
    background: var(--md-default-bg-color);
    transition: box-shadow 0.2s ease;
}

.feature-card:hover {
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
}

.feature-card h3 {
    margin-top: 0;
    color: var(--md-primary-fg-color);
}

.feature-card .icon {
    font-size: 2em;
    margin-bottom: 0.5em;
}

/* Status indicators */
.status-indicator {
    display: inline-flex;
    align-items: center;
    gap: 0.3em;
    padding: 0.2em 0.6em;
    border-radius: 12px;
    font-size: 0.85em;
    font-weight: 500;
}

.status-indicator.success {
    background: #d4edda;
    color: #155724;
    border: 1px solid #c3e6cb;
}

.status-indicator.warning {
    background: #fff3cd;
    color: #856404;
    border: 1px solid #ffeaa7;
}

.status-indicator.error {
    background: #f8d7da;
    color: #721c24;
    border: 1px solid #f5c6cb;
}

.status-indicator.info {
    background: #d1ecf1;
    color: #0c5460;
    border: 1px solid #bee5eb;
}

[data-md-color-scheme="slate"] .status-indicator.success {
    background: #1e4d2b;
    color: #9fdf9f;
    border-color: #2d5a3d;
}

[data-md-color-scheme="slate"] .status-indicator.warning {
    background: #4d3d0a;
    color: #ffc107;
    border-color: #5d4b0c;
}

[data-md-color-scheme="slate"] .status-indicator.error {
    background: #4d1e26;
    color: #f5b7b1;
    border-color: #5d2530;
}

[data-md-color-scheme="slate"] .status-indicator.info {
    background: #0a3441;
    color: #85d3e5;
    border-color: #0c4654;
}

/* Progress bars */
.progress-bar {
    background: #e9ecef;
    border-radius: 4px;
    height: 8px;
    overflow: hidden;
    margin: 0.5em 0;
}

.progress-bar .progress {
    background: linear-gradient(90deg, #28a745, #20c997);
    height: 100%;
    transition: width 0.3s ease;
}

/* Responsive improvements */
@media (max-width: 768px) {
    .feature-grid {
        grid-template-columns: 1fr;
    }
    
    .api-signature {
        font-size: 0.8em;
        overflow-x: auto;
    }
    
    .model-comparison {
        font-size: 0.85em;
    }
}

/* Print styles */
@media print {
    .feature-card {
        break-inside: avoid;
        border: 1px solid #000;
    }
    
    .cli-command {
        background: #f5f5f5 !important;
        color: #000 !important;
        border-left: 4px solid #000;
    }
}
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py
# Language: python

import sys
from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger
import traceback

def test_context_out_usage(()) -> None:
    """Test that model-specific context_out values are used in testing."""

def compare_inference_modes(()) -> None:
    """Compare inference with different out_ctx values."""


<document index="58">
<source>test_prompt_example.txt</source>
<document_content>
Write a haiku about artificial intelligence.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/test_streaming.py
# Language: python

import sys
from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ModelRegistry

def test_streaming(()) -> None:
    """Test streaming inference."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from collections.abc import Generator
from pathlib import Path
from typing import Any
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()) -> Mock:
    """Mock LMStudioClient for testing."""

def mock_llm(()) -> Mock:
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()) -> dict[str, Any]:
    """Sample model data for testing."""

def tmp_models_dir((tmp_path: Path)) -> Path:
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path: Path)) -> Path:
    """Create a temporary registry file path."""

def event_loop(()) -> Generator[asyncio.AbstractEventLoop, None, None]:
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()) -> dict[str, Any]:
    """Mock completion response from LM Studio."""

def mock_prompt_template(()) -> dict[str, Any]:
    """Sample prompt template for testing."""

def mock_context_data(()) -> dict[str, str]:
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys
from lmstrix.utils.logging import logger

def run_tests(()) -> int:
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self: "TestLMStudioClient")) -> None:
        """Test client initialization with different verbose settings."""
    def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test successful completion."""
    def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
        """Test completion failure."""
    def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test completion with default parameters."""

def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self: "TestLMStudioClient")) -> None:
    """Test client initialization with different verbose settings."""

def test_list_models_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful list_models call."""

def test_list_models_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test list_models with connection error."""

def test_load_model_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful model loading."""

def test_load_model_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test model loading failure."""

def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test successful completion."""

def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
    """Test completion failure."""

def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

from lmstrix.api.exceptions import (
    APIConnectionError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)) -> None:
        """Test base LMStrixError class."""
    def test_api_connection_error((self)) -> None:
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)) -> None:
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)) -> None:
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)) -> None:
        """Test that all exceptions inherit from lmstrixError."""

def test_api_error_base((self)) -> None:
    """Test base LMStrixError class."""

def test_api_connection_error((self)) -> None:
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)) -> None:
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)) -> None:
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)) -> None:
    """Test that all exceptions inherit from lmstrixError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

from datetime import datetime
from unittest.mock import Mock, patch
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.models import Model

class TestContextTestResult:
    """Test ContextTestResult data class."""
    def test_result_creation((self)) -> None:
        """Test creating a context test result."""
    def test_result_to_dict((self)) -> None:
        """Test converting result to dictionary."""

class TestContextTester:
    """Test ContextTester PUBLIC API."""
    def test_initialization((self)) -> None:
        """Test context tester initialization."""
    def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test initialization with custom client."""
    def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
        """Test testing a model that doesn't exist."""
    def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
        """Test batch testing of models."""

def test_result_creation((self)) -> None:
    """Test creating a context test result."""

def test_result_to_dict((self)) -> None:
    """Test converting result to dictionary."""

def test_initialization((self)) -> None:
    """Test context tester initialization."""

def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test initialization with custom client."""

def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
    """Test testing a model that doesn't exist."""

def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
    """Test batch testing of models."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model

class TestInferenceDict:
    """Test inference result dictionary structure."""
    def test_inference_dict_success((self)) -> None:
        """Test successful inference result dict."""
    def test_inference_dict_failure((self)) -> None:
        """Test failed inference result dict."""
    def test_inference_dict_empty_response((self)) -> None:
        """Test inference result dict with empty response."""

class TestInferenceManager:
    """Test InferenceManager PUBLIC API."""
    def test_manager_initialization((self)) -> None:
        """Test inference manager initialization."""
    def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test manager with custom client."""
    def test_infer_model_not_found((self)) -> None:
        """Test inference with non-existent model."""
    def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
        """Test basic successful inference."""

def test_inference_dict_success((self)) -> None:
    """Test successful inference result dict."""

def test_inference_dict_failure((self)) -> None:
    """Test failed inference result dict."""

def test_inference_dict_empty_response((self)) -> None:
    """Test inference result dict with empty response."""

def test_manager_initialization((self)) -> None:
    """Test inference manager initialization."""

def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test manager with custom client."""

def test_infer_model_not_found((self)) -> None:
    """Test inference with non-existent model."""

def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
    """Test basic successful inference."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from typing import Any
import pytest
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self: "TestContextTestStatus")) -> None:
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self: "TestModel")) -> None:
        """Test model creation using field aliases."""
    def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test model with context testing information."""
    def test_model_path_validation((self: "TestModel")) -> None:
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self: "TestModel")) -> None:
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self: "TestModel")) -> None:
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving and loading models."""
    def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test getting a model by ID."""
    def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test listing all models."""
    def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test removing a model."""
    def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving/loading models with context test information."""
    def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test that the saved JSON has the expected format."""

def test_enum_values((self: "TestContextTestStatus")) -> None:
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self: "TestModel")) -> None:
    """Test model creation using field aliases."""

def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test model with context testing information."""

def test_model_path_validation((self: "TestModel")) -> None:
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self: "TestModel")) -> None:
    """Test sanitized_id method."""

def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self: "TestModel")) -> None:
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving and loading models."""

def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test getting a model by ID."""

def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test listing all models."""

def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test removing a model."""

def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving/loading models with context test information."""

def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)) -> None:
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)) -> None:
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)) -> None:
        """Test resolver initialization."""
    def test_find_placeholders((self)) -> None:
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)) -> None:
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)) -> None:
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)) -> None:
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)) -> None:
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)) -> None:
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)) -> None:
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)) -> None:
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)) -> None:
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)) -> None:
        """Test resolution with empty string values."""
    def test_count_tokens((self)) -> None:
        """Test token counting."""
    def test_resolve_with_special_characters((self)) -> None:
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)) -> None:
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)) -> None:
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)) -> None:
    """Test resolver initialization."""

def test_find_placeholders((self)) -> None:
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)) -> None:
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)) -> None:
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)) -> None:
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)) -> None:
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)) -> None:
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)) -> None:
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)) -> None:
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)) -> None:
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)) -> None:
    """Test resolution with empty string values."""

def test_count_tokens((self)) -> None:
    """Test token counting."""

def test_resolve_with_special_characters((self)) -> None:
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that non-model files are skipped."""

def test_scanner_initialization((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanner initialization."""

def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a single model file."""

def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that non-model files are skipped."""

def test_scan_models((self: "TestModelScanner", mock_get_path: Mock, tmp_path: Path)) -> None:
    """Test scanning for models."""

def test_sync_with_registry((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.__main__ import LMStrixCLI

class TestCLIIntegration:
    """Test CLI integration - basic functionality only."""
    def test_cli_initialization((self)) -> None:
        """Test CLI can be initialized."""
    def test_infer_requires_parameters((self)) -> None:
        """Test infer command validates required parameters."""

def mock_registry((self, tmp_path: Path)) -> Path:
    """Create a mock model registry."""

def test_cli_initialization((self)) -> None:
    """Test CLI can be initialized."""

def test_list_command((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command shows models."""

def test_list_json_format((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command with JSON output."""

def test_infer_requires_parameters((self)) -> None:
    """Test infer command validates required parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import load_context

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path: Path)) -> None:
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path: Path)) -> None:
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path: Path)) -> None:
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path: Path)) -> None:
        """Test loading large context file."""

def test_load_context_simple((self, tmp_path: Path)) -> None:
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path: Path)) -> None:
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path: Path)) -> None:
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path: Path)) -> None:
    """Test loading large context file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry to custom path."""

def test_scan_and_update_models((
        self: "TestModelLoader",
        mock_save_registry: Mock,
        mock_load_registry: Mock,
        mock_client_class: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((
        self: "TestModelLoader",
        mock_load_registry: Mock,
        mock_client_class: Mock,
    )) -> None:
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from empty TOML file."""

def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from empty TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting default models file path."""
    def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context test log path."""
    def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting prompts directory."""
    def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test finding LM Studio in shared location."""

def exists_side_effect((self: Path)) -> bool:

def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting default models file path."""

def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context tests directory."""

def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context test log path."""

def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting prompts directory."""

def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test handling permission errors when creating directories."""


</documents>