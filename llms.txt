Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ _keep_this
â”‚   â””â”€â”€ ğŸ“ adam
â”œâ”€â”€ ğŸ“ docs
â”‚   â”œâ”€â”€ ğŸ“„ _config.yml
â”‚   â”œâ”€â”€ ğŸ“„ api.md
â”‚   â”œâ”€â”€ ğŸ“„ how-it-works.md
â”‚   â”œâ”€â”€ ğŸ“„ index.md
â”‚   â”œâ”€â”€ ğŸ“„ installation.md
â”‚   â””â”€â”€ ğŸ“„ usage.md
â”œâ”€â”€ ğŸ“ examples
â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ basic_workflow.sh
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚   â”‚   â”‚   context_control_examples.sh
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference_examples.sh
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_state_demo.sh
â”‚   â”‚   â””â”€â”€ ğŸ“„ model_testing.sh
â”‚   â”œâ”€â”€ ğŸ“ data
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sample_context.txt
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_questions.json
â”‚   â”œâ”€â”€ ğŸ“ prompts
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ analysis.toml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ coding.toml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ creative.toml
â”‚   â”‚   â””â”€â”€ ğŸ“„ qa.toml
â”‚   â”œâ”€â”€ ğŸ“ python
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ advanced_testing.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ basic_usage.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ batch_processing.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ custom_inference.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_templates_demo.py
â”‚   â”œâ”€â”€ ğŸ“ specialized
â”‚   â”‚   â””â”€â”€ ğŸ“„ elo_liczby.py
â”‚   â”œâ”€â”€ ğŸ“„ prompts.toml
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â””â”€â”€ ğŸ“„ run_all_examples.sh
â”œâ”€â”€ ğŸ“ issues
â”‚   â”œâ”€â”€ ğŸ“„ 301.txt
â”‚   â””â”€â”€ ğŸ“„ 302.txt
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ concrete_config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference_manager.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_parser.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ logging.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ state.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __main__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ py.typed
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“ test_api
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_client.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_exceptions.py
â”‚   â”œâ”€â”€ ğŸ“ test_core
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_tester.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_inference.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_models.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_prompts.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_scanner.py
â”‚   â”œâ”€â”€ ğŸ“ test_integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_cli_integration.py
â”‚   â”œâ”€â”€ ğŸ“ test_loaders
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_model_loader.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_prompt_loader.py
â”‚   â”œâ”€â”€ ğŸ“ test_utils
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_paths.py
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py
â”‚   â””â”€â”€ ğŸ“„ run_tests.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ AGENTS.md
â”œâ”€â”€ ğŸ“„ build.sh
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ CLAUDE.md
â”œâ”€â”€ ğŸ“„ cleanup.sh
â”œâ”€â”€ ğŸ“„ cleanup.txt
â”œâ”€â”€ ğŸ“„ GEMINI.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TESTING.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="2">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
llms.txt
CLEANUP.txt
src/lmstrix/_version.py

_keep_this/adam
</document_content>
</document>

<document index="3">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="4">
<source>AGENTS.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="5">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Major Improvements & Bug Fixes

#### Issues 201-204 (Completed)
- **Issue 201**: Enhanced model persistence - models now stay loaded between inference calls when no explicit context is specified
- **Issue 202**: Beautiful enhanced logging with emojis, model info, config details, and prompt statistics  
- **Issue 203**: Fixed model lookup to find by both path and ID without changing JSON structure
- **Issue 204**: Added comprehensive verbose stats logging including time to first token, predicted tokens, tokens/second, and total inference time

#### Model Registry Improvements
- Fixed smart model lookup that works with both model paths and IDs
- Preserved original JSON structure keyed by path
- No data duplication - registry size maintained
- Backward compatible with existing path-based lookups

#### Enhanced Logging & Statistics
- Beautiful formatted logging with visual separators and emojis
- Complete inference statistics display including:
  - âš¡ Time to first token
  - â±ï¸ Total inference time  
  - ğŸ”¢ Predicted tokens
  - ğŸ“ Prompt tokens
  - ğŸ¯ Total tokens
  - ğŸš€ Tokens/second
  - ğŸ›‘ Stop reason
- Eliminated duplicate stats display at end of output

### Added

- **Context Parameter Percentage Support**
  - `--out_ctx` parameter now supports percentage notation (e.g., "80%")
  - Created `utils/context_parser.py` for parsing context parameters
  - Percentage calculated from model's tested or declared maximum context

- **Improved CLI Output**
  - Non-verbose mode for `infer` command now shows only model response
  - Removed extra formatting and status information in quiet mode

- **Prompt File Support with TOML (Issue #104)**
  - Added `--file_prompt` parameter to load prompts from TOML files
  - Added `--dict` parameter for passing key=value pairs for placeholder resolution
  - When using `--file_prompt`, the prompt parameter refers to the prompt name in the TOML file
  - Supports nested placeholders and internal template references
  - Reports unresolved placeholders in verbose mode
  - Includes comprehensive example prompt file with various use cases

- **Enhanced Infer Context Control (Issue #103)**
  - Added `--in_ctx` parameter to control model loading context size
  - Added `--out_ctx` parameter to replace deprecated `--max_tokens`
  - Supports `--in_ctx 0` to load model without context specification
  - When `--in_ctx` is not specified, uses optimal context (tested or declared)
  - Explicit `--in_ctx` always unloads existing models and reloads with specified context
  - Smart unloading: only unloads models that were explicitly loaded with `--in_ctx`

- **Smart Model Loading**
  - Added model state detection to check if models are already loaded
  - Reuses existing loaded models when no explicit context specified
  - Added `--force-reload` flag to force model reload even if already loaded
  - Shows clear status messages about model reuse vs reload

- **Model State Persistence (Issue #201)**
  - Models now stay loaded between `infer` calls when `--in_ctx` is not specified
  - Added StateManager to track last-used model across sessions
  - Model ID parameter (`-m`) is now optional - uses last-used model when omitted
  - Significantly improves performance by avoiding repeated model loading/unloading
  - Created `examples/cli/model_state_demo.sh` demonstrating the feature

- **Context Validation**
  - Validates requested context against model's declared and tested limits
  - Warns when context exceeds safe limits
  - Suggests optimal context values

### Changed

- **Inference Command**
  - Deprecated `--max_tokens` in favor of `--out_ctx` (backward compatible with warnings)
  - Updated help text and documentation for new parameters
  - Improved model loading logic for better memory management
  - Enhanced status messages during inference operations

## [1.0.53] - 2025-07-29

### Fixed

- **Git Configuration**
  - Fixed git pull error by setting upstream branch tracking with `git branch --set-upstream-to=origin/main main`
  - Resolved "exit code(1)" error when running `git pull -v -- origin` without branch specification

### Changed

- **Version Maintenance**
  - Updated to version 1.0.53 with proper git configuration fixes

## [1.0.28 - 1.0.52] - 2025-07-25 to 2025-07-29

### Added

- **Enhanced CLI Features**
  - Added `--sort` option to `lmstrix test --all` command with same sort options as list (id, ctx, dtx, size, etc.)
  - Added `--ctx` option to `lmstrix test --all` for testing all untested models at a specific context size
  - Added `--show` option to `lmstrix list` with three output formats:
    - `id`: Plain newline-delimited list of model IDs
    - `path`: Newline-delimited list of relative paths (same as id currently)
    - `json`: Full JSON array from the registry
  - All `--show` formats respect the `--sort` option for flexible output

- **CLI Improvements**
  - Modified `--ctx` to work with `--all` flag for batch testing at specific context sizes
  - `test --all --ctx` filters models based on context limits and safety checks
  - Added proper status updates and persistence when using `--ctx` for single model tests
  - Fixed model field updates (tested_max_context, last_known_good_context) during --ctx testing

### Changed

- **Removed all asyncio dependencies (Issue #204)**
  - Converted entire codebase from async to synchronous
  - Now uses the native synchronous `lmstudio` package API directly
  - Simplified architecture by removing async/await complexity
  - Implemented signal-based timeout for Unix systems
  - All methods now return results directly without await

### Added

- **Context Size Safety Validation**
  - Added validation to prevent testing at or above `last_known_bad_context`
  - CLI `--ctx` parameter now checks against `last_known_bad_context` and limits to 90% of last bad
  - Automatic testing algorithms now respect `last_known_bad_context` during iterations
  - Added warning messages when context size approaches 80% or 90% of last known bad context
  - Prevents system crashes by avoiding previously failed context sizes

- **Enhanced Context Testing Strategy (Issue #201)**
  - Added `--threshold` parameter to test command (default: 102,400 tokens)
  - Prevents system crashes by limiting initial test size
  - New incremental testing algorithm: test at 1024, then threshold, then increment by 10,240
  - Optimized batch testing for `--all` flag with pass-based approach
  - Models sorted by declared context size to minimize loading/unloading
  - Rich table output showing test results with efficiency percentages

- **Smart Context Testing with Progress Saving**
  - Context tests now start with small context (32) to verify model loads
  - Added fields to track last known good/bad context sizes
  - Tests can resume from previous state if interrupted
  - Progress is saved to JSON after each test iteration
  - Changed test prompt from "2+2=" to "Say hello" for better reliability

### Fixed

- **Terminology Improvements**
  - Changed "Loaded X models" to "Read X models" to avoid confusion with LM Studio's model loading
  - Replaced generic "Check logs for details" with specific error messages

- **Context Testing Stability**
  - Added delays between model load/unload operations to prevent rapid cycling
  - Fixed connection reset issues caused by too-rapid operations
  - Enhanced binary search logging to show progress clearly

### Changed

- **Model Data Structure**
  - Added `last_known_good_context` field for resumable testing
  - Added `last_known_bad_context` field for resumable testing
  - Updated registry serialization to include new fields

## [1.0.28] - 2025-07-25

### Added

- **GitHub Pages Documentation Site**
  - Created comprehensive documentation site structure under `docs/`
  - Added Jekyll configuration with custom theme and navigation
  - Created documentation pages: installation, usage, API reference, how-it-works
  - Set up automatic changelog integration with documentation
  - Added responsive design and syntax highlighting

- **Example Improvements**
  - Added example output logging to `examples.log.txt` and `examples.err.txt`
  - Enhanced error handling in example scripts
  - Added more detailed comments in Python examples

### Fixed

- **Client Compatibility**
  - Fixed attribute access issues in `api/client.py` for embedding models
  - Added proper handling for different model types (LLMs vs Embeddings)
  - Improved error messages for unsupported model types

- **Context Testing Robustness**
  - Enhanced context size detection with better error handling
  - Improved inference validation logic
  - Added fallback mechanisms for edge cases

### Changed

- **CLI Enhancements**
  - Improved output formatting for model listings
  - Better progress indicators during testing
  - More informative error messages

- **Documentation Updates**
  - Updated README with clearer examples
  - Enhanced API documentation with more details
  - Added troubleshooting section

## [1.0.23] - 2025-07-25

### Added

- Implemented comprehensive `test_binary_search_logic` test function with edge cases:
  - Model that works at all sizes
  - Model that never works  
  - Model that loads but never passes inference
  - Model that works up to a specific context size
  - Model that never loads

### Fixed

- Fixed syntax errors in Python example files:
  - `examples/python/batch_processing.py` - Fixed unterminated string literals
  - `examples/python/custom_inference.py` - Fixed similar syntax issues
- Improved code quality by fixing linting issues:
  - Updated file operations to use `Path.open()` instead of `open()` (PTH123)
  - Added proper exception chaining with `from e` (B904)
  - Fixed shadowing of Python builtin `all` by renaming to `all_models` (A002)
- Updated `api/client.py` to handle different attribute names for LLMs vs Embeddings

### Changed

- All mypy type checking now passes without errors
- Package builds successfully as v1.0.23

## [1.0.21] - 2025-07-25

### Fixed

- Code formatting improvements in test files for better readability
- Updated `cleanup.sh` script
- Reduced size of `llms.txt` file significantly for better performance

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="6">
<source>CLAUDE.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1.  **API Layer** (`src/lmstrix/api/`)
    -   `client.py`: Async client for LM Studio server API with retry logic using `tenacity`.
    -   `exceptions.py`: Custom exception hierarchy for better error handling.

2.  **Core Engine** (`src/lmstrix/core/`)
    -   `context_tester.py`: Binary search algorithm to find optimal context window size, with `rich` progress bar integration.
    -   `inference.py`: Handles the inference process, including prompt building.
    -   `models.py`: Model registry with persistence for tracking tested context limits.
    -   `scanner.py`: Discovers and catalogs available LM Studio models.
    -   `prompts.py`: Prompt resolution and template management.
    -   `context.py`: Manages context, including prompt templates and token counting using `tiktoken`.

3.  **Loaders** (`src/lmstrix/loaders/`)
    -   `model_loader.py`: Manages model registry persistence (JSON).
    -   `prompt_loader.py`: Loads prompt templates from TOML files.
    -   `context_loader.py`: Loads context data from text files.

4.  **CLI** (`src/lmstrix/cli/`)
    -   `main.py`: `fire`-based CLI with commands: `scan`, `list`, `test`, `infer`.
    -   Uses `rich` for beautiful terminal output.

### 2.2. Critical Design Patterns

-   **Async-First**: All API operations use `async/await` for high performance.
-   **Retry Logic**: Uses `tenacity` for automatic retries with exponential backoff.
-   **Model Registry**: Persists discovered models and their tested limits to JSON.
-   **Two-Phase Prompts**: Separates prompt template structure from runtime context.
-   **Binary Search**: Efficiently finds maximum context window through targeted testing.

### 2.3. Dependencies

-   `lmstudio-python`: Official LM Studio Python SDK.
-   `httpx`: Async HTTP client.
-   `pydantic`: Data validation and models.
-   `fire`: CLI framework.
-   `rich`: Terminal formatting.
-   `tenacity`: Retry logic.
-   `tiktoken`: Token counting.
-   `loguru`: Logging.

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="7">
<source>GEMINI.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="8">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="9">
<source>PLAN.md</source>
<document_content>
# LMStrix Current Development Plan

## Current Status
Recent issues 201-204 have been completed successfully:
- âœ… Enhanced model persistence and state management
- âœ… Beautiful enhanced logging with comprehensive statistics  
- âœ… Fixed model lookup to work with both paths and IDs
- âœ… Integrated verbose stats display without duplication

## CRITICAL: Issue #302 - Inference Output Mismatch

### Problem Description
When running the same translation prompt through LM Studio GUI vs lmstrix CLI, we get drastically different results:
- **LM Studio GUI**: Produces proper Polish translation (639 tokens)
- **lmstrix CLI**: Only outputs `</translate>` (4 tokens)

### Root Cause Analysis

#### Configuration Differences Found
1. **Temperature**: GUI uses 0.8, CLI uses 0.7
2. **top_k**: GUI uses 20, CLI uses 40  
3. **Context Length**: GUI uses full 131072, CLI uses 65536 (reduced)
4. **max_predict**: GUI uses -1 (unlimited), CLI calculates 117964
5. **Sampling Parameters**: Multiple differences in repeat_penalty, min_p, etc.

#### Potential Issues
1. **Early Stop Token**: Model might be hitting a stop token immediately
2. **Prompt Format**: The prompt might be wrapped or modified differently
3. **Chat Template**: Possible incorrect chat template application
4. **Parameter Mismatch**: Inference parameters not matching LM Studio defaults

## CRITICAL: Test Suite Failures (cleanup.txt analysis)

### Priority 0: Critical Test Failures - MUST FIX IMMEDIATELY

#### 0.1 Missing Methods Causing AttributeErrors
**Severity: CRITICAL - Breaks core functionality**

1. **Model.validate_integrity() Missing**
   - Location: src/lmstrix/loaders/model_loader.py:181
   - Error: `AttributeError: 'Model' object has no attribute 'validate_integrity'`
   - Impact: Prevents model loading and registry updates
   - Fix: Add validate_integrity() method to Model class

2. **PromptResolver Methods Missing**
   - Errors: Multiple AttributeErrors for:
     - `_resolve_phase` (private method)
     - `resolve_template` (public method)
     - `_count_tokens` (private method)
   - Impact: Completely breaks prompt resolution
   - Fix: Implement all missing methods in PromptResolver

3. **ContextTester Methods Missing**
   - Missing: `_test_at_context`, `test_model`
   - Impact: Context testing non-functional
   - Fix: Add methods to ContextTester class

4. **ModelScanner.sync_with_registry() Missing**
   - Impact: Cannot sync discovered models with registry
   - Fix: Implement sync_with_registry method

#### 0.2 Type and Format Errors
**Severity: HIGH - Breaks tests and logging**

1. **Mock Format String Errors**
   - Location: src/lmstrix/api/client.py:221
   - Error: `TypeError: unsupported format string passed to Mock.__format__`
   - Cause: `logger.info(f"ğŸ“ CONTEXT: {llm.config.contextLength:,} tokens")`
   - Fix: Handle None/Mock values in format strings

2. **Invalid Format Specifier**
   - Location: src/lmstrix/core/prompts.py:133
   - Error: `ValueError: Invalid format specifier ' 'Important'' for object of type 'str'`
   - Fix: Properly escape format strings in templates

#### 0.3 Initialization and Path Issues
**Severity: MEDIUM - Test failures**

1. **Model Constructor Arguments**
   - Error: `TypeError: Model.__init__() missing 3 required positional arguments`
   - Fix: Update tests to provide required args: path, size_bytes, ctx_in

2. **Path Handling in Scanner**
   - Error: `ValueError: path is not in the subpath`
   - Fix: Handle paths outside expected directory

## Immediate Priorities

### 1. Fix Issue #302 - Inference Output Mismatch
**Priority: CRITICAL**
- This is blocking proper inference functionality
- Users cannot get correct model outputs
- Must be fixed before any other work

#### Implementation Steps:

##### Step 1: Add Diagnostic Logging
- Log the exact prompt being sent (with escape sequences visible)
- Log all inference parameters in detail
- Log stop tokens being used
- Add comparison with LM Studio expected values

##### Step 2: Parameter Alignment
- Change default temperature from 0.7 to 0.8
- Add CLI parameters for all inference settings:
  - `--top_k`, `--top_p`, `--min_p`
  - `--repeat_penalty`, `--repeat_last_n`
  - `--stop_tokens` (to override defaults)
- Remove or make optional the maxTokens calculation

##### Step 3: Context Length Fix
- Fix context reduction from 131072 to 65536
- Use full model context unless explicitly limited
- Add warning when context is reduced

##### Step 4: Stop Token Investigation  
- Check why model stops at `</translate>`
- Verify stop token configuration
- Test with stop tokens disabled

##### Step 5: Testing and Validation
- Compare output with LM Studio for identical prompts
- Ensure token counts match
- Verify quality of translation output

### 2. Fix All Critical Test Failures
**Priority: HIGH** (after Issue #302)
- See Priority 0 section below - required for stability
- Estimated time: 2-3 days
- Success metric: All tests passing

### 2. Issue #105 - Adam.toml Simplification
**Priority: High** (but AFTER test fixes)
- Simplify adam.toml structure to use flat format instead of nested groups
- Add --text and --text_file parameters to infer command for direct text input
- Update all prompt examples to use simplified approach
- Ensure backward compatibility with existing TOML files

### 3. Context Testing Streamlining  
**Priority: Medium**
- Simplify ContextTester class by merging methods into single test_context() function
- Remove complex state management and resumption logic
- Streamline binary search algorithm
- Consolidate test result logging

### 4. Model Loading Optimization
**Priority: Medium**  
- Improve model reuse detection to avoid unnecessary loading messages
- Add context length display in enhanced logging when available
- Optimize model loading workflow for better performance

## Implementation Order

### Phase 0: CRITICAL FIX (Immediate - 2-3 days)
1. **Day 1: Core Method Implementation**
   - Add Model.validate_integrity() 
   - Implement all PromptResolver missing methods
   - Add ContextTester missing methods
   - Add ModelScanner.sync_with_registry()

2. **Day 2: Type/Format Fixes**
   - Fix mock format string handling in client.py
   - Fix prompt format specifier issues
   - Update tests with proper model initialization

3. **Day 3: Testing & Validation**
   - Run full test suite
   - Fix any remaining failures
   - Ensure no regressions

### Phase 1: Resume Normal Development (After tests pass)
- Continue with priorities 2-4 above

## Future Development Phases

### Phase A: Core Simplification (2-3 weeks)
1. **Configuration Unification**
   - Create utils/config.py for centralized configuration handling
   - Consolidate path handling functions (get_lmstudio_path, etc.)
   - Remove redundant configuration code

2. **Error Handling Standardization**
   - Review and simplify custom exception hierarchy
   - Standardize error messages across codebase
   - Implement consistent logging patterns

3. **Code Quality Improvements**
   - Add comprehensive type hints to public APIs
   - Ensure all functions have proper docstrings
   - Remove deprecated TODO comments from code

### Phase B: CLI Enhancement (1-2 weeks)
1. **Command Improvements**
   - Enhance `scan` command with better progress reporting
   - Improve `list` command with filtering and sorting options
   - Add `reset` command for clearing model test data

2. **User Experience**
   - Better error messages with helpful suggestions
   - Improved help text and documentation
   - Enhanced progress indicators for long-running operations

### Phase C: Testing & Documentation (1 week)
1. **Test Suite Completion**
   - Ensure >90% test coverage maintained
   - Add integration tests for new features
   - Performance benchmarking of improvements

2. **Documentation Updates**
   - Update README.md with latest features
   - Create comprehensive CLI reference
   - Update examples to demonstrate new capabilities

## Technical Debt Reduction

### Code Architecture
- Review and simplify InferenceManager class structure
- Consolidate duplicate logic across modules
- Improve separation of concerns between CLI and core logic

### Performance Optimization
- Profile model loading and caching behavior
- Optimize JSON registry read/write operations
- Reduce memory usage in context testing

### Dependency Management
- Review and minimize external dependencies
- Ensure compatibility with latest Python versions
- Update build and packaging configuration

## Success Metrics

- **Functionality**: All existing CLI commands work without regression
- **Performance**: Model loading and inference speed improvements
- **Usability**: Cleaner, more informative user interface
- **Maintainability**: Reduced complexity, better code organization
- **Documentation**: Up-to-date and comprehensive user guides

## Long-term Vision

The goal is to make LMStrix the most user-friendly and efficient tool for managing and testing LM Studio models, with:
- Intuitive CLI interface with beautiful, informative output
- Smart model management with automatic optimization
- Comprehensive testing capabilities with clear results
- Excellent developer experience with clean, well-documented code
</document_content>
</document>

<document index="10">
<source>README.md</source>
<document_content>
# LMStrix

LMStrix is a professional Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and Python API for managing, testing, and running local language models, with a standout feature: **Adaptive Context Optimization**.

## Key Features

- **ğŸ” Automatic Context Discovery**: Binary search algorithm to find the true operational context limit of any model
- **ğŸ“Š Beautiful Verbose Logging**: Enhanced stats display with emojis showing inference metrics, timing, and token usage
- **ğŸš€ Smart Model Management**: Models persist between calls to reduce loading overhead
- **ğŸ¯ Flexible Inference Engine**: Run inference with powerful prompt templating and percentage-based output control
- **ğŸ“‹ Comprehensive Model Registry**: Track models, their context limits, and test results with JSON persistence
- **ğŸ›¡ï¸ Safety Controls**: Configurable thresholds and fail-safes to prevent system crashes
- **ğŸ’» Rich CLI Interface**: Beautiful terminal output with progress indicators and formatted tables

## Installation

```bash
# Using pip
pip install lmstrix

# Using uv (recommended)
uv pip install lmstrix
```

## Quick Start

### Command-Line Interface

```bash
# Scan for available models in LM Studio
lmstrix scan

# List all models with their context limits and test status
lmstrix list

# Test context limit for a specific model
lmstrix test llama-3.2-3b-instruct

# Test all untested models with safety threshold
lmstrix test --all --threshold 102400

# Run inference with enhanced verbose logging
lmstrix infer "What is the capital of Poland?" -m llama-3.2-3b-instruct --verbose

# Run inference with percentage-based output tokens
lmstrix infer "Explain quantum computing" -m llama-3.2-3b-instruct --out_ctx "25%"

# Use file-based prompts with templates
lmstrix infer summary -m llama-3.2-3b-instruct --file_prompt adam.toml --text_file document.txt

# Direct text input for prompts
lmstrix infer "Summarize: {{text}}" -m llama-3.2-3b-instruct --text "Your content here"
```

### Enhanced Verbose Output

When using `--verbose`, LMStrix provides comprehensive statistics:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¤– MODEL: llama-3.2-3b-instruct
ğŸ”§ CONFIG: maxTokens=26214, temperature=0.7
ğŸ“ PROMPT (1 lines, 18 chars): Capital of Poland?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â ¸ Running inference...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š INFERENCE STATS
âš¡ Time to first token: 0.82s
â±ï¸  Total inference time: 11.66s
ğŸ”¢ Predicted tokens: 338
ğŸ“ Prompt tokens: 5
ğŸ¯ Total tokens: 343
ğŸš€ Tokens/second: 32.04
ğŸ›‘ Stop reason: eosFound
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Python API

```python
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.core.inference_manager import InferenceManager

# Load model registry
registry = load_model_registry()

# List available models
models = registry.list_models()
print(f"Available models: {len(models)}")

# Run inference
manager = InferenceManager(verbose=True)
result = manager.infer(
    model_id="llama-3.2-3b-instruct",
    prompt="What is the meaning of life?",
    out_ctx=100,
    temperature=0.7
)

if result["succeeded"]:
    print(f"Response: {result['response']}")
    print(f"Tokens used: {result['tokens_used']}")
    print(f"Time: {result['inference_time']:.2f}s")
```

## Context Testing & Optimization

LMStrix uses a sophisticated binary search algorithm to discover true model context limits:

### Safety Features
- **Threshold Protection**: Configurable maximum context size to prevent system crashes
- **Progressive Testing**: Starts with small contexts and increases safely
- **Persistent Results**: Saves test results to avoid re-testing

### Testing Commands
```bash
# Test specific model
lmstrix test llama-3.2-3b-instruct

# Test all models with custom threshold
lmstrix test --all --threshold 65536

# Test at specific context size
lmstrix test --all --ctx 32768

# Reset and re-test a model
lmstrix test llama-3.2-3b-instruct --reset
```

## Model Management

### Registry Commands
```bash
# Scan for new models
lmstrix scan --verbose

# List models with different sorting
lmstrix list --sort size        # Sort by size
lmstrix list --sort ctx         # Sort by tested context
lmstrix list --show json        # Export as JSON

# Check system health
lmstrix health --verbose
```

### Model Persistence
Models stay loaded between inference calls for improved performance:
- When no explicit context is specified, models remain loaded
- Last-used model is remembered for subsequent calls
- Explicit context changes trigger model reloading

## Prompt Templating

LMStrix supports flexible prompt templating with TOML files:

```toml
# adam.toml
[aps]
prompt = """
You are an AI assistant skilled in Abstractive Proposition Segmentation.
Convert the following text: {{text}}
"""

[summary] 
prompt = "Create a comprehensive summary: {{text}}"
```

Use with CLI:
```bash
lmstrix infer aps --file_prompt adam.toml --text "Your text here"
lmstrix infer summary --file_prompt adam.toml --text_file document.txt
```

## Development

```bash
# Clone repository
git clone https://github.com/twardoch/lmstrix.git
cd lmstrix

# Install for development
pip install -e ".[dev]"

# Run tests
pytest

# Run linting
hatch run lint:all
```

## Project Structure

```
src/lmstrix/
â”œâ”€â”€ cli/main.py              # CLI interface
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ inference_manager.py # Unified inference engine
â”‚   â”œâ”€â”€ models.py            # Model registry
â”‚   â””â”€â”€ context_tester.py    # Context limit testing
â”œâ”€â”€ api/client.py            # LM Studio API client
â”œâ”€â”€ loaders/                 # Data loading utilities
â””â”€â”€ utils/                   # Helper utilities
```

## Features in Detail

### Adaptive Context Optimizer
- Binary search algorithm for efficient context limit discovery
- Safety thresholds to prevent system crashes
- Automatic persistence of test results
- Resume capability for interrupted tests

### Enhanced Logging
- Beautiful emoji-rich output in verbose mode
- Comprehensive inference statistics
- Progress indicators for long operations
- Clear error messages with context

### Smart Model Management
- Automatic model discovery from LM Studio
- Persistent registry with JSON storage
- Model state tracking (loaded/unloaded)
- Batch operations for multiple models

## Requirements

- Python 3.11+
- LM Studio installed and configured
- Models downloaded in LM Studio

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions welcome! Please read our contributing guidelines and submit pull requests for any improvements.
</document_content>
</document>

<document index="11">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures and configuration
â”œâ”€â”€ run_tests.py             # Simple test runner script
â”œâ”€â”€ test_api/                # API layer tests
â”‚   â”œâ”€â”€ test_client.py       # LMStudioClient tests
â”‚   â””â”€â”€ test_exceptions.py   # Custom exception tests
â”œâ”€â”€ test_core/               # Core module tests
â”‚   â”œâ”€â”€ test_context_tester.py  # Context optimization tests
â”‚   â”œâ”€â”€ test_inference.py    # Inference engine tests
â”‚   â”œâ”€â”€ test_models.py       # Model and registry tests
â”‚   â”œâ”€â”€ test_prompts.py      # Prompt resolution tests
â”‚   â””â”€â”€ test_scanner.py      # Model scanner tests
â”œâ”€â”€ test_loaders/            # Loader tests
â”‚   â”œâ”€â”€ test_context_loader.py   # Context file loading tests
â”‚   â”œâ”€â”€ test_model_loader.py     # Model loader tests
â”‚   â””â”€â”€ test_prompt_loader.py    # Prompt loader tests
â”œâ”€â”€ test_utils/              # Utility tests
â”‚   â””â”€â”€ test_paths.py        # Path utility tests
â”œâ”€â”€ test_integration/        # Integration tests
â”‚   â””â”€â”€ test_cli_integration.py  # CLI integration tests
â””â”€â”€ test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="12">
<source>TODO.md</source>
<document_content>
# LMStrix TODO List

## CRITICAL: Issue #302 - Fix Inference Output Mismatch (Priority 0)

### Diagnostic Investigation
- [ ] Add detailed logging of exact prompt being sent to model
- [ ] Log all inference parameters before sending request
- [ ] Log stop tokens configuration
- [ ] Add comparison mode to show differences from LM Studio defaults
- [ ] Check if chat template is being applied incorrectly

### Parameter Alignment  
- [ ] Change default temperature from 0.7 to 0.8
- [ ] Add CLI flags for all inference parameters (--top_k, --top_p, --min_p)
- [ ] Add --repeat_penalty and --repeat_last_n flags
- [ ] Add --stop_tokens flag to override defaults
- [ ] Remove or make optional the maxTokens calculation (90% of context)

### Context Length Fix
- [ ] Fix context reduction from 131072 to 65536 issue
- [ ] Use full model context unless explicitly limited by user
- [ ] Add warning message when context is reduced
- [ ] Ensure --in_ctx parameter works correctly

### Stop Token Investigation
- [ ] Investigate why model stops at `</translate>` 
- [ ] Test with stop tokens disabled
- [ ] Compare stop token handling with LM Studio
- [ ] Add --no-stop-tokens flag for testing

### Testing and Validation
- [ ] Compare output with LM Studio for identical prompts
- [ ] Ensure token counts match between lmstrix and LM Studio
- [ ] Verify translation quality matches expected output
- [ ] Create regression test for this issue

## HIGH PRIORITY: Fix Test Suite Failures (After Issue #302)

### Missing Methods - CRITICAL (COMPLETED)
- [x] Add Model.validate_integrity() method to src/lmstrix/core/models.py
- [x] Implement PromptResolver._resolve_phase() in src/lmstrix/core/prompts.py
- [x] Implement PromptResolver.resolve_template() in src/lmstrix/core/prompts.py
- [x] Implement PromptResolver._count_tokens() in src/lmstrix/core/prompts.py
- [x] Add ContextTester._test_at_context() to src/lmstrix/core/context_tester.py
- [x] Add ContextTester.test_model() to src/lmstrix/core/context_tester.py
- [x] Implement ModelScanner.sync_with_registry() in src/lmstrix/core/scanner.py

### Type/Format Errors - HIGH (COMPLETED)
- [x] Fix Mock format string error in src/lmstrix/api/client.py:221
- [x] Fix invalid format specifier in src/lmstrix/core/prompts.py:133
- [x] Handle None/Mock values in logging format strings

### Test Fixes - MEDIUM
- [ ] Update Model initialization tests to provide required arguments
- [ ] Fix path handling in ModelScanner for paths outside models directory
- [ ] Fix JSON output in CLI list command
- [ ] Update test expectations to match current implementation

### Infrastructure - LOW
- [ ] Fix loguru I/O closed file errors in tests
- [ ] Address pkg_resources deprecation warnings
- [ ] Configure pytest-golden in pytest.ini

## Immediate Priorities (AFTER Issue #302 and test fixes)

### Issue #105 - Adam.toml Simplification (High Priority)
- [ ] Simplify adam.toml structure to use flat format instead of nested groups
- [ ] Add --text and --text_file parameters to infer command for direct text input
- [ ] Update all prompt examples to use simplified approach
- [ ] Ensure backward compatibility with existing TOML files
- [ ] Test all prompt examples with new structure
- [ ] Update documentation for new TOML format

### Context Testing Streamlining (Medium Priority)
- [ ] Simplify ContextTester class by merging methods into single test_context() function
- [ ] Remove complex state management and resumption logic
- [ ] Streamline binary search algorithm
- [ ] Consolidate test result logging
- [ ] Update context testing tests to match new structure

### Model Loading Optimization (Medium Priority)
- [ ] Improve model reuse detection to avoid unnecessary loading messages
- [ ] Add context length display in enhanced logging when available
- [ ] Optimize model loading workflow for better performance
- [ ] Add better feedback when models are being reused vs loaded fresh

## Phase A: Core Simplification

### Configuration Unification
- [ ] Create utils/config.py for centralized configuration handling
- [ ] Consolidate path handling functions (get_lmstudio_path, etc.)
- [ ] Remove redundant configuration code
- [ ] Update all imports to use centralized config

### Error Handling Standardization
- [ ] Review and simplify custom exception hierarchy
- [ ] Standardize error messages across codebase
- [ ] Implement consistent logging patterns
- [ ] Update error handling to use standard exceptions where appropriate

### Code Quality Improvements
- [ ] Add comprehensive type hints to public APIs
- [ ] Ensure all functions have proper docstrings
- [ ] Remove deprecated TODO comments from code
- [ ] Run code quality checks and fix issues

## Phase B: CLI Enhancement

### Command Improvements
- [ ] Enhance `scan` command with better progress reporting
- [ ] Improve `list` command with filtering and sorting options
- [ ] Add `reset` command for clearing model test data
- [ ] Add `health` command for system diagnostics

### User Experience
- [ ] Better error messages with helpful suggestions
- [ ] Improved help text and documentation
- [ ] Enhanced progress indicators for long-running operations
- [ ] Add command aliases for common operations

## Phase C: Testing & Documentation

### Test Suite Completion
- [ ] Ensure >90% test coverage maintained
- [ ] Add integration tests for new features
- [ ] Performance benchmarking of improvements
- [ ] Add regression tests for fixed issues

### Documentation Updates
- [ ] Update README.md with latest features
- [ ] Create comprehensive CLI reference
- [ ] Update examples to demonstrate new capabilities
- [ ] Create migration guide for any breaking changes

## Technical Debt Reduction

### Code Architecture
- [ ] Review and simplify InferenceManager class structure
- [ ] Consolidate duplicate logic across modules
- [ ] Improve separation of concerns between CLI and core logic
- [ ] Refactor overly complex functions

### Performance Optimization
- [ ] Profile model loading and caching behavior
- [ ] Optimize JSON registry read/write operations
- [ ] Reduce memory usage in context testing
- [ ] Benchmark before/after performance improvements

### Dependency Management
- [ ] Review and minimize external dependencies
- [ ] Ensure compatibility with latest Python versions
- [ ] Update build and packaging configuration
- [ ] Test installation on clean environments

## Quality Assurance

### Testing
- [ ] Run full test suite on all changes
- [ ] Test CLI commands with various model types
- [ ] Verify backward compatibility
- [ ] Performance regression testing

### Documentation
- [ ] Update CHANGELOG.md with all changes
- [ ] Review and update docstrings
- [ ] Ensure examples work correctly
- [ ] Update any configuration documentation

### Release Preparation  
- [ ] Version bump and release notes
- [ ] Tag release in git
- [ ] Test PyPI package build
- [ ] Verify clean installation works
</document_content>
</document>

<document index="13">
<source>WORK.md</source>
<document_content>
# Current Work Progress

## ACTIVE: Issue #302 - Fix Inference Output Mismatch

### Problem Summary
When running the same translation prompt:
- **LM Studio GUI**: Produces proper Polish translation (639 tokens)
- **lmstrix CLI**: Only outputs `</translate>` (4 tokens)

### Root Cause Analysis
Found several configuration differences:
1. Temperature: GUI=0.8, CLI=0.7
2. top_k: GUI=20, CLI=40
3. Context: GUI=131072, CLI=65536 (reduced)
4. max_predict: GUI=-1, CLI=117964
5. Stop tokens configuration may differ

### Current Work Items

#### 1. Add Diagnostic Logging (IN PROGRESS)
- [ ] Log exact prompt with escape sequences visible
- [ ] Log all inference parameters in detail
- [ ] Add comparison with LM Studio defaults
- [ ] Log stop token configuration

#### 2. Parameter Alignment
- [ ] Change default temperature to 0.8
- [ ] Add CLI flags for inference parameters
- [ ] Fix maxTokens calculation
- [ ] Add stop token configuration

#### 3. Context Length Fix
- [ ] Fix context reduction issue
- [ ] Use full model context by default
- [ ] Add warning for context reduction

#### 4. Testing
- [ ] Compare with LM Studio output
- [ ] Verify token counts match
- [ ] Test translation quality

### Implementation Progress
Starting with diagnostic logging to understand the exact differences...

## Recently Completed Work

### Test Suite Fixes (Priority 0) âœ…
All critical AttributeError issues have been resolved:
1. **Model.validate_integrity()** âœ…
2. **PromptResolver methods** âœ…  
3. **ContextTester methods** âœ…
4. **ModelScanner.sync_with_registry()** âœ…

### Issues 201-204 âœ…
- Model persistence between calls
- Beautiful enhanced logging
- Fixed model lookup for paths/IDs
- Comprehensive inference statistics

## Next Steps After Issue #302
1. Complete remaining test fixes
2. Issue #105 - Adam.toml simplification
3. Context testing streamlining
4. Model loading optimization
</document_content>
</document>

<document index="14">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="15">
<source>cleanup.sh</source>
<document_content>
#!/usr/bin/env bash
cd "$(dirname "$0")" || exit

fd -e py -x uvx autoflake -i {}
fd -e py -x uvx pyupgrade --py312-plus {}
fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}
fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}
llms . "llms.txt"
uvx hatch clean
gitnextvers .
uvx hatch build
uvx hatch publish

</document_content>
</document>

<document index="16">
<source>cleanup.txt</source>
<document_content>
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)  # type: ignore
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_golden/plugin.py:53: GoldenTestUsageWarning: Add 'enable_assertion_pass_hook=true' to pytest.ini for safer usage of pytest-golden.
  warnings.warn(
============================= test session starts ==============================
platform darwin -- Python 3.12.9, pytest-8.4.1, pluggy-1.6.0 -- /usr/local/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/Users/Shared/lmstudio/lmstrix/.hypothesis/examples'))
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
Fugue tests will be initialized with options:
rootdir: /Users/Shared/lmstudio/lmstrix
configfile: pyproject.toml
testpaths: tests
plugins: recording-0.13.2, cov-6.0.0, subtests-0.14.1, darkgraylib-1.0.0, instafail-0.5.0, anyio-4.9.0, enabler-3.3.0, ignore-flaky-2.2.1, integration-0.2.3, sugar-1.0.0, langchain-0.1.0, env-1.1.5, flaky-3.8.1, time-machine-2.16.0, memray-1.7.0, shutil-1.8.1, checkdocs-2.13.0, hypothesis-6.124.7, black-0.6.0, golden-0.2.2, ruff-0.4.1, benchmark-5.1.0, pmxbot-1122.17.0, virtualenv-1.8.1, jaxtyping-0.0.1, perf-0.15.0, rerunfailures-15.0, typeguard-4.4.4, fugue-0.9.2.dev1, timeout-2.3.1, mock-3.14.0, hydra-core-1.3.2, asyncio-1.0.0, xdist-3.6.1, depends-1.0.1, jaraco.mongodb-0.0.0, requests-mock-1.12.1
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
created: 8/8 workers
8 workers [164 items]

scheduling tests via LoadScheduling

tests/__init__.py::ruff 
tests/test_core/__init__.py::black 
tests/test_api/__init__.py::ruff::format 
tests/test_api/test_client.py::TestLMStudioClient::test_load_model_success 
[gw4] [  0%] PASSED tests/test_api/test_client.py::TestLMStudioClient::test_load_model_success 
tests/test_api/test_client.py::TestLMStudioClient::test_load_model_failure 
[gw4] [  1%] FAILED tests/test_api/test_client.py::TestLMStudioClient::test_load_model_failure 
tests/test_api/test_client.py::TestLMStudioClient::test_completion_success 
[gw4] [  1%] FAILED tests/test_api/test_client.py::TestLMStudioClient::test_completion_success 
tests/test_api/test_client.py::TestLMStudioClient::test_completion_failure 
[gw4] [  2%] FAILED tests/test_api/test_client.py::TestLMStudioClient::test_completion_failure 
tests/test_api/test_client.py::TestCompletionResponse::test_completion_response_creation 
[gw3] [  3%] PASSED tests/test_api/test_client.py::TestCompletionResponse::test_completion_response_creation 
tests/test_api/test_client.py::TestCompletionResponse::test_completion_response_minimal 
[gw3] [  3%] PASSED tests/test_api/test_client.py::TestCompletionResponse::test_completion_response_minimal 
tests/test_api/test_client.py::TestLMStudioClient::test_client_initialization 
[gw3] [  4%] PASSED tests/test_api/test_client.py::TestLMStudioClient::test_client_initialization 
tests/test_api/test_client.py::TestLMStudioClient::test_list_models_success 
[gw3] [  4%] FAILED tests/test_api/test_client.py::TestLMStudioClient::test_list_models_success 
tests/conftest.py::black 
tests/test_api/test_exceptions.py::ruff 
tests/test_api/test_client.py::TestLMStudioClient::test_list_models_failure 
[gw2] [  5%] PASSED tests/test_api/__init__.py::ruff::format 
[gw5] [  6%] PASSED tests/test_api/test_exceptions.py::ruff 
tests/test_api/test_client.py::TestLMStudioClient::test_completion_with_defaults 
tests/test_api/test_exceptions.py::TestAPIExceptions::test_model_load_error 
[gw4] [  6%] FAILED tests/test_api/test_client.py::TestLMStudioClient::test_completion_with_defaults 
tests/test_core/test_context_tester.py::TestContextTestResult::test_result_to_dict 
[gw4] [  7%] PASSED tests/test_core/test_context_tester.py::TestContextTestResult::test_result_to_dict 
tests/test_core/test_context_tester.py::TestContextTester::test_initialization 
[gw4] [  7%] PASSED tests/test_core/test_context_tester.py::TestContextTester::test_initialization 
tests/test_core/test_context_tester.py::TestContextTester::test_initialization_with_client 
[gw4] [  8%] PASSED tests/test_core/test_context_tester.py::TestContextTester::test_initialization_with_client 
tests/test_core/test_context_tester.py::TestContextTester::test_test_model_not_found 
[gw4] [  9%] FAILED tests/test_core/test_context_tester.py::TestContextTester::test_test_model_not_found 
tests/test_api/__init__.py::black 
[gw6] [  9%] PASSED tests/test_api/test_exceptions.py::TestAPIExceptions::test_model_load_error 
[gw3] [ 10%] PASSED tests/test_api/test_client.py::TestLMStudioClient::test_list_models_failure 
[gw0] [ 10%] PASSED tests/__init__.py::ruff 
tests/test_api/test_exceptions.py::TestAPIExceptions::test_inference_error 
tests/test_api/test_exceptions.py::ruff::format 
tests/test_core/test_inference.py::ruff 
tests/__init__.py::ruff::format 
tests/test_core/test_context_tester.py::TestContextTester::test_test_all_models 
[gw5] [ 11%] PASSED tests/test_api/test_exceptions.py::ruff::format 
[gw6] [ 12%] PASSED tests/test_api/test_exceptions.py::TestAPIExceptions::test_inference_error 
tests/test_api/test_exceptions.py::TestAPIExceptions::test_exception_inheritance 
[gw6] [ 12%] PASSED tests/test_api/test_exceptions.py::TestAPIExceptions::test_exception_inheritance 
tests/test_core/__init__.py::ruff 
[gw6] [ 13%] PASSED tests/test_core/__init__.py::ruff 
[gw3] [ 14%] PASSED tests/test_core/test_inference.py::ruff 
tests/test_core/test_inference.py::ruff::format 
[gw0] [ 14%] PASSED tests/__init__.py::ruff::format 
tests/test_api/test_exceptions.py::black 
tests/__init__.py::black 
[gw4] [ 15%] FAILED tests/test_core/test_context_tester.py::TestContextTester::test_test_all_models 
tests/test_core/test_inference.py::TestInferenceDict::test_inference_dict_empty_response 
[gw4] [ 15%] PASSED tests/test_core/test_inference.py::TestInferenceDict::test_inference_dict_empty_response 
tests/test_core/test_inference.py::TestInferenceManager::test_manager_initialization 
[gw4] [ 16%] PASSED tests/test_core/test_inference.py::TestInferenceManager::test_manager_initialization 
tests/test_core/test_inference.py::TestInferenceManager::test_manager_with_custom_client 
[gw4] [ 17%] PASSED tests/test_core/test_inference.py::TestInferenceManager::test_manager_with_custom_client 
tests/test_core/test_inference.py::TestInferenceManager::test_infer_model_not_found 
[gw4] [ 17%] PASSED tests/test_core/test_inference.py::TestInferenceManager::test_infer_model_not_found 
tests/test_core/__init__.py::ruff::format 
[gw3] [ 18%] PASSED tests/test_core/test_inference.py::ruff::format 
tests/test_core/test_inference.py::black 
tests/test_core/test_inference.py::TestInferenceManager::test_infer_basic_success 
[gw4] [ 18%] PASSED tests/test_core/test_inference.py::TestInferenceManager::test_infer_basic_success 
tests/test_core/test_models.py::TestModel::test_model_creation_minimal 
[gw4] [ 19%] PASSED tests/test_core/test_models.py::TestModel::test_model_creation_minimal 
tests/test_core/test_models.py::TestModel::test_model_creation_with_aliases 
[gw4] [ 20%] PASSED tests/test_core/test_models.py::TestModel::test_model_creation_with_aliases 
tests/test_core/test_models.py::TestModel::test_model_with_context_testing 
[gw4] [ 20%] PASSED tests/test_core/test_models.py::TestModel::test_model_with_context_testing 
[gw6] [ 21%] PASSED tests/test_core/__init__.py::ruff::format 
tests/test_core/test_models.py::ruff 
tests/test_core/test_models.py::TestModel::test_model_path_validation 
[gw4] [ 21%] FAILED tests/test_core/test_models.py::TestModel::test_model_path_validation 
tests/test_core/test_models.py::TestModel::test_model_sanitized_id 
[gw4] [ 22%] PASSED tests/test_core/test_models.py::TestModel::test_model_sanitized_id 
tests/test_core/test_models.py::TestModel::test_model_to_registry_dict 
[gw4] [ 23%] PASSED tests/test_core/test_models.py::TestModel::test_model_to_registry_dict 
tests/test_core/test_models.py::TestModel::test_model_validation_error 
[gw4] [ 23%] FAILED tests/test_core/test_models.py::TestModel::test_model_validation_error 
tests/test_core/test_models.py::TestModelRegistry::test_registry_initialization 
[gw4] [ 24%] PASSED tests/test_core/test_models.py::TestModelRegistry::test_registry_initialization 
tests/test_core/test_models.py::TestModelRegistry::test_registry_save_and_load 
[gw4] [ 25%] PASSED tests/test_core/test_models.py::TestModelRegistry::test_registry_save_and_load 
tests/test_core/test_models.py::TestModelRegistry::test_registry_get_model 
[gw4] [ 25%] PASSED tests/test_core/test_models.py::TestModelRegistry::test_registry_get_model 
tests/test_core/test_models.py::TestModelRegistry::test_registry_list_models 
[gw4] [ 26%] PASSED tests/test_core/test_models.py::TestModelRegistry::test_registry_list_models 
tests/test_core/test_models.py::TestModelRegistry::test_registry_remove_model 
[gw4] [ 26%] PASSED tests/test_core/test_models.py::TestModelRegistry::test_registry_remove_model 
tests/test_core/test_models.py::TestModelRegistry::test_registry_with_context_test_data 
[gw4] [ 27%] PASSED tests/test_core/test_models.py::TestModelRegistry::test_registry_with_context_test_data 
tests/test_core/test_models.py::TestModelRegistry::test_registry_json_format 
[gw4] [ 28%] PASSED tests/test_core/test_models.py::TestModelRegistry::test_registry_json_format 
tests/test_core/test_prompts.py::ruff 
[gw6] [ 28%] PASSED tests/test_core/test_models.py::ruff 
tests/test_core/test_models.py::ruff::format 
[gw4] [ 29%] PASSED tests/test_core/test_prompts.py::ruff 
[gw6] [ 29%] PASSED tests/test_core/test_models.py::ruff::format 
tests/test_core/test_models.py::black 
tests/test_core/test_prompts.py::ruff::format 
[gw4] [ 30%] PASSED tests/test_core/test_prompts.py::ruff::format 
tests/test_core/test_prompts.py::black 
[gw7] [ 31%] PASSED tests/test_core/__init__.py::black 
tests/test_core/test_context_tester.py::ruff 
[gw1] [ 31%] PASSED tests/conftest.py::black 
tests/run_tests.py::ruff 
[gw5] [ 32%] PASSED tests/test_api/test_exceptions.py::black 
tests/test_api/test_exceptions.py::TestAPIExceptions::test_api_error_base 
[gw5] [ 32%] PASSED tests/test_api/test_exceptions.py::TestAPIExceptions::test_api_error_base 
[gw0] [ 33%] PASSED tests/__init__.py::black 
tests/test_api/test_exceptions.py::TestAPIExceptions::test_api_connection_error 
[gw7] [ 34%] PASSED tests/test_core/test_context_tester.py::ruff 
[gw2] [ 34%] PASSED tests/test_api/__init__.py::black 
tests/conftest.py::ruff 
tests/test_api/test_client.py::ruff 
[gw5] [ 35%] PASSED tests/test_api/test_exceptions.py::TestAPIExceptions::test_api_connection_error 
tests/test_core/test_prompts.py::TestPromptResolver::test_find_placeholders 
tests/test_core/test_context_tester.py::ruff::format 
[gw1] [ 35%] PASSED tests/run_tests.py::ruff 
[gw3] [ 36%] PASSED tests/test_core/test_inference.py::black 
[gw0] [ 37%] PASSED tests/conftest.py::ruff 
tests/run_tests.py::ruff::format 
[gw2] [ 37%] PASSED tests/test_api/test_client.py::ruff 
[gw5] [ 38%] PASSED tests/test_core/test_prompts.py::TestPromptResolver::test_find_placeholders 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_simple 
tests/test_core/test_inference.py::TestInferenceDict::test_inference_dict_success 
[gw3] [ 39%] PASSED tests/test_core/test_inference.py::TestInferenceDict::test_inference_dict_success 
tests/test_api/test_client.py::ruff::format 
[gw7] [ 39%] PASSED tests/test_core/test_context_tester.py::ruff::format 
tests/test_core/test_context_tester.py::black 
tests/test_core/test_inference.py::TestInferenceDict::test_inference_dict_failure 
[gw3] [ 40%] PASSED tests/test_core/test_inference.py::TestInferenceDict::test_inference_dict_failure 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_two_phase 
[gw5] [ 40%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_simple 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_missing_placeholder 
[gw5] [ 41%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_missing_placeholder 
tests/conftest.py::ruff::format 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_extra_context 
[gw5] [ 42%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_extra_context 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_with_special_characters 
[gw5] [ 42%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_with_special_characters 
tests/test_core/test_scanner.py::ruff 
[gw2] [ 43%] PASSED tests/test_api/test_client.py::ruff::format 
[gw1] [ 43%] PASSED tests/run_tests.py::ruff::format 
tests/run_tests.py::black 
tests/test_api/test_client.py::black 
[gw0] [ 44%] PASSED tests/conftest.py::ruff::format 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_numeric_values 
[gw3] [ 45%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_two_phase 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_recursive 
[gw5] [ 45%] PASSED tests/test_core/test_scanner.py::ruff 
[gw3] [ 46%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_recursive 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_circular_reference 
[gw3] [ 46%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_circular_reference 
tests/test_core/test_scanner.py::ruff::format 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_no_placeholders 
[gw3] [ 47%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_no_placeholders 
tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_mlx_directory 
[gw5] [ 48%] PASSED tests/test_core/test_scanner.py::ruff::format 
tests/test_core/test_scanner.py::TestModelScanner::test_get_model_size_directory 
[gw5] [ 48%] PASSED tests/test_core/test_scanner.py::TestModelScanner::test_get_model_size_directory 
tests/test_core/test_scanner.py::TestModelScanner::test_get_model_size_nonexistent 
[gw3] [ 49%] FAILED tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_mlx_directory 
tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_hidden_file 
[gw3] [ 50%] PASSED tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_hidden_file 
[gw5] [ 50%] PASSED tests/test_core/test_scanner.py::TestModelScanner::test_get_model_size_nonexistent 
tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_gguf_file 
[gw0] [ 51%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_numeric_values 
[gw5] [ 51%] FAILED tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_gguf_file 
tests/test_integration/__init__.py::ruff 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_empty_value 
[gw0] [ 52%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_empty_value 
tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_non_model_file 
[gw3] [ 53%] PASSED tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_non_model_file 
tests/test_core/test_scanner.py::TestModelScanner::test_scan_models 
[gw3] [ 53%] FAILED tests/test_core/test_scanner.py::TestModelScanner::test_scan_models 
tests/test_core/test_prompts.py::TestPromptResolver::test_count_tokens 
[gw0] [ 54%] FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_count_tokens 
tests/test_integration/__init__.py::black 
tests/test_core/test_scanner.py::TestModelScanner::test_sync_with_registry 
[gw3] [ 54%] FAILED tests/test_core/test_scanner.py::TestModelScanner::test_sync_with_registry 
tests/test_integration/test_cli_integration.py::ruff::format 
[gw5] [ 55%] PASSED tests/test_integration/__init__.py::ruff 
[gw3] [ 56%] PASSED tests/test_integration/test_cli_integration.py::ruff::format 
tests/test_integration/__init__.py::ruff::format 
tests/test_integration/test_cli_integration.py::black 
[gw5] [ 56%] PASSED tests/test_integration/__init__.py::ruff::format 
tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_cli_initialization 
[gw5] [ 57%] PASSED tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_cli_initialization 
tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_list_command 
[gw5] [ 57%] FAILED tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_list_command 
tests/test_loaders/__init__.py::ruff 
[gw5] [ 58%] PASSED tests/test_loaders/__init__.py::ruff 
tests/test_loaders/__init__.py::ruff::format 
[gw5] [ 59%] PASSED tests/test_loaders/__init__.py::ruff::format 
tests/test_loaders/__init__.py::black 
[gw4] [ 59%] PASSED tests/test_core/test_prompts.py::black 
tests/test_core/test_prompts.py::TestResolvedPrompt::test_resolved_prompt_creation 
[gw4] [ 60%] PASSED tests/test_core/test_prompts.py::TestResolvedPrompt::test_resolved_prompt_creation 
tests/test_core/test_prompts.py::TestResolvedPrompt::test_resolved_prompt_minimal 
[gw4] [ 60%] PASSED tests/test_core/test_prompts.py::TestResolvedPrompt::test_resolved_prompt_minimal 
tests/test_core/test_prompts.py::TestPromptResolver::test_resolver_initialization 
[gw6] [ 61%] PASSED tests/test_core/test_models.py::black 
tests/test_core/test_models.py::TestContextTestStatus::test_enum_values 
[gw6] [ 62%] PASSED tests/test_core/test_models.py::TestContextTestStatus::test_enum_values 
tests/test_loaders/test_context_loader.py::black 
[gw4] [ 62%] PASSED tests/test_core/test_prompts.py::TestPromptResolver::test_resolver_initialization 
tests/test_loaders/test_context_loader.py::ruff::format 
[gw4] [ 63%] PASSED tests/test_loaders/test_context_loader.py::ruff::format 
tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_with_encoding 
[gw4] [ 64%] PASSED tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_with_encoding 
tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_nonexistent_file 
[gw4] [ 64%] PASSED tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_nonexistent_file 
tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_read_error 
[gw4] [ 65%] PASSED tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_read_error 
tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_string_path 
[gw4] [ 65%] PASSED tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_string_path 
tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_large_file 
[gw4] [ 66%] PASSED tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_large_file 
tests/test_loaders/test_model_loader.py::ruff 
[gw4] [ 67%] PASSED tests/test_loaders/test_model_loader.py::ruff 
tests/test_loaders/test_model_loader.py::ruff::format 
[gw4] [ 67%] PASSED tests/test_loaders/test_model_loader.py::ruff::format 
tests/test_loaders/test_model_loader.py::black 
[gw0] [ 68%] PASSED tests/test_integration/__init__.py::black 
tests/test_integration/test_cli_integration.py::ruff 
[gw5] [ 68%] PASSED tests/test_loaders/__init__.py::black 
[gw2] [ 69%] PASSED tests/test_api/test_client.py::black 
tests/test_core/test_scanner.py::black 
tests/test_loaders/test_context_loader.py::ruff 
[gw5] [ 70%] PASSED tests/test_loaders/test_context_loader.py::ruff 
tests/test_loaders/test_model_loader.py::TestModelLoader::test_load_model_registry_nonexistent_file 
[gw5] [ 70%] PASSED tests/test_loaders/test_model_loader.py::TestModelLoader::test_load_model_registry_nonexistent_file 
tests/test_loaders/test_model_loader.py::TestModelLoader::test_save_model_registry_default_path 
[gw5] [ 71%] PASSED tests/test_loaders/test_model_loader.py::TestModelLoader::test_save_model_registry_default_path 
tests/test_loaders/test_model_loader.py::TestModelLoader::test_save_model_registry_custom_path 
[gw5] [ 71%] FAILED tests/test_loaders/test_model_loader.py::TestModelLoader::test_save_model_registry_custom_path 
tests/test_loaders/test_model_loader.py::TestModelLoader::test_scan_and_update_models 
[gw3] [ 72%] PASSED tests/test_integration/test_cli_integration.py::black 
tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_list_json_format 
[gw3] [ 73%] FAILED tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_list_json_format 
[gw5] [ 73%] FAILED tests/test_loaders/test_model_loader.py::TestModelLoader::test_scan_and_update_models 
tests/test_loaders/test_model_loader.py::TestModelLoader::test_scan_and_update_models_default_client 
[gw5] [ 74%] PASSED tests/test_loaders/test_model_loader.py::TestModelLoader::test_scan_and_update_models_default_client 
tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_infer_requires_parameters 
tests/test_loaders/test_prompt_loader.py::ruff 
[gw3] [ 75%] PASSED tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_infer_requires_parameters 
[gw0] [ 75%] FAILED tests/test_integration/test_cli_integration.py::ruff 
tests/test_loaders/test_prompt_loader.py::ruff::format 
tests/test_loaders/test_model_loader.py::TestModelLoader::test_load_model_registry_custom_path 
[gw0] [ 76%] PASSED tests/test_loaders/test_model_loader.py::TestModelLoader::test_load_model_registry_custom_path 
tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_nonexistent_file 
[gw0] [ 76%] PASSED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_nonexistent_file 
tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_invalid_toml 
[gw0] [ 77%] FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_invalid_toml 
tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_nested_placeholders 
[gw1] [ 78%] PASSED tests/run_tests.py::black 
[gw0] [ 78%] FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_nested_placeholders 
[gw5] [ 79%] PASSED tests/test_loaders/test_prompt_loader.py::ruff 
[gw3] [ 79%] PASSED tests/test_loaders/test_prompt_loader.py::ruff::format 
tests/test_api/__init__.py::ruff 
tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_simple 
tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_missing_params 
[gw3] [ 80%] FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_simple 
[gw0] [ 81%] FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_missing_params 
tests/test_loaders/test_prompt_loader.py::black 
[gw7] [ 81%] PASSED tests/test_core/test_context_tester.py::black 
tests/test_utils/__init__.py::ruff 
tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_custom_resolver 
[gw0] [ 82%] FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_custom_resolver 
tests/test_core/test_context_tester.py::TestContextTestResult::test_result_creation 
[gw1] [ 82%] PASSED tests/test_api/__init__.py::ruff 
tests/test_utils/test_paths.py::ruff 
[gw7] [ 83%] PASSED tests/test_core/test_context_tester.py::TestContextTestResult::test_result_creation 
tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_empty_file 
tests/test_utils/test_paths.py::ruff::format 
[gw3] [ 84%] PASSED tests/test_utils/__init__.py::ruff 
tests/test_utils/__init__.py::black 
[gw0] [ 84%] PASSED tests/test_utils/test_paths.py::ruff 
[gw7] [ 85%] PASSED tests/test_utils/test_paths.py::ruff::format 
tests/test_utils/test_paths.py::black 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_fallback_locations 
[gw7] [ 85%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_fallback_locations 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstrix_data_dir 
[gw7] [ 86%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstrix_data_dir 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstrix_data_dir_exists 
[gw7] [ 87%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstrix_data_dir_exists 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_default_models_file 
[gw7] [ 87%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_default_models_file 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_context_tests_dir 
[gw7] [ 88%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_context_tests_dir 
[gw1] [ 89%] PASSED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_empty_file 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_context_test_log_path 
[gw7] [ 89%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_context_test_log_path 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_from_pointer 
[gw1] [ 90%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_from_pointer 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_prompts_dir 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_contexts_dir 
[gw1] [ 90%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_contexts_dir 
[gw7] [ 91%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_get_prompts_dir 
tests/test_utils/test_paths.py::TestPathUtilities::test_directory_creation_permissions_error 
[gw7] [ 92%] PASSED tests/test_utils/test_paths.py::TestPathUtilities::test_directory_creation_permissions_error 
[gw6] [ 92%] PASSED tests/test_loaders/test_context_loader.py::black 
tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_simple 
[gw6] [ 93%] PASSED tests/test_loaders/test_context_loader.py::TestContextLoader::test_load_context_simple 
[gw4] [ 93%] PASSED tests/test_loaders/test_model_loader.py::black 
tests/test_loaders/test_model_loader.py::TestModelLoader::test_load_model_registry_default_path 
[gw4] [ 94%] PASSED tests/test_loaders/test_model_loader.py::TestModelLoader::test_load_model_registry_default_path 
[gw2] [ 95%] PASSED tests/test_core/test_scanner.py::black 
tests/test_core/test_scanner.py::TestModelScanner::test_scanner_initialization 
[gw2] [ 95%] PASSED tests/test_core/test_scanner.py::TestModelScanner::test_scanner_initialization 
tests/test_core/test_scanner.py::TestModelScanner::test_get_model_size_file 
[gw2] [ 96%] PASSED tests/test_core/test_scanner.py::TestModelScanner::test_get_model_size_file 
[gw5] [ 96%] PASSED tests/test_loaders/test_prompt_loader.py::black 
tests/test_utils/__init__.py::ruff::format 
[gw0] [ 97%] PASSED tests/test_utils/test_paths.py::black 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_not_found 
[gw0] [ 98%] FAILED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_not_found 
[gw5] [ 98%] PASSED tests/test_utils/__init__.py::ruff::format 
[gw3] [ 99%] PASSED tests/test_utils/__init__.py::black 
tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_shared_location 
[gw3] [100%] FAILED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_shared_location 

=================================== FAILURES ===================================
__________________ TestLMStudioClient.test_load_model_failure __________________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_api/test_client.py:146: in test_load_model_failure
    client.load_model("test-model", 4096)
src/lmstrix/api/client.py:72: in load_model
    return lmstudio.llm(model_path, config=config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1198: in _execute_mock_call
    raise effect
E   Exception: Model not found
__________________ TestLMStudioClient.test_completion_success __________________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_api/test_client.py:161: in test_completion_success
    result = client.completion(
src/lmstrix/api/client.py:221: in completion
    logger.info(f"ğŸ“ CONTEXT: {llm.config.contextLength:,} tokens")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: unsupported format string passed to Mock.__format__
__________________ TestLMStudioClient.test_completion_failure __________________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_api/test_client.py:187: in test_completion_failure
    client.completion(mock_llm, "test prompt")
src/lmstrix/api/client.py:221: in completion
    logger.info(f"ğŸ“ CONTEXT: {llm.config.contextLength:,} tokens")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: unsupported format string passed to Mock.__format__
_________________ TestLMStudioClient.test_list_models_success __________________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_api/test_client.py:109: in test_list_models_success
    assert result == expected_result
E   AssertionError: assert [{'architectu...': True, ...}] == [{'architectu...': True, ...}]
E     
E     At index 1 diff: {'id': 'model2', 'path': '/path/to/model2', 'size_bytes': 2000, 'context_length': 8192, 'display_name': 'Model Two', 'architecture': 'mistral', 'has_tools': True, 'has_vision': False, 'model_type': 'llm'} != {'id': 'model2', 'path': '/path/to/model2', 'size_bytes': 2000, 'context_length': 8192, 'display_name': 'Model Two', 'architecture': 'mistral', 'has_tools': True, 'has_vision': True, 'model_type': 'llm'}
E     
E     Full diff:
E       [
E           {
E               'architecture': 'llama',...
E     
E     ...Full output truncated (24 lines hidden), use '-vv' to show
_______________ TestLMStudioClient.test_completion_with_defaults _______________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_api/test_client.py:201: in test_completion_with_defaults
    client.completion(
src/lmstrix/api/client.py:221: in completion
    logger.info(f"ğŸ“ CONTEXT: {llm.config.contextLength:,} tokens")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: unsupported format string passed to Mock.__format__
_________________ TestContextTester.test_test_model_not_found __________________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_context_tester.py:76: in test_test_model_not_found
    with patch.object(tester, "_test_at_context") as mock_test:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: in get_original
    raise AttributeError(
E   AttributeError: <lmstrix.core.context_tester.ContextTester object at 0x144403320> does not have the attribute '_test_at_context'
____________________ TestContextTester.test_test_all_models ____________________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_context_tester.py:107: in test_test_all_models
    with patch.object(tester, "test_model") as mock_test:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1467: in __enter__
    original, local = self.get_original()
                      ^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1437: in get_original
    raise AttributeError(
E   AttributeError: <lmstrix.core.context_tester.ContextTester object at 0x143bd68a0> does not have the attribute 'test_model'
_____________________ TestModel.test_model_path_validation _____________________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_models.py:94: in test_model_path_validation
    assert isinstance(model.path, Path)
E   AssertionError: assert False
E    +  where False = isinstance('/string/path/model.gguf', Path)
E    +    where '/string/path/model.gguf' = <lmstrix.core.models.Model object at 0x1444357c0>.path
____________________ TestModel.test_model_validation_error _____________________
[gw4] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_models.py:152: in test_model_validation_error
    Model(id="test")  # Missing required fields
    ^^^^^^^^^^^^^^^^
E   TypeError: Model.__init__() missing 3 required positional arguments: 'path', 'size_bytes', and 'ctx_in'
_________________ TestPromptResolver.test_resolve_phase_simple _________________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:84: in test_resolve_phase_simple
    resolved, found, resolved_list, unresolved = resolver._resolve_phase(template, context)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute '_resolve_phase'
__________ TestPromptResolver.test_resolve_phase_missing_placeholder ___________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:98: in test_resolve_phase_missing_placeholder
    resolved, found, resolved_list, unresolved = resolver._resolve_phase(template, context)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute '_resolve_phase'
_____________ TestPromptResolver.test_resolve_phase_extra_context ______________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:112: in test_resolve_phase_extra_context
    resolved, found, resolved_list, unresolved = resolver._resolve_phase(template, context)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute '_resolve_phase'
___________ TestPromptResolver.test_resolve_with_special_characters ____________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:232: in test_resolve_with_special_characters
    result = resolver.resolve_template("special", template, context, {})
             ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute 'resolve_template'
______________ TestPromptResolver.test_resolve_template_two_phase ______________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:127: in test_resolve_template_two_phase
    result = resolver.resolve_template("test", template, prompt_context, runtime_context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute 'resolve_template'
______________ TestPromptResolver.test_resolve_template_recursive ______________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:147: in test_resolve_template_recursive
    result = resolver.resolve_template("recursive", template, prompt_context, {})
             ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute 'resolve_template'
_________ TestPromptResolver.test_resolve_template_circular_reference __________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:164: in test_resolve_template_circular_reference
    result = resolver.resolve_template("circular", template, prompt_context, {})
             ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute 'resolve_template'
___________ TestPromptResolver.test_resolve_template_no_placeholders ___________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:176: in test_resolve_template_no_placeholders
    result = resolver.resolve_template("static", template, {}, {})
             ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute 'resolve_template'
____________ TestModelScanner.test_extract_model_info_mlx_directory ____________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_scanner.py:87: in test_extract_model_info_mlx_directory
    info = scanner._extract_model_info(model_dir)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/lmstrix/core/scanner.py:62: in _extract_model_info
    relative_path = model_path.relative_to(self.models_dir)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:682: in relative_to
    raise ValueError(f"{str(self)!r} is not in the subpath of {str(other)!r}")
E   ValueError: '/private/var/folders/wd/jx_fxd9d7wg_py26vfx_tz080000gn/T/pytest-of-adam/pytest-3/popen-gw3/test_extract_model_info_mlx_di0/llama-7b-mlx' is not in the subpath of '/Users/Shared/lmstudio/models'
___________ TestPromptResolver.test_resolve_template_numeric_values ____________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:190: in test_resolve_template_numeric_values
    result = resolver.resolve_template("numeric", template, context, {})
             ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute 'resolve_template'
______________ TestModelScanner.test_extract_model_info_gguf_file ______________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_scanner.py:69: in test_extract_model_info_gguf_file
    info = scanner._extract_model_info(model_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/lmstrix/core/scanner.py:62: in _extract_model_info
    relative_path = model_path.relative_to(self.models_dir)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:682: in relative_to
    raise ValueError(f"{str(self)!r} is not in the subpath of {str(other)!r}")
E   ValueError: '/private/var/folders/wd/jx_fxd9d7wg_py26vfx_tz080000gn/T/pytest-of-adam/pytest-3/popen-gw5/test_extract_model_info_gguf_f0/llama-7b.gguf' is not in the subpath of '/Users/Shared/lmstudio/models'
_____________ TestPromptResolver.test_resolve_template_empty_value _____________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:202: in test_resolve_template_empty_value
    result = resolver.resolve_template("empty", template, context, {})
             ^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute 'resolve_template'
______________________ TestModelScanner.test_scan_models _______________________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_scanner.py:143: in test_scan_models
    model_ids = [m["id"] for m in models]
                 ^^^^^^^
E   TypeError: string indices must be integers, not 'str'
_____________________ TestPromptResolver.test_count_tokens _____________________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_prompts.py:212: in test_count_tokens
    count = resolver._count_tokens("Hello World!")
            ^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'PromptResolver' object has no attribute '_count_tokens'
___________________ TestModelScanner.test_sync_with_registry ___________________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_core/test_scanner.py:176: in test_sync_with_registry
    new_models, removed_models = scanner.sync_with_registry(mock_registry)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'ModelScanner' object has no attribute 'sync_with_registry'
_____________________ TestCLIIntegration.test_list_command _____________________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_integration/test_cli_integration.py:55: in test_list_command
    assert "test-model" in captured.out
E   AssertionError: assert 'test-model' in ''
E    +  where '' = CaptureResult(out='', err='').out
_____________ TestModelLoader.test_save_model_registry_custom_path _____________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_loaders/test_model_loader.py:122: in test_save_model_registry_custom_path
    assert custom_file.exists()
E   AssertionError: assert False
E    +  where False = exists()
E    +    where exists = PosixPath('/private/var/folders/wd/jx_fxd9d7wg_py26vfx_tz080000gn/T/pytest-of-adam/pytest-3/popen-gw5/test_save_model_registry_custo0/custom.json').exists
___________________ TestCLIIntegration.test_list_json_format ___________________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_integration/test_cli_integration.py:68: in test_list_json_format
    data = json.loads(captured.out)
           ^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:346: in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:338: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:356: in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
E   json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
_________________ TestModelLoader.test_scan_and_update_models __________________
[gw5] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_loaders/test_model_loader.py:158: in test_scan_and_update_models
    updated_registry = scan_and_update_registry(
src/lmstrix/loaders/model_loader.py:275: in scan_and_update_registry
    new_model = _add_new_model(model_data)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
src/lmstrix/loaders/model_loader.py:181: in _add_new_model
    if not new_model.validate_integrity():
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   AttributeError: 'Model' object has no attribute 'validate_integrity'
----------------------------- Captured stderr call -----------------------------
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=96773), 'exception': None, 'extra': {}, 'file': (name='model_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py'), 'function': 'scan_and_update_registry', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 234, 'message': 'Scanning for LM Studio models...', 'module': 'model_loader', 'name': 'lmstrix.loaders.model_loader', 'process': (id=17703, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 542491, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=97821), 'exception': None, 'extra': {}, 'file': (name='model_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py'), 'function': 'scan_and_update_registry', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 237, 'message': 'Found 1 models in LM Studio.', 'module': 'model_loader', 'name': 'lmstrix.loaders.model_loader', 'process': (id=17703, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 543539, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=98214), 'exception': None, 'extra': {}, 'file': (name='model_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py'), 'function': 'scan_and_update_registry', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 250, 'message': 'Processing 1 valid models (filtered from 1)', 'module': 'model_loader', 'name': 'lmstrix.loaders.model_loader', 'process': (id=17703, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 543932, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=98573), 'exception': None, 'extra': {}, 'file': (name='model_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py'), 'function': '_add_new_model', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 166, 'message': 'Discovered new model: /path/to/new-model.gguf', 'module': 'model_loader', 'name': 'lmstrix.loaders.model_loader', 'process': (id=17703, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 544291, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
_________________________________ test session _________________________________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/_pytest/runner.py:344: in from_call
    result: TResult | None = func()
                             ^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/flaky/flaky_pytest_plugin.py:146: in <lambda>
    lambda: ihook(item=item, **kwds), when=when, reraise=reraise
            ^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_hooks.py:512: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:167: in _multicall
    raise exception
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:139: in _multicall
    teardown.throw(exception)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/_pytest/logging.py:850: in pytest_runtest_call
    yield
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:139: in _multicall
    teardown.throw(exception)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:53: in run_old_style_hookwrapper
    return result.get_result()
           ^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_result.py:103: in get_result
    raise exc.with_traceback(tb)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:38: in run_old_style_hookwrapper
    res = yield
          ^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:139: in _multicall
    teardown.throw(exception)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/_pytest/capture.py:900: in pytest_runtest_call
    return (yield)
            ^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:139: in _multicall
    teardown.throw(exception)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:53: in run_old_style_hookwrapper
    return result.get_result()
           ^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_result.py:103: in get_result
    raise exc.with_traceback(tb)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:38: in run_old_style_hookwrapper
    res = yield
          ^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:139: in _multicall
    teardown.throw(exception)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:53: in run_old_style_hookwrapper
    return result.get_result()
           ^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_result.py:103: in get_result
    raise exc.with_traceback(tb)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:38: in run_old_style_hookwrapper
    res = yield
          ^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:139: in _multicall
    teardown.throw(exception)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/_pytest/skipping.py:263: in pytest_runtest_call
    return (yield)
            ^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pluggy/_callers.py:121: in _multicall
    res = hook_impl.function(*args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/_pytest/runner.py:178: in pytest_runtest_call
    item.runtest()
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_ruff/__init__.py:141: in runtest
    self.handler(path=self.fspath)
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_ruff/__init__.py:151: in handler
    return check_file(path)
           ^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytest_ruff/__init__.py:101: in check_file
    raise RuffError(stdout.decode())
E   pytest_ruff.RuffError: tests/test_integration/test_cli_integration.py:47:75: ANN001 Missing type annotation for function argument `capsys`
E      |
E   46 |     @patch("lmstrix.cli.main.get_default_models_file")
E   47 |     def test_list_command(self, mock_get_file: Mock, mock_registry: Path, capsys) -> None:
E      |                                                                           ^^^^^^ ANN001
E   48 |         """Test list command shows models."""
E   49 |         mock_get_file.return_value = mock_registry
E      |
E   
E   tests/test_integration/test_cli_integration.py:59:79: ANN001 Missing type annotation for function argument `capsys`
E      |
E   58 |     @patch("lmstrix.cli.main.get_default_models_file")
E   59 |     def test_list_json_format(self, mock_get_file: Mock, mock_registry: Path, capsys) -> None:
E      |                                                                               ^^^^^^ ANN001
E   60 |         """Test list command with JSON output."""
E   61 |         mock_get_file.return_value = mock_registry
E      |
_______________ TestPromptLoader.test_load_prompts_invalid_toml ________________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_loaders/test_prompt_loader.py:68: in test_load_prompts_invalid_toml
    assert "parse" in str(exc_info.value).lower()
E   assert 'parse' in "configuration error for 'prompts_file': failed to load toml file: expected '=' after a key in a key/value pair (at line 1, column 6)"
E    +  where "configuration error for 'prompts_file': failed to load toml file: expected '=' after a key in a key/value pair (at line 1, column 6)" = <built-in method lower of str object at 0x11fc87890>()
E    +    where <built-in method lower of str object at 0x11fc87890> = "Configuration error for 'prompts_file': Failed to load TOML file: Expected '=' after a key in a key/value pair (at line 1, column 6)".lower
E    +      where "Configuration error for 'prompts_file': Failed to load TOML file: Expected '=' after a key in a key/value pair (at line 1, column 6)" = str(ConfigurationError("Configuration error for 'prompts_file': Failed to load TOML file: Expected '=' after a key in a key/value pair (at line 1, column 6)"))
E    +        where ConfigurationError("Configuration error for 'prompts_file': Failed to load TOML file: Expected '=' after a key in a key/value pair (at line 1, column 6)") = <ExceptionInfo ConfigurationError("Configuration error for 'prompts_file': Failed to load TOML file: Expected '=' after a key in a key/value pair (at line 1, column 6)") tblen=2>.value
_________ TestPromptLoader.test_load_prompts_with_nested_placeholders __________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_loaders/test_prompt_loader.py:89: in test_load_prompts_with_nested_placeholders
    prompts = load_prompts(toml_file, content="Test message")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/lmstrix/loaders/prompt_loader.py:80: in load_prompts
    resolved_prompts = resolver.resolve_all_prompts(data, **params)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/lmstrix/core/prompts.py:243: in resolve_all_prompts
    process_prompts(prompts_data)
src/lmstrix/core/prompts.py:241: in process_prompts
    process_prompts(value, f"{full_key}.")
src/lmstrix/core/prompts.py:237: in process_prompts
    results[full_key] = self.resolve_prompt(full_key, prompts_data, **params)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/lmstrix/core/prompts.py:196: in resolve_prompt
    current_text, resolved = self._resolve_external(current_text, MappingProxyType(params))
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/lmstrix/core/prompts.py:133: in _resolve_external
    result = temp.format_map(_SafeDict(params))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   ValueError: Invalid format specifier ' 'Important'' for object of type 'str'
__________________ TestPromptLoader.test_load_prompts_simple ___________________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_loaders/test_prompt_loader.py:35: in test_load_prompts_simple
    assert len(prompts) == 2
E   AssertionError: assert 4 == 2
E    +  where 4 = len({'greeting.description': ResolvedPrompt(name='greeting.description', template='A simple greeting', resolved='A simple greeting', tokens=3, placeholders_found=[], placeholders_resolved=[], placeholders_unresolved=[]), 'greeting.template': ResolvedPrompt(name='greeting.template', template='Hello World!', resolved='Hello World!', tokens=3, placeholders_found=[], placeholders_resolved=[], placeholders_unresolved=[]), 'question.description': ResolvedPrompt(name='question.description', template='Question template', resolved='Question template', tokens=2, placeholders_found=[], placeholders_resolved=[], placeholders_unresolved=[]), 'question.template': ResolvedPrompt(name='question.template', template='What is Python? Please provide a detailed answer.', resolved='What is Python? Please provide a detailed answer.', tokens=10, placeholders_found=[], placeholders_resolved=[], placeholders_unresolved=[])})
----------------------------- Captured stderr call -----------------------------
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=542690), 'exception': None, 'extra': {}, 'file': (name='prompt_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py'), 'function': 'load_prompts', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 57, 'message': 'Loaded prompts from /private/var/folders/wd/jx_fxd9d7wg_py26vfx_tz080000gn/T/pytest-of-adam/pytest-3/popen-gw3/test_load_prompts_simple0/prompts.toml', 'module': 'prompt_loader', 'name': 'lmstrix.loaders.prompt_loader', 'process': (id=17701, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 761208, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=544098), 'exception': None, 'extra': {}, 'file': (name='prompt_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py'), 'function': 'load_prompts', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 70, 'message': 'Resolved placeholders with TOPL', 'module': 'prompt_loader', 'name': 'lmstrix.loaders.prompt_loader', 'process': (id=17701, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 762616, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=546309), 'exception': None, 'extra': {}, 'file': (name='prompt_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py'), 'function': 'load_prompts', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 82, 'message': 'Resolved 4 prompts', 'module': 'prompt_loader', 'name': 'lmstrix.loaders.prompt_loader', 'process': (id=17701, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 764827, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=6, microseconds=546653), 'exception': None, 'extra': {}, 'file': (name='prompt_loader.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py'), 'function': 'load_prompts', 'level': (name='INFO', no=20, icon='â„¹ï¸'), 'line': 84, 'message': "Applied parameters: ['name', 'query']", 'module': 'prompt_loader', 'name': 'lmstrix.loaders.prompt_loader', 'process': (id=17701, name='MainProcess'), 'thread': (id=8520598848, name='MainThread'), 'time': datetime(2025, 7, 31, 17, 10, 34, 765171, tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 206, in emit
    self._sink.write(str_record)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_simple_sinks.py", line 16, in write
    self._stream.write(message)
ValueError: I/O operation on closed file.
--- End of logging error ---
____________ TestPromptLoader.test_load_prompts_with_missing_params ____________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_loaders/test_prompt_loader.py:110: in test_load_prompts_with_missing_params
    template1 = prompts["template1"]
                ^^^^^^^^^^^^^^^^^^^^
E   KeyError: 'template1'
------------------------------ Captured log call -------------------------------
WARNING  topl.core:core.py:155 Could not resolve 1 placeholder(s): {{city}}
___________ TestPromptLoader.test_load_prompts_with_custom_resolver ____________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_loaders/test_prompt_loader.py:130: in test_load_prompts_with_custom_resolver
    assert prompts["test"].resolved == "Result: 42"
           ^^^^^^^^^^^^^^^
E   KeyError: 'test'
______________ TestPathUtilities.test_get_lmstudio_path_not_found ______________
[gw0] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_utils/test_paths.py:96: in test_get_lmstudio_path_not_found
    with pytest.raises(RuntimeError) as exc_info:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
E   Failed: DID NOT RAISE <class 'RuntimeError'>
___________ TestPathUtilities.test_get_lmstudio_path_shared_location ___________
[gw3] darwin -- Python 3.12.9 /usr/local/bin/python
tests/test_utils/test_paths.py:86: in test_get_lmstudio_path_shared_location
    result = get_lmstudio_path()
             ^^^^^^^^^^^^^^^^^^^
src/lmstrix/utils/paths.py:21: in get_lmstudio_path
    if home_pointer.exists():
       ^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1139: in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1143: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1204: in _execute_mock_call
    result = effect(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^
E   TypeError: TestPathUtilities.test_get_lmstudio_path_shared_location.<locals>.exists_side_effect() missing 1 required positional argument: 'self'

---------- coverage: platform darwin, python 3.12.9-final-0 ----------
Name                                    Stmts   Miss Branch BrPart   Cover
--------------------------------------------------------------------------
src/lmstrix/__main__.py                     1      1      0      0   0.00%
src/lmstrix/_version.py                    13      3      2      1  73.33%
src/lmstrix/api/client.py                 154    103     50      3  28.43%
src/lmstrix/api/exceptions.py              72     18      2      1  74.32%
src/lmstrix/cli/main.py                   517    478    196      1   5.61%
src/lmstrix/core/concrete_config.py        62     50     22      0  14.29%
src/lmstrix/core/context.py                81     59     14      0  23.16%
src/lmstrix/core/context_tester.py         27      1      0      0  96.30%
src/lmstrix/core/inference.py             168    168     38      0   0.00%
src/lmstrix/core/inference_manager.py     115     75     24      5  33.81%
src/lmstrix/core/models.py                129     30     20      0  75.84%
src/lmstrix/core/prompts.py               117     25     40      5  73.25%
src/lmstrix/core/scanner.py                86     31     42      7  65.62%
src/lmstrix/loaders/context_loader.py      63     39     12      0  37.33%
src/lmstrix/loaders/model_loader.py       167     87     56     12  44.84%
src/lmstrix/loaders/prompt_loader.py       77     42     18      0  45.26%
src/lmstrix/utils/context_parser.py        32     28     14      0   8.70%
src/lmstrix/utils/logging.py               23      6      4      2  70.37%
src/lmstrix/utils/paths.py                 53      5     16      4  86.96%
src/lmstrix/utils/state.py                 31     20      4      0  31.43%
--------------------------------------------------------------------------
TOTAL                                    1988   1269    574     41  33.10%

=========================== short test summary info ============================
FAILED tests/test_api/test_client.py::TestLMStudioClient::test_load_model_failure
FAILED tests/test_api/test_client.py::TestLMStudioClient::test_completion_success
FAILED tests/test_api/test_client.py::TestLMStudioClient::test_completion_failure
FAILED tests/test_api/test_client.py::TestLMStudioClient::test_list_models_success
FAILED tests/test_api/test_client.py::TestLMStudioClient::test_completion_with_defaults
FAILED tests/test_core/test_context_tester.py::TestContextTester::test_test_model_not_found
FAILED tests/test_core/test_context_tester.py::TestContextTester::test_test_all_models
FAILED tests/test_core/test_models.py::TestModel::test_model_path_validation
FAILED tests/test_core/test_models.py::TestModel::test_model_validation_error
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_simple
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_missing_placeholder
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_phase_extra_context
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_with_special_characters
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_two_phase
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_recursive
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_circular_reference
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_no_placeholders
FAILED tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_mlx_directory
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_numeric_values
FAILED tests/test_core/test_scanner.py::TestModelScanner::test_extract_model_info_gguf_file
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_resolve_template_empty_value
FAILED tests/test_core/test_scanner.py::TestModelScanner::test_scan_models - ...
FAILED tests/test_core/test_prompts.py::TestPromptResolver::test_count_tokens
FAILED tests/test_core/test_scanner.py::TestModelScanner::test_sync_with_registry
FAILED tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_list_command
FAILED tests/test_loaders/test_model_loader.py::TestModelLoader::test_save_model_registry_custom_path
FAILED tests/test_integration/test_cli_integration.py::TestCLIIntegration::test_list_json_format
FAILED tests/test_loaders/test_model_loader.py::TestModelLoader::test_scan_and_update_models
FAILED tests/test_integration/test_cli_integration.py::ruff - pytest_ruff.Ruf...
FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_invalid_toml
FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_nested_placeholders
FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_simple
FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_missing_params
FAILED tests/test_loaders/test_prompt_loader.py::TestPromptLoader::test_load_prompts_with_custom_resolver
FAILED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_not_found
FAILED tests/test_utils/test_paths.py::TestPathUtilities::test_get_lmstudio_path_shared_location
======================= 36 failed, 128 passed in 18.95s ========================

</document_content>
</document>

<document index="17">
<source>docs/_config.yml</source>
<document_content>
title: LMStrix
description: The Unofficial Toolkit for Mastering LM Studio
remote_theme: just-the-docs/just-the-docs

url: https://twardoch.github.io/lmstrix

color_scheme: dark

search_enabled: false

aux_links:
  "LMStrix on GitHub":
    - "https://github.com/twardoch/lmstrix"

plugins:
  - jekyll-remote-theme
</document_content>
</document>

<document index="18">
<source>docs/api.md</source>
<document_content>
---
title: API Reference
---

## Python API Reference

This section provides a detailed reference for the LMStrix Python API.

### The `LMStrix` Class

The main entry point for interacting with the API.

`LMStrix(base_uri: str = "http://localhost:1234/v1")`

- `base_uri`: The base URI of the LM Studio server.

#### Methods

- `async scan_models()`: Scans for available models in LM Studio and updates the local model registry.
- `async list_models()`: Returns a list of `Model` objects from the local registry.
- `async test_model(model_id: str)`: Tests the context limit of a specific model and updates the registry with the result.
- `async infer(prompt: str, model_id: str, **kwargs)`: Runs inference on a model. Any additional keyword arguments are passed to the `complete()` method of the `lmstudio` client.

### The `Model` Class

Represents a model in LM Studio.

#### Attributes

- `id`: The model ID (e.g., `lmstudio-community/gemma-2b-it-GGUF`).
- `context_limit`: The declared context limit of the model, as reported by LM Studio.
- `tested_max_context`: The empirically tested maximum context limit that the model can handle on your machine. `None` if the model has not been tested.
- `context_test_status`: The status of the context test. Can be one of `"passed"`, `"failed"`, or `"not_tested"`.

### The `InferenceResponse` Class

Represents the response from an inference request.

#### Attributes

- `content`: The text content of the model's response.
- `usage`: A dictionary containing token usage information, e.g., `{'prompt_tokens': 10, 'completion_tokens': 20, 'total_tokens': 30}`.

</document_content>
</document>

<document index="19">
<source>docs/how-it-works.md</source>
<document_content>
---
title: How It Works
---

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **adaptive testing algorithm** (enhanced in v1.1):

1. **Initial Verification**: Tests at 1,024 tokens to ensure the model loads properly
2. **Threshold Test**: Tests at min(threshold, declared_max) where threshold defaults to 102,400 tokens
   - This prevents system crashes from attempting very large context sizes
3. **Adaptive Search**:
   - If the threshold test succeeds and is below the declared max: incrementally increases by 10,240 tokens until failure
   - If the threshold test fails: performs binary search between 1,024 and the failed size
4. **Progress Tracking**: Saves results after each test, allowing resumption if interrupted

**Batch Testing Optimization** (new in v1.1):
When testing multiple models with `--all`, LMStrix now:
- Sorts models by declared context size (ascending)
- Tests in passes to minimize model loading/unloading
- Excludes failed models from subsequent passes
- Provides detailed progress with Rich table output

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

### The `test` Command

The `test` command is the heart of the context optimization process. When you run `lmstrix test <model-id>`, it performs the binary search algorithm described above.

The command saves the results of the test to a local registry, so you only need to test each model once.

</document_content>
</document>

<document index="20">
<source>docs/index.md</source>
<document_content>
---
---

# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

</document_content>
</document>

<document index="21">
<source>docs/installation.md</source>
<document_content>
---
title: Installation
---

## Installation

### Requirements

- Python 3.10 or higher
- [LM Studio](https://lmstudio.ai/) installed and running locally
- At least one model downloaded in LM Studio

### Using pip

```bash
pip install lmstrix
```

### Using uv (recommended)

```bash
uv pip install lmstrix
```

### For Development

If you want to contribute to LMStrix or install it in an editable mode, you can clone the repository:

```bash
git clone https://github.com/twardoch/lmstrix
cd lmstrix
pip install -e .[dev]
```

</document_content>
</document>

<document index="22">
<source>docs/usage.md</source>
<document_content>
---
title: Usage
---

## Command-Line Interface (CLI)

LMStrix provides a powerful and intuitive CLI for interacting with your local models.

### Scanning for Models

Before you can use LMStrix, you need to scan for available models in LM Studio. This command discovers all models that you have downloaded.

```bash
lmstrix scan
```

### Listing Models

To see a list of all discovered models, their context length, and test status, use the `list` command.

```bash
lmstrix list
```

### Testing Context Limits

This is the core feature of LMStrix. The `test` command automatically determines the maximum context window a model can handle on your machine.

```bash
# Test a specific model by its ID
lmstrix test "model-id-here"

# Test all models that haven't been tested yet
lmstrix test --all

# Test with a custom threshold (default: 102,400 tokens)
# This prevents system crashes by limiting the maximum initial test size
lmstrix test "model-id-here" --threshold 51200

# Test all models with a lower threshold for safety
lmstrix test --all --threshold 32768
```

**New in v1.1**: The `--threshold` parameter (default: 102,400 tokens) prevents system crashes when testing models with very large declared context sizes. The testing algorithm now:
1. Tests at 1,024 tokens to verify the model loads
2. Tests at min(threshold, declared_max)
3. If successful and below declared max, increments by 10,240 tokens
4. If failed, performs binary search to find the exact limit

For more details on how this works, see the [How It Works](./how-it-works.md) page.

### Running Inference

You can run inference directly from the command line.

```bash
# Run a simple prompt
lmstrix infer "Your prompt here" --model "model-id" --max-tokens 150

# Use a prompt from a file
lmstrix infer "@prompts.toml:greeting" --model "model-id"
```

### Verbose Mode

For more detailed output and debugging, you can use the `--verbose` flag with any command.

```bash
lmstrix scan --verbose
```

## Python API

The Python API provides a clean, `async`-first interface for programmatic access to LMStrix features.

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    lms = LMStrix()
    
    # Scan for available models
    await lms.scan_models()
    
    # List all models
    models = await lms.list_models()
    for model in models:
        print(f"Model: {model.id}")
        print(f"  Context limit: {model.context_limit:,} tokens")
        print(f"  Tested limit: {model.tested_max_context or 'Not tested'}")
        print(f"  Status: {model.context_test_status}")
    
    # Test a specific model's context limits
    model_id = models[0].id if models else None
    if model_id:
        print(f"\nTesting context limits for {model_id}...")
        result = await lms.test_model(model_id)
        print(f"Optimal context: {result.tested_max_context} tokens")
        print(f"Test status: {result.context_test_status}")
    
    # Run inference
    if model_id:
        response = await lms.infer(
            prompt="What is the meaning of life?",
            model_id=model_id,
            max_tokens=100
        )
        print(f"\nInference result:\n{response.content}")

if __name__ == "__main__":
    asyncio.run(main())
```

</document_content>
</document>

<document index="23">
<source>examples/README.md</source>
<document_content>
# LMStrix Usage Examples

This directory contains comprehensive examples demonstrating all features of the LMStrix CLI and Python API, including the latest context control, prompt templates, and model state management features.

## Prerequisites

1. **LMStrix Installed**: Ensure you have installed LMStrix (`pip install lmstrix`)
2. **LM Studio Running**: Most examples require LM Studio running with at least one model downloaded
3. **Model Downloaded**: Download models in LM Studio (e.g., Llama, Mistral, Phi, Qwen)

**Note**: Examples use "llama" as a placeholder model ID. Update this to match your downloaded models. Run `lmstrix list` to see available models.

## Quick Start

Run all examples at once:
```bash
bash run_all_examples.sh
```

Or run individual examples as shown below.

---

## CLI Examples (`cli/`)

### Core Workflow Examples

#### `basic_workflow.sh`
Complete end-to-end workflow demonstrating:
- Model scanning with verbose output
- Listing models with `--sort` and `--show` options
- Context testing with new fixed strategy
- Inference with `--in_ctx` and `--out_ctx` parameters
- Model state reuse demonstration

```bash
bash cli/basic_workflow.sh
```

#### `model_testing.sh`
Advanced context testing features:
- Fixed context testing strategy (30k, 40k, 60k, etc.)
- `--threshold` parameter for safety limits
- `--fast` mode for quick validation
- `--ctx` for specific context testing
- Batch testing with `--all`
- Test resumption capabilities

```bash
bash cli/model_testing.sh
```

#### `inference_examples.sh`
Comprehensive inference scenarios:
- Context control with `--in_ctx` and `--out_ctx`
- Load without context specification (`--in_ctx 0`)
- Model state detection and reuse
- `--force-reload` demonstration
- TOML prompt templates with `--file_prompt` and `--dict`
- Temperature control for creativity

```bash
bash cli/inference_examples.sh
```

### New Feature Examples

#### `context_control_examples.sh` *(NEW)*
Deep dive into context management:
- Understanding `--in_ctx` vs `--out_ctx`
- Memory-conscious loading strategies
- Context size performance impact
- Long document processing
- Optimal context selection

```bash
bash cli/context_control_examples.sh
```

#### `model_state_demo.sh` *(NEW)*
Model state detection and management:
- How model reuse works
- Performance comparison (load vs reuse)
- Force reload scenarios
- Model switching strategies
- Context change behavior

```bash
bash cli/model_state_demo.sh
```

---

## Python API Examples (`python/`)

### Core API Usage

#### `basic_usage.py`
Fundamentals of the LMStrix Python API:
- Initializing `LMStrix` client
- Scanning and listing models
- Basic inference with `out_ctx`
- Advanced inference with `in_ctx`
- Model state detection
- Error handling

```bash
python3 python/basic_usage.py
```

#### `advanced_testing.py`
Context testing programmatically:
- Fixed context testing strategy
- Fast mode testing
- Custom threshold limits
- Specific context testing
- Batch testing multiple models
- Test result analysis

```bash
python3 python/advanced_testing.py
```

#### `custom_inference.py`
Advanced inference techniques:
- Context control (`in_ctx` and `out_ctx`)
- Temperature adjustment
- TOML prompt template loading
- Structured JSON output
- Model state reuse
- Force reload scenarios

```bash
python3 python/custom_inference.py
```

#### `batch_processing.py`
Working with multiple models:
- Batch testing strategies
- Model response comparison
- Performance benchmarking
- Efficiency analysis
- Smart testing order

```bash
python3 python/batch_processing.py
```

### New Python Examples

#### `prompt_templates_demo.py` *(NEW)*
Advanced prompt template features:
- Creating prompts programmatically
- Nested placeholder resolution
- Loading from TOML files
- Batch prompt resolution
- Missing placeholder handling
- Context injection with truncation

```bash
python3 python/prompt_templates_demo.py
```

---

## Prompt Templates (`prompts/`)

### Main Template File

#### `prompts.toml`
Comprehensive prompt template examples:
- Greetings (formal, casual, professional)
- Templates with internal references
- Code review and explanation prompts
- Research summarization templates
- Math and science prompts
- Creative writing templates
- Q&A formats
- System prompts

### Domain-Specific Templates

- `prompts/analysis.toml` - Data analysis prompts
- `prompts/coding.toml` - Programming assistance
- `prompts/creative.toml` - Creative writing
- `prompts/qa.toml` - Question answering

---

## Data Files (`data/`)

- `sample_context.txt` - Large text for context testing
- `test_questions.json` - Sample Q&A scenarios

---

## Key Features Demonstrated

### Context Management
- **`--in_ctx`**: Control model loading context size
- **`--out_ctx`**: Control maximum generation tokens
- **Model reuse**: Automatic reuse of loaded models
- **Force reload**: Refresh models with `--force-reload`

### Testing Enhancements
- **Fixed contexts**: Tests at 30k, 40k, 60k, 80k, 100k, 120k
- **Threshold safety**: Limit initial test size
- **Fast mode**: Skip semantic verification
- **Test resumption**: Continue interrupted tests

### Prompt Templates
- **TOML loading**: Load prompts from `.toml` files
- **Placeholders**: Dynamic value substitution
- **Nested references**: Templates can reference other templates
- **Batch resolution**: Process multiple prompts at once

### Performance Features
- **Smart sorting**: Optimal model testing order
- **Model state tracking**: Know when models are loaded
- **Efficiency metrics**: Track tested vs declared ratios
- **Batch operations**: Test or query multiple models

---

## Running Examples

### Individual Examples
```bash
# CLI examples
bash cli/basic_workflow.sh
bash cli/model_testing.sh
bash cli/inference_examples.sh
bash cli/context_control_examples.sh
bash cli/model_state_demo.sh

# Python examples
python3 python/basic_usage.py
python3 python/advanced_testing.py
python3 python/custom_inference.py
python3 python/batch_processing.py
python3 python/prompt_templates_demo.py
```

### All Examples
```bash
bash run_all_examples.sh
```

---

## Tips for Success

1. **Update Model IDs**: Change "llama" to match your downloaded models
2. **Check Model List**: Run `lmstrix list` to see available models
3. **Start Small**: Begin with `basic_workflow.sh` or `basic_usage.py`
4. **Test First**: Run `lmstrix test` to find optimal context for your models
5. **Use Verbose Mode**: Add `--verbose` for detailed output
6. **Monitor Memory**: Larger contexts use more GPU/RAM

---

## Troubleshooting

If examples fail:
1. Ensure LM Studio is running
2. Check you have models downloaded
3. Update model IDs in scripts
4. Verify LMStrix installation: `lmstrix --version`
5. Check available models: `lmstrix list`

For more help, see the main [LMStrix documentation](https://github.com/AdamAdli/lmstrix).
</document_content>
</document>

<document index="24">
<source>examples/cli/basic_workflow.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates the complete, basic workflow of LMStrix.
# 1. Scan for downloaded models.
# 2. List the models found with various display options.
# 3. Test the context length of a specific model.
# 4. Run inference with the tested model using new features.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Basic Workflow Demo ###"

# Step 1: Scan for models
# This command discovers all models downloaded in your LM Studio installation
# and updates the local registry file (lmstrix.json).
echo -e "\n--- Step 1: Scanning for models ---"
lmstrix scan --verbose
echo "Scan complete. Model registry updated."

# Step 2: List models with different display options
# This demonstrates the new --show and --sort options
echo -e "\n--- Step 2a: Listing models (default view) ---"
lmstrix list
echo -e "\n--- Step 2b: Listing models sorted by context size ---"
lmstrix list --sort ctx
echo -e "\n--- Step 2c: Showing just model IDs ---"
lmstrix list --show id
echo "Model listing demonstrations complete."

# Step 3: Test a model's context length
# We'll use a common model identifier that users might have
echo -e "\n--- Step 3: Testing a model's context length ---"
echo "Note: This may take several minutes depending on the model and your hardware."
# Common model patterns users might have:
# "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded
echo "Looking for models matching: $MODEL_ID"
# Show models matching the pattern
lmstrix list --show id | grep -i "$MODEL_ID" || echo "No models found matching '$MODEL_ID'"
echo -e "\nTesting model: $MODEL_ID"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Context test complete."

# Step 4: Run inference with new context control features
# Demonstrates --out_ctx instead of deprecated --max_tokens
echo -e "\n--- Step 4a: Running basic inference ---"
lmstrix infer "What is the capital of France?" "$MODEL_ID" --out_ctx 50

echo -e "\n--- Step 4b: Running inference with specific loading context ---"
lmstrix infer "Explain quantum computing in simple terms." "$MODEL_ID" --in_ctx 4096 --out_ctx 200

echo -e "\n--- Step 4c: Checking if model is already loaded (reuse demo) ---"
lmstrix infer "What is 2+2?" "$MODEL_ID" --out_ctx 10

echo -e "\nInference demonstrations complete."

echo -e "\n### Workflow Demo Finished ###"
</document_content>
</document>

<document index="25">
<source>examples/cli/context_control_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates advanced context control features in LMStrix.
# Shows --in_ctx usage, model state detection, and context optimization.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Context Control Examples ###"

# Replace with a model identifier that matches your downloaded models
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Understanding Context Parameters
echo -e "\n--- Example 1: Understanding Context Parameters ---"
echo "LMStrix has two context parameters:"
echo "  --in_ctx: Controls the context size when LOADING the model"
echo "  --out_ctx: Controls the maximum tokens to GENERATE"
echo -e "\nLet's see them in action..."

# Example 2: Default Context Loading
echo -e "\n--- Example 2: Default Context Loading ---"
echo "When --in_ctx is not specified, LMStrix uses the optimal context:"
lmstrix infer "What is machine learning?" "$MODEL_ID" --out_ctx 50 --verbose
echo "(Model loaded with optimal context based on testing or declared limit)"

# Example 3: Specific Context Loading
echo -e "\n--- Example 3: Loading with Specific Context ---"
echo "Load model with exactly 4096 tokens of context:"
lmstrix infer "Explain neural networks briefly." "$MODEL_ID" --in_ctx 4096 --out_ctx 100 --verbose
echo "(Model loaded with 4096 token context)"

# Example 4: Model State Detection
echo -e "\n--- Example 4: Model State Detection ---"
echo "First inference - model loads:"
lmstrix infer "What is 5 + 5?" "$MODEL_ID" --out_ctx 10
echo -e "\nSecond inference - model already loaded (should be faster):"
lmstrix infer "What is 10 + 10?" "$MODEL_ID" --out_ctx 10
echo "(Notice: Model was reused, not reloaded)"

# Example 5: Context Size Impact
echo -e "\n--- Example 5: Context Size Impact ---"
echo "Smaller context (faster loading, less memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 2048 --out_ctx 10

echo -e "\nLarger context (slower loading, more memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 16384 --out_ctx 10

# Example 6: Zero Context Loading
echo -e "\n--- Example 6: Zero Context Loading (--in_ctx 0) ---"
echo "Load model without specifying context (uses model's default):"
lmstrix infer "What is Python?" "$MODEL_ID" --in_ctx 0 --out_ctx 50
echo "(Model loaded with its default context configuration)"

# Example 7: Context Optimization Strategy
echo -e "\n--- Example 7: Context Optimization Strategy ---"
echo "For best performance:"
echo "1. Test your model first to find optimal context:"
echo "   lmstrix test --model_id $MODEL_ID"
echo -e "\n2. Use the tested context for inference:"
echo "   lmstrix infer 'prompt' $MODEL_ID --in_ctx <tested_value>"
echo -e "\n3. Or let LMStrix choose automatically (omit --in_ctx)"

# Example 8: Long Context Use Case
echo -e "\n--- Example 8: Long Context Use Case ---"
echo "Processing a long document (simulated):"
LONG_PROMPT="Summarize this text: The history of artificial intelligence began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by classical philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols."
echo "Prompt length: ${#LONG_PROMPT} characters"
lmstrix infer "$LONG_PROMPT" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 9: Memory-Conscious Loading
echo -e "\n--- Example 9: Memory-Conscious Loading ---"
echo "For limited memory systems, use smaller context:"
lmstrix infer "What is RAM?" "$MODEL_ID" --in_ctx 2048 --out_ctx 50
echo "(Smaller context = less GPU/RAM usage)"

# Example 10: Comparing Context Sizes
echo -e "\n--- Example 10: Context Size Comparison ---"
echo "Let's compare the same prompt with different contexts:"

echo -e "\nWith 2K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 2048 --out_ctx 100

echo -e "\nWith 8K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 8192 --out_ctx 100

echo -e "\nNote: Larger context doesn't always mean better output for simple prompts"

echo -e "\n### Context Control Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- --in_ctx controls model loading context (memory usage)"
echo "- --out_ctx controls generation length"
echo "- Models are reused when possible for efficiency"
echo "- Optimal context depends on your use case"
echo "- Test models first to find their true limits"
</document_content>
</document>

<document index="26">
<source>examples/cli/inference_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various inference scenarios using LMStrix.
# Shows new features: --in_ctx, --out_ctx, --file_prompt, --dict, --force-reload
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Inference Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Simple Question with new --out_ctx parameter
echo -e "\n--- Example 1: Simple Question with Output Context Control ---"
lmstrix infer "Explain the theory of relativity in simple terms." "$MODEL_ID" --out_ctx 200

# Example 2: Context Control - Load model with specific context size
echo -e "\n--- Example 2: Load Model with Specific Context Size ---"
lmstrix infer "What are the benefits of renewable energy?" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 3: Load model without context specification (--in_ctx 0)
echo -e "\n--- Example 3: Load Model with Default Context ---"
lmstrix infer "What is machine learning?" "$MODEL_ID" --in_ctx 0 --out_ctx 100

# Example 4: Model State Detection - Second call reuses loaded model
echo -e "\n--- Example 4: Model State Detection (Reuse) ---"
echo "First call - loads the model:"
lmstrix infer "What is Python?" "$MODEL_ID" --out_ctx 50
echo -e "\nSecond call - reuses loaded model:"
lmstrix infer "What is JavaScript?" "$MODEL_ID" --out_ctx 50

# Example 5: Force Reload - Reload model even if already loaded
echo -e "\n--- Example 5: Force Model Reload ---"
lmstrix infer "What is artificial intelligence?" "$MODEL_ID" --force-reload --out_ctx 100

# Example 6: Using TOML Prompt Files with Parameters
echo -e "\n--- Example 6: Using Prompt Templates from TOML ---"
# First, let's check if prompts.toml exists
if [ -f "examples/prompts.toml" ]; then
    PROMPT_FILE="examples/prompts.toml"
elif [ -f "prompts.toml" ]; then
    PROMPT_FILE="prompts.toml"
else
    # Create a simple prompts.toml for demonstration
    cat > temp_prompts.toml <<EOL
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"

[code]
review = "Please review this {{language}} code and suggest improvements: {{code}}"
explain = "Explain this {{language}} code in simple terms: {{code}}"
EOL
    PROMPT_FILE="temp_prompts.toml"
fi

echo "Using prompt file: $PROMPT_FILE"
lmstrix infer greetings.formal "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "name=Alice,topic=Python" --out_ctx 100

# Example 7: Multiple Parameters with Different Format
echo -e "\n--- Example 7: Code Review with Template ---"
lmstrix infer code.review "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "language=Python,code=def factorial(n): return 1 if n <= 1 else n * factorial(n-1)" --out_ctx 300

# Example 8: Adjusting Temperature for Creative Output
echo -e "\n--- Example 8: Creative Writing with High Temperature ---"
lmstrix infer "Write a short poem about the sea." "$MODEL_ID" --temperature 1.5 --out_ctx 150

# Example 9: Low Temperature for Deterministic Output
echo -e "\n--- Example 9: Math Problem with Low Temperature ---"
lmstrix infer "Calculate: 25 * 4 + 10 - 5" "$MODEL_ID" --temperature 0.1 --out_ctx 20

# Example 10: Combining Features - Context Control + Template + Temperature
echo -e "\n--- Example 10: Combined Features Demo ---"
lmstrix infer greetings.casual "$MODEL_ID" \
    --file_prompt "$PROMPT_FILE" \
    --dict "name=Bob,topic=quantum computing" \
    --in_ctx 4096 \
    --out_ctx 150 \
    --temperature 0.8

# Cleanup temporary file if created
if [ "$PROMPT_FILE" = "temp_prompts.toml" ]; then
    rm temp_prompts.toml
fi

echo -e "\n### Inference Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- Use --out_ctx instead of deprecated --max_tokens"
echo "- Use --in_ctx to control model loading context size"
echo "- Use --file_prompt with --dict for template-based prompts"
echo "- Models are reused when already loaded (unless --force-reload)"
echo "- Combine features for advanced use cases"
</document_content>
</document>

<document index="27">
<source>examples/cli/model_state_demo.sh</source>
<document_content>
#!/bin/bash
# this_file: examples/cli/model_state_demo.sh
#
# Demonstration of model state persistence in LMStrix
# Shows how models remain loaded across multiple infer calls
#

set -e

echo "### LMStrix Model State Persistence Demo ###"
echo ""
echo "This demo shows how LMStrix now keeps models loaded between calls"
echo "for better performance and resource efficiency."
echo ""

MODEL_ID="llama-3.2-3b-instruct"  # Change this to match your model

# Step 1: First inference with explicit model and context
echo "=== Step 1: First inference with explicit model and context ==="
echo "Command: lmstrix infer \"What is 2+2?\" -m \"$MODEL_ID\" --out_ctx \"25%\" --verbose"
echo "(This will load the model with specified context)"
echo ""
lmstrix infer "What is 2+2?" -m "$MODEL_ID" --out_ctx "25%" --verbose || echo "Failed"

echo ""
echo "=== Step 2: Second inference without in_ctx ==="
echo "Command: lmstrix infer \"What is 3+3?\" -m \"$MODEL_ID\" --verbose"
echo "(This should reuse the already loaded model)"
echo ""
sleep 2
lmstrix infer "What is 3+3?" -m "$MODEL_ID" --verbose || echo "Failed"

echo ""
echo "=== Step 3: Third inference without model_id ==="
echo "Command: lmstrix infer \"What is 4+4?\" --verbose"
echo "(This should use the last-used model: $MODEL_ID)"
echo ""
sleep 2
lmstrix infer "What is 4+4?" --verbose || echo "Failed"

echo ""
echo "=== Demo Complete ==="
echo ""
echo "Key features demonstrated:"
echo "1. Models stay loaded between infer calls when in_ctx is not specified"
echo "2. Last-used model is remembered when -m is not specified"
echo "3. Better performance by avoiding repeated model loading/unloading"
echo ""
echo "Check the verbose output above to see:"
echo "- First call: 'Loading with optimal context...'"
echo "- Second call: 'Model already loaded... reusing...'"
echo "- Third call: 'Using last-used model...'"
</document_content>
</document>

<document index="28">
<source>examples/cli/model_testing.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various context testing scenarios with LMStrix.
# Shows new features: --threshold, --all --ctx, --fast, and test resumption.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Model Testing Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Standard Test with New Fixed Context Strategy
# Tests at fixed context sizes: 30k, 40k, 60k, 80k, 100k, 120k, and declared_max-1
echo -e "\n--- Example 1: Standard Context Test ---"
echo "Testing model '$MODEL_ID' with fixed context strategy"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Standard test complete."

# Example 2: Test with Threshold Limit
# Prevents initial test from exceeding specified threshold (default: 102,400)
echo -e "\n--- Example 2: Test with Custom Threshold ---"
echo "Testing with maximum initial context of 50,000 tokens"
lmstrix test --model_id "$MODEL_ID" --threshold 50000
echo "Threshold test complete."

# Example 3: Fast Mode Testing
# Only checks if model can load and generate, skips semantic verification
echo -e "\n--- Example 3: Fast Mode Test ---"
echo "Running fast test (skip semantic verification)"
lmstrix test --model_id "$MODEL_ID" --fast
echo "Fast test complete."

# Example 4: Test at Specific Context Size
# Tests a model at a specific context size only
echo -e "\n--- Example 4: Test at Specific Context ---"
echo "Testing model at exactly 8192 tokens"
lmstrix test --model_id "$MODEL_ID" --ctx 8192
echo "Specific context test complete."

# Example 5: Batch Test All Models
# Tests all untested models in the registry
echo -e "\n--- Example 5: Batch Test All Models ---"
echo "Testing all models (this may take a while)..."
echo "Note: This will test ALL models. Press Ctrl+C to cancel."
read -p "Press Enter to continue..." -t 5 || true
lmstrix test --all --verbose
echo "Batch test complete."

# Example 6: Batch Test with Context Limit
# Tests all models at a specific context size
echo -e "\n--- Example 6: Batch Test at Specific Context ---"
echo "Testing all untested models at 4096 tokens"
lmstrix test --all --ctx 4096
echo "Batch context test complete."

# Example 7: Fast Batch Testing
# Quick test of all models without semantic verification
echo -e "\n--- Example 7: Fast Batch Test ---"
echo "Running fast test on all models"
lmstrix test --all --fast
echo "Fast batch test complete."

# Example 8: Show Test Results
# Display models sorted by their tested context size
echo -e "\n--- Example 8: View Test Results ---"
echo "Models sorted by tested context size:"
lmstrix list --sort ctx
echo -e "\nModels sorted by efficiency (tested/declared ratio):"
lmstrix list --sort eff

# Example 9: Test with Smart Sorting
# When testing all models, they're sorted for efficiency
echo -e "\n--- Example 9: Smart Sorted Batch Test ---"
echo "Testing all models with smart sorting (smaller models first)"
lmstrix test --all --threshold 30000 --fast
echo "Smart sorted test complete."

# Example 10: Resume Interrupted Test
# If a test is interrupted, it can resume from where it left off
echo -e "\n--- Example 10: Test Resumption Demo ---"
echo "LMStrix automatically saves progress during testing."
echo "If a test is interrupted, simply run the same command again."
echo "The test will resume from the last successful context size."

echo -e "\n### Model Testing Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- New fixed context testing strategy (30k, 40k, 60k, etc.)"
echo "- Use --threshold to limit maximum initial test size"
echo "- Use --fast for quick testing without semantic checks"
echo "- Use --ctx to test at specific context sizes"
echo "- Batch testing with --all supports all options"
echo "- Tests automatically resume if interrupted"
</document_content>
</document>

<document index="29">
<source>examples/data/sample_context.txt</source>
<document_content>
This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

</document_content>
</document>

<document index="30">
<source>examples/data/test_questions.json</source>
<document_content>
[
  {
    "id": "qa_1",
    "question": "What is the primary purpose of the sample_context.txt file?",
    "context_source": "sample_context.txt"
... (file content truncated to first 5 lines)
</document_content>
</document>

<document index="31">
<source>examples/prompts/analysis.toml</source>
<document_content>
# examples/prompts/analysis.toml

[summarize_text]
prompt = "Summarize the key points of the following text in three bullet points:

{text}"
system_prompt = "You are a helpful assistant that specializes in text summarization."
description = "Summarizes a long piece of text."

[sentiment_analysis]
prompt = "Analyze the sentiment of the following review. Respond with 'Positive', 'Negative', or 'Neutral'.

Review: "{review_text}""
system_prompt = "You are a sentiment analysis AI."
description = "Analyzes the sentiment of a given text."
</document_content>
</document>

<document index="32">
<source>examples/prompts/coding.toml</source>
<document_content>
# examples/prompts/coding.toml

[python_function]
prompt = "Write a Python function that does the following: {request}. Include a docstring and type hints."
system_prompt = "You are an expert Python programmer who writes clean, efficient, and well-documented code."
description = "Generates a Python function based on a request."

[explain_code]
prompt = "Explain what this code does in simple terms. Identify potential bugs if any.

Code:
```
{code_snippet}
```"
system_prompt = "You are a senior software engineer who is excellent at code reviews and explaining complex concepts."
description = "Explains a snippet of code."
</document_content>
</document>

<document index="33">
<source>examples/prompts/creative.toml</source>
<document_content>
# examples/prompts/creative.toml

[write_poem]
prompt = "Write a short, four-line poem about {topic}."
system_prompt = "You are a world-class poet."
description = "Writes a short poem on a given topic."

[generate_story_idea]
prompt = "Generate a compelling story idea based on the following genre and character: Genre: {genre}, Character: {character_description}"
system_prompt = "You are a creative writing assistant, skilled in generating unique plot ideas."
description = "Generates a story idea from a genre and character."
</document_content>
</document>

<document index="34">
<source>examples/prompts/qa.toml</source>
<document_content>
# examples/prompts/qa.toml

[simple_question]
prompt = "Answer the following question: {question}"
system_prompt = "You are a helpful and factual question-answering AI."
description = "Answers a direct question."

[contextual_question]
prompt = "Based on the text below, answer the question.

Text: {context}

Question: {question}"
system_prompt = "You are an AI assistant that answers questions based *only* on the provided context."
description = "Answers a question based on a provided context."
</document_content>
</document>

<document index="35">
<source>examples/prompts.toml</source>
<document_content>
# Example prompt templates for LMStrix
# Demonstrates placeholder resolution and template reuse

# Simple greeting prompts
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"
professional = "Hello {{name}}, I'm here to help you with {{topic}}. Please describe your requirements."

# Templates for different use cases
[templates]
base = "You are an expert assistant specializing in {{domain}}."
instruction = """{{templates.base}}

Please provide a {{style}} explanation of {{concept}}.
Your response should be appropriate for someone with {{level}} knowledge."""

# Code-related prompts
[code]
review = """You are a code reviewer. Please review the following {{language}} code:

{{code}}

Focus on:
1. Code quality and best practices
2. Potential bugs or issues
3. Performance considerations
4. Security concerns"""

explain = "Explain this {{language}} code in simple terms: {{code}}"

# Research prompts
[research]
summarize = """Please summarize the following text about {{topic}}:

{{text}}

Provide a {{length}} summary focusing on the key points."""

analyze = """Analyze the following information about {{subject}}:

{{content}}

Consider the following aspects:
- {{aspect1}}
- {{aspect2}}
- {{aspect3}}"""

# Math and science prompts
[math]
solve = "Solve this {{difficulty}} math problem step by step: {{problem}}"
explain_concept = "Explain the concept of {{concept}} in {{field}} using {{approach}} approach."

# Creative writing prompts
[creative]
story = "Write a {{genre}} story about {{character}} who {{situation}}. The story should be {{length}} and include {{elements}}."
poem = "Create a {{style}} poem about {{theme}} that incorporates {{imagery}}."

# Question answering
[qa]
simple = "{{question}}"
detailed = """Question: {{question}}

Please provide a comprehensive answer that includes:
- Direct answer
- Explanation
- Examples where relevant
- Any important caveats or exceptions"""

# System prompts
[system]
assistant = """You are a helpful AI assistant. Your traits:
- {{personality}}
- Knowledge level: {{expertise}}
- Communication style: {{style}}
- Primary language: {{language}}"""

chatbot = """You are {{bot_name}}, a {{bot_type}} chatbot.
Your purpose: {{purpose}}
Your tone: {{tone}}
Special instructions: {{instructions}}"""
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/python/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py
# Language: python

from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates advanced context testing with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py
# Language: python

from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError, ModelNotFoundError
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates basic usage of the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py
# Language: python

import time
from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates batch processing of multiple models for testing or inference."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py
# Language: python

from pathlib import Path
from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates custom inference workflows with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py
# Language: python

from pathlib import Path
import toml
from lmstrix import LMStrix
from lmstrix.core.prompts import PromptResolver
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates prompt template features."""


<document index="36">
<source>examples/run_all_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script runs all the examples in the `cli` and `python` directories.
# It's a way to functionally test that all example code is working as expected.
#
# NOTE: This script assumes you have a model downloaded in LM Studio
# and that the identifier 'ultron-summarizer-1b' will match it. If not, please edit the
# example files to use an identifier for a model you have.
#

set -e # Exit on any error

echo "===== Running All LMStrix Examples ====="

# --- Running CLI Examples ---
echo -e "

--- Testing CLI Examples ---"
echo "NOTE: The CLI scripts have a placeholder model identifier ('ultron-summarizer-1b')."
echo "Please edit them if you don't have a model matching that ID."

echo -e "
>>> Running basic_workflow.sh"
bash "$(dirname "$0")/cli/basic_workflow.sh"

echo -e "
>>> Running model_testing.sh"
bash "$(dirname "$0")/cli/model_testing.sh"

echo -e "
>>> Running inference_examples.sh"
bash "$(dirname "$0")/cli/inference_examples.sh"

echo -e "
--- CLI Examples Complete ---"


# --- Running Python Examples ---
echo -e "

--- Testing Python Examples ---"
echo "NOTE: The Python scripts will use the first model they find."

echo -e "
>>> Running basic_usage.py"
python3 "$(dirname "$0")/python/basic_usage.py"

echo -e "
>>> Running advanced_testing.py"
python3 "$(dirname "$0")/python/advanced_testing.py"

echo -e "
>>> Running custom_inference.py"
python3 "$(dirname "$0")/python/custom_inference.py"

echo -e "
>>> Running batch_processing.py"
python3 "$(dirname "$0")/python/batch_processing.py"

echo -e "
--- Python Examples Complete ---"


echo -e "

===== All Examples Finished Successfully ====="
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py
# Language: python

import sys
from pathlib import Path
import fire
from rich.console import Console
from slugify import slugify
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.utils.logging import logger

def process_text_with_model((
    input_file: str,
    model_id: str | None = None,
    verbose: bool = False,
)) -> str:
    """ Process a text file by sending paragraphs to a model for number-to-words conversion...."""

def main(()) -> None:
    """Main entry point using Fire CLI."""


<document index="37">
<source>issues/301.txt</source>
<document_content>
```
$ ./adam2.sh
15:41:35 | INFO | lmstrix.loaders.prompt_loader:load_single_prompt:128 - Loaded prompts from adam.toml
15:41:35 | WARNING | lmstrix.loaders.prompt_loader:load_single_prompt:143 - TOPL placeholder resolution failed: Format string contains positional fields
15:41:35 | DEBUG | lmstrix.core.prompts:resolve_prompt:186 - Internal resolution complete after 2 passes
15:41:35 | INFO | lmstrix.loaders.prompt_loader:load_single_prompt:153 - Resolved prompt 'translate'
Loaded prompt 'translate' from adam.toml
Resolved placeholders: text, think
15:41:35 | DEBUG | lmstrix.utils.paths:get_lmstudio_path:26 - Found LM Studio at /Users/Shared/lmstudio
15:41:35 | DEBUG | lmstrix.utils.paths:get_lmstudio_path:26 - Found LM Studio at /Users/Shared/lmstudio
15:41:35 | INFO | lmstrix.loaders.model_loader:load_model_registry:43 - Read 75 models from /Users/Shared/lmstudio/lmstrix.json
15:41:35 | INFO | lmstrix.core.inference_manager:infer:92 - Explicit context specified: 75%
15:41:35 | DEBUG | lmstrix.api.client:load_model_by_id:104 - Found model match: qwen/qwen3-4b -> qwen/qwen3-4b
15:41:35 | ERROR | lmstrix.core.inference_manager:infer:188 - Inference failed for model qwen/qwen3-4b: Failed to load model 'qwen/qwen3-4b': Failed to load model 'qwen/qwen3-4b'. Available models: ['google/gemma-3n-e4b', 'microsoft/phi-4-mini-reasoning', 'deepseek-r1-0528-qwen3-8b', 'magistral-small-2507-rebased-vision-i1', 'huihui-qwen3-14b-abliterated-v2']
Inference failed: Failed to load model 'qwen/qwen3-4b': Failed to load model 'qwen/qwen3-4b'. Available models: ['google/gemma-3n-e4b',
'microsoft/phi-4-mini-reasoning', 'deepseek-r1-0528-qwen3-8b', 'magistral-small-2507-rebased-vision-i1', 'huihui-qwen3-14b-abliterated-v2']
```

but our lmstrix.json database has

```
"qwen/qwen3-4b": {
"id": "qwen/qwen3-4b",
"short_id": "qwen3-4b",
"path": "qwen/qwen3-4b",
"size_bytes": 4280583359,
"ctx_in": 32768,
"ctx_out": 4096,
"has_tools": false,
"has_vision": false,
"tested_max_context": 32768,
"loadable_max_context": null,
"last_known_good_context": 32768,
"last_known_bad_context": null,
"context_test_status": "completed",
"context_test_log": "/Users/Shared/lmstudio/lmstrix/context_tests/qwen_qwen3-4b_context_test.log",
"context_test_date": "2025-07-28 01:18:12.706392",
"failed": false,
"error_msg": ""
},
```

</document_content>
</document>

<document index="38">
<source>issues/302.txt</source>
<document_content>
I manually run this prompt in LM Studio with the model `llama-3.2-3b-instruct`: 


```
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given line-per-line line `input term = output termâ€”alternative output term â‰  avoided translation`, where `=` means: translate like that, `|` means: or translate like this depending on the context `â‰ ` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font â‰  czcionka
variable font = font zmienny â‰  czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe â‰  czcionki webowe
small caps = kapitaliki â‰  maÅ‚e kapitaliki
double-click = dwuklikâ€”dwukliknÄ…Ä‡ â‰  podwÃ³jne klikniÄ™cie
oldstyle numerals = cyfry nautyczne
typeface = krÃ³j pisma | krÃ³j
stem = kreska gÅ‚Ã³wna
hint = hint | hintowaÄ‡ â‰  wskazÃ³wka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days, and start making fonts today!
</input>
```

and I got this: 

```
Oto przekÅ‚ad:

FontLab 8 jest zaawansowanym edytorem pism do komputera Mac i Windows, ktÃ³ry pomaga Ci stworzyÄ‡ pisma od podstaw aÅ¼ do koÅ„ca, od prostego projektu aÅ¼ do zÅ‚oÅ¼onej pracy, i dodaje czapkÄ™ magicznoÅ›ci w designie pism. ProszÄ™ o FontLab 8 za darmo przez 10 dni i zacznij tworzenie pism dzisiaj!

WysÅ‚ugiwaÅ‚oby siÄ™ taki przekÅ‚ad:

FontLab 8 jest zaawansowanym edytorem pism do komputera Mac i Windows, ktÃ³ry pomaga Ci stworzyÄ‡ pisma od podstaw aÅ¼ do koÅ„ca, od prostego projektu aÅ¼ do zÅ‚oÅ¼onej pracy, i dodaje czapkÄ™ magicznoÅ›ci w designie pism. | ProszÄ™ o FontLab 8 za darmo przez 10 dni i zacznij tworzyÄ‡ pisma dzisiaj!
ProszÄ™ oFontLab 8 za darmoâ€”wskazÃ³wka â‰  proszÄ™ o FontLab 8
W powyÅ¼szym przypadku uÅ¼yÅ‚ siÄ™ "proszÄ™" zamiast "o" w wyrazie, co jest bardziej przyzwoite dla polskiego jÄ™zyka.

Oto druga wersja przekÅ‚adu bez "wskazÃ³wki":

FontLab 8 to zaawansowany editor pism do komputera Mac i Windows, ktÃ³ry pomaga Ci stworzyÄ‡ pisma od podstaw aÅ¼ do koÅ„ca, od prostego projektu aÅ¼ do zÅ‚oÅ¼onej pracy, i dodaje czapkÄ™ magicznoÅ›ci w designie pism. | Try FontLab 8 za darmo przez 10 dni i zacznij tworzyÄ‡ pisma dzisiaj!
Try FontLab 8 za darmoâ€”hint â‰  podwÃ³jne klikniÄ™cie
W tym przypadku uÅ¼yÅ‚ siÄ™ sÅ‚owa "try" zamiast "wskazÃ³wki", co jest bardziej naturalne dla polskiego jÄ™zyka.
ProszÄ™ o fonty webowe do tworzenia pism â€” niektÃ³re z nich mogÄ… byÄ‡ wykorzystywane w FontLab 8â€”fonty webowe â‰  fonty komputerowe
W tym przypadku uÅ¼yÅ‚ siÄ™ "fonty webowe" zamiast "fontÃ³w komputerowych", co jest bardziej praktyczne dla polskiego jÄ™zyka.
Uwagi:
- PrzekÅ‚ad zastosowaÅ‚ odpowiedni typografia i sÅ‚owa, ktÃ³re sÄ… okreÅ›lone w sÅ‚owniku.
- UÅ¼yto skrÃ³tu i przeniesienia sÅ‚Ã³w, aby utrzymaÄ‡ czytelnoÅ›Ä‡ i pÅ‚ynny przekÅ‚ad.
- PrzekÅ‚ad miaÅ‚ na celu utrzymanie zgodnoÅ›ci z oczekiwanimi sÅ‚ownika.
```

the the log was: 

```

Developer Logs
2025-07-31 18:06:10 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 20, top_p = 0.950, min_p = 0.000, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:06:10 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 16384, n_batch = 512, n_predict = -1, n_keep = 356
2025-07-31 18:06:10 [DEBUG]
 Looking for non-prefix contiguous prompt sequences of size >= 256 to reuse from cache
2025-07-31 18:06:10 [DEBUG]
 Cache reuse summary: 0/356 of prompt (0%), 0 prefix, 0 non-prefix
2025-07-31 18:06:10 [DEBUG]
 Total prompt tokens: 356
Prompt tokens to decode: 356
BeginProcessingPrompt
2025-07-31 18:06:12 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:06:45 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1715.77 ms
llama_perf_context_print: prompt eval time =    1684.12 ms /   356 tokens (    4.73 ms per token,   211.39 tokens per second)
llama_perf_context_print:        eval time =   31729.07 ms /   465 runs   (   68.23 ms per token,    14.66 tokens per second)
llama_perf_context_print:       total time =   34031.51 ms /   821 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:06:45 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 20, top_p = 0.950, min_p = 0.000, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:06:45 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 16384, n_batch = 512, n_predict = 30, n_keep = 356
2025-07-31 18:06:45 [DEBUG]
 Total prompt tokens: 891
Prompt tokens to decode: 70
BeginProcessingPrompt
2025-07-31 18:06:45 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:06:46 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1715.77 ms
llama_perf_context_print: prompt eval time =     612.92 ms /    70 tokens (    8.76 ms per token,   114.21 tokens per second)
llama_perf_context_print:        eval time =    1143.03 ms /    14 runs   (   81.64 ms per token,    12.25 tokens per second)
llama_perf_context_print:       total time =    1809.70 ms /    84 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] GPU Configuration:
  Strategy: evenly
  Priority: []
  Disabled GPUs: []
  Limit weight offload to dedicated GPU Memory: OFF
  Offload KV Cache to GPU: ON
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Live GPU memory info:
No live GPU info available
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Model load size estimate with raw num offload layers 'max' and context length '131072':
  Model: 1.72 GB
  Context: 21.56 GB
  Total: 23.28 GB
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Not using full context length for VRAM overflow calculations due to single GPU setup. Instead, using '8192' as context length for the calculation. Original context length: '131072'.
[LM Studio] Strict GPU VRAM cap is OFF: GPU offload layers will not be checked for adjustment
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Resolved GPU config options:
  Num Offload Layers: max
  Main GPU: 0
  Tensor Split: [0]
  Disabled GPUs: []
2025-07-31 18:07:06 [DEBUG]
 Metal : CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |
2025-07-31 18:07:06 [DEBUG]
 llama_model_load_from_file_impl: using device Metal (Apple M2) - 18186 MiB free
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /Users/Shared/lmstudio/models/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = llama3.2
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 14
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["Ä  Ä ", "Ä  Ä Ä Ä ", "Ä Ä  Ä Ä ", "...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...
llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  189 tensors
llama_model_loader: - type q5_K:    7 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Small
print_info: file size   = 1.79 GiB (4.78 BPW)
2025-07-31 18:07:06 [DEBUG]
 load: special tokens cache size = 256
2025-07-31 18:07:06 [DEBUG]
 load: token to piece cache size = 0.7999 MB
2025-07-31 18:07:06 [DEBUG]
 print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
2025-07-31 18:07:07 [DEBUG]
 load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: Metal_Mapped model buffer size =  1831.41 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
2025-07-31 18:07:07 [DEBUG]
 llama_context: constructing llama_context
llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_per_seq = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 1
llama_context: kv_unified    = true
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2
ggml_metal_init: picking default device: Apple M2
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_load_library: loading '/Users/Shared/lmstudio/extensions/backends/llama.cpp-mac-arm64-apple-metal-advsimd-1.42.0/default.metallib'
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: GPU name:   Apple M2
ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 19069.67 MB
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 llama_context:        CPU  output buffer size =     0.49 MiB
2025-07-31 18:07:07 [DEBUG]
 llama_kv_cache_unified:      Metal KV buffer size = 14336.00 MiB
2025-07-31 18:07:09 [DEBUG]
 llama_kv_cache_unified: size = 14336.00 MiB (131072 cells,  28 layers,  1/ 1 seqs), K (f16): 7168.00 MiB, V (f16): 7168.00 MiB
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
2025-07-31 18:07:09 [DEBUG]
 llama_context:      Metal compute buffer size =   408.00 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 931
llama_context: graph splits = 2
2025-07-31 18:07:09 [DEBUG]
 common_init_from_params: added <|end_of_text|> logit bias = -inf
common_init_from_params: added <|eom_id|> logit bias = -inf
common_init_from_params: added <|eot_id|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 131072
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-07-31 18:07:13 [DEBUG]
 GgmlThreadpools: llama threadpool init = n_threads = 6
2025-07-31 18:07:30 [DEBUG]
 Sampling params:
2025-07-31 18:07:30 [DEBUG]
 repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:07:30 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 131072, n_batch = 512, n_predict = -1, n_keep = 390
2025-07-31 18:07:30 [DEBUG]
 Total prompt tokens: 390
Prompt tokens to decode: 390
2025-07-31 18:07:30 [DEBUG]
 BeginProcessingPrompt
2025-07-31 18:07:31 [DEBUG]
 FinishedProcessingPrompt. Progress:
2025-07-31 18:07:31 [DEBUG]
 100
2025-07-31 18:07:56 [DEBUG]
 Target model llama_perf stats:
2025-07-31 18:07:56 [DEBUG]
 llama_perf_context_print:        load time =    7680.27 ms
llama_perf_context_print: prompt eval time =    1715.59 ms /   390 tokens (    4.40 ms per token,   227.33 tokens per second)
llama_perf_context_print:        eval time =   23627.92 ms /   639 runs   (   36.98 ms per token,    27.04 tokens per second)
llama_perf_context_print:       total time =   26104.33 ms /  1029 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:07:56 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:07:56 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 131072, n_batch = 512, n_predict = 30, n_keep = 390
2025-07-31 18:07:56 [DEBUG]
 Total prompt tokens: 1099
Prompt tokens to decode: 70
2025-07-31 18:07:56 [DEBUG]
 BeginProcessingPrompt
2025-07-31 18:07:56 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:07:56 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    7680.27 ms
llama_perf_context_print: prompt eval time =     419.37 ms /    70 tokens (    5.99 ms per token,   166.92 tokens per second)
llama_perf_context_print:        eval time =     318.05 ms /     9 runs   (   35.34 ms per token,    28.30 tokens per second)
llama_perf_context_print:       total time =     746.49 ms /    79 tokens
llama_perf_context_print:    graphs reused =          0
```


----

Then I run `./adam3.sh` which is `lmstrix infer translate --verbose -m llama-3.2-3b-instruct --file_prompt adam.toml --text_file fontlab8a.md --in_ctx 50%`


and in Terminal I get

```
Othello:adam adam$ ./adam3.sh
[I] Loaded prompts from adam.toml lmstrix.loaders.prompt_loader:load_single_prompt:128
[I] Resolved placeholders with TOPL lmstrix.loaders.prompt_loader:load_single_prompt:141
[D] Internal resolution complete after 1 passes lmstrix.core.prompts:resolve_prompt:185
[I] Resolved prompt 'translate' lmstrix.loaders.prompt_loader:load_single_prompt:153
[D] Loaded prompt 'translate' from adam.toml lmstrix.cli.main:infer:683
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[I] Read 75 models from /Users/Shared/lmstudio/lmstrix.json lmstrix.loaders.model_loader:load_model_registry:42
[D] Parsed out_ctx '50%' as 50.0% of 131072 = 65536 tokens lmstrix.utils.context_parser:parse_out_ctx:60
[D] Parsed in_ctx '50%' as 65536 tokens lmstrix.cli.main:infer:755
[I] Explicit context specified: 65536 lmstrix.core.inference_manager:infer:92
[D] Found model match: llama-3.2-3b-instruct -> llama-3.2-3b-instruct lmstrix.api.client:load_model_by_id:105
â ¦ Running inference on llama-3.2-3b-instruct...[I] Running inference on model llama-3.2-3b-instruct lmstrix.core.inference_manager:infer:163
[D] Using 90% of context length (131072) = 117964 tokens as default maxTokens lmstrix.api.client:completion:204
[I] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• lmstrix.api.client:completion:214
[I] ğŸ¤– MODEL: llama-3.2-3b-instruct lmstrix.api.client:completion:215
[I] ğŸ”§ CONFIG: maxTokens=117964, temperature=0.7 lmstrix.api.client:completion:216
[I] ğŸ“ Prompt (29 lines, 1452 chars): lmstrix.api.client:completion:236
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given
line-per-line line `input term = output termâ€”alternative output term â‰  avoided translation`, where `=` means: translate
like that, `|` means: or translate like this depending on the context `â‰ ` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical
jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font â‰  czcionka
variable font = font zmienny â‰  czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe â‰  czcionki webowe
small caps = kapitaliki â‰  maÅ‚e kapitaliki
double-click = dwuklikâ€”dwukliknÄ…Ä‡ â‰  podwÃ³jne klikniÄ™cie
oldstyle numerals = cyfry nautyczne
typeface = krÃ³j pisma | krÃ³j
stem = kreska gÅ‚Ã³wna
hint = hint | hintowaÄ‡ â‰  wskazÃ³wka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a
simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days,
and start making fonts today!
</input>

â § Running inference on llama-3.2-3b-instruct...[I] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• lmstrix.api.client:completion:240
â ‹ Running inference on llama-3.2-3b-instruct...[I] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• lmstrix.api.client:completion:256
[I] ğŸ“Š INFERENCE STATS lmstrix.api.client:completion:257
[I] âš¡ Time to first token: 1.04s lmstrix.api.client:completion:261
[I] â±ï¸  Total inference time: 1.13s lmstrix.api.client:completion:264
[I] ğŸ”¢ Predicted tokens: 4 lmstrix.api.client:completion:268
[I] ğŸ“ Prompt tokens: 355 lmstrix.api.client:completion:271
[I] ğŸ¯ Total tokens: 359 lmstrix.api.client:completion:274
[I] ğŸš€ Tokens/second: 49.67 lmstrix.api.client:completion:278
[I] ğŸ›‘ Stop reason: eosFound lmstrix.api.client:completion:282
[I] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• lmstrix.api.client:completion:284

Model Response:
</translation>
Othello:adam adam$
```

and in the LMStudio log: 

```

Developer Logs
2025-07-31 18:24:27 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4] Client created.
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listLoaded] Listing loaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listLoaded] Listing loaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listDownloadedModels] Listing downloaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-31 18:24:27 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=getOrLoad] Model not found by identifier. Trying to load.
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] GPU Configuration:
  Strategy: evenly
  Priority: []
  Disabled GPUs: []
  Limit weight offload to dedicated GPU Memory: OFF
  Offload KV Cache to GPU: ON
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] Live GPU memory info:
No live GPU info available
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] Model load size estimate with raw num offload layers 'max' and context length '65536':
  Model: 1.72 GB
  Context: 10.79 GB
  Total: 12.51 GB
[LM Studio] Not using full context length for VRAM overflow calculations due to single GPU setup. Instead, using '8192' as context length for the calculation. Original context length: '65536'.
[LM Studio] Strict GPU VRAM cap is OFF: GPU offload layers will not be checked for adjustment
[LM Studio] Resolved GPU config options:
  Num Offload Layers: max
  Main GPU: 0
  Tensor Split: [0]
  Disabled GPUs: []
2025-07-31 18:24:27 [DEBUG]
 Metal : CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |
2025-07-31 18:24:27 [DEBUG]
 llama_model_load_from_file_impl: using device Metal (Apple M2) - 18186 MiB free
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /Users/Shared/lmstudio/models/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = llama3.2
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 14
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["Ä  Ä ", "Ä  Ä Ä Ä ", "Ä Ä  Ä Ä ", "...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...
llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  189 tensors
llama_model_loader: - type q5_K:    7 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Small
print_info: file size   = 1.79 GiB (4.78 BPW)
2025-07-31 18:24:27 [DEBUG]
 load: special tokens cache size = 256
2025-07-31 18:24:27 [DEBUG]
 load: token to piece cache size = 0.7999 MB
2025-07-31 18:24:27 [DEBUG]
 print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'ÄŠ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
2025-07-31 18:24:27 [DEBUG]
 load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: Metal_Mapped model buffer size =  1831.41 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
2025-07-31 18:24:27 [DEBUG]
 llama_context: constructing llama_context
llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 65536
llama_context: n_ctx_per_seq = 65536
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 1
llama_context: kv_unified    = true
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (65536) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2
ggml_metal_init: picking default device: Apple M2
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_load_library: loading '/Users/Shared/lmstudio/extensions/backends/llama.cpp-mac-arm64-apple-metal-advsimd-1.42.0/default.metallib'
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: GPU name:   Apple M2
ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 19069.67 MB
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 llama_context:        CPU  output buffer size =     0.49 MiB
2025-07-31 18:24:27 [DEBUG]
 llama_kv_cache_unified:      Metal KV buffer size =  7168.00 MiB
2025-07-31 18:24:28 [DEBUG]
 llama_kv_cache_unified: size = 7168.00 MiB ( 65536 cells,  28 layers,  1/ 1 seqs), K (f16): 3584.00 MiB, V (f16): 3584.00 MiB
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
2025-07-31 18:24:28 [DEBUG]
 llama_context:      Metal compute buffer size =   256.50 MiB
llama_context:        CPU compute buffer size =   134.01 MiB
llama_context: graph nodes  = 931
llama_context: graph splits = 2
2025-07-31 18:24:28 [DEBUG]
 common_init_from_params: added <|end_of_text|> logit bias = -inf
common_init_from_params: added <|eom_id|> logit bias = -inf
common_init_from_params: added <|eot_id|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 65536
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-07-31 18:24:28 [DEBUG]
 GgmlThreadpools: llama threadpool init = n_threads = 6
2025-07-31 18:24:28 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:24:28 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 65536, n_batch = 512, n_predict = 117964, n_keep = 355
2025-07-31 18:24:28 [DEBUG]
 Total prompt tokens: 355
Prompt tokens to decode: 355
BeginProcessingPrompt
2025-07-31 18:24:29 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:24:29 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1025.51 ms
llama_perf_context_print: prompt eval time =    1033.97 ms /   355 tokens (    2.91 ms per token,   343.34 tokens per second)
llama_perf_context_print:        eval time =      79.33 ms /     3 runs   (   26.44 ms per token,    37.82 tokens per second)
2025-07-31 18:24:29 [DEBUG]
 llama_perf_context_print:       total time =    1116.33 ms /   358 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:24:29  [INFO]
 Client disconnected: Error: read ECONNRESET
2025-07-31 18:24:29 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4] Client disconnected.
```

</document_content>
</document>

<document index="39">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.11"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.7.0",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml-topl>=1.0.5",
  "tomli>=2.0.1; python_version < '3.11'",
  "hydra-core",
  "omegaconf",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.__main__:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
  "setuptools>=68.0.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "-v",
  "--tb=short",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
  "ignore:Benchmarks are automatically disabled",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]


[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

from importlib.metadata import PackageNotFoundError, version
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix._version import __version__

class LMStrix:
    """High-level interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> list[Model]:
        """Scan for LM Studio models and update the registry."""
    def list_models((self)) -> list[Model]:
        """List all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Test a model's true operational context limits."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> list[Model]:
    """Scan for LM Studio models and update the registry."""

def list_models((self)) -> list[Model]:
    """List all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Test a model's true operational context limits."""

def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

import sys
import fire
from lmstrix.api.main import LMStrixService

class LMStrixCLI:
    """A thin CLI wrapper for LMStrix commands."""
    def __init__((self)) -> None:
        """Initialize the CLI with the service layer."""
    def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
        """List all models from the registry with their test status."""
    def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
    )) -> None:
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def health((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""

def __init__((self)) -> None:
    """Initialize the CLI with the service layer."""

def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
    """Scan for LM Studio models and update the local registry."""

def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
    """List all models from the registry with their test status."""

def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
    )) -> None:
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def health((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def main(()) -> None:
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

import sys
from typing import Any
import lmstudio
from lmstudio import LMStudioServerError
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError
from lmstrix.utils.logging import logger
import time

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
        """Load a model with a specific context length using model path."""
    def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length using model ID."""
    def get_loaded_models((self)) -> list[dict[str, Any]]:
        """Get information about currently loaded models."""
    def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
        """Check if a specific model is currently loaded."""
    def unload_all_models((self)) -> None:
        """Unload all currently loaded models to free up resources."""
    def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
        """Make a completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
    """Load a model with a specific context length using model path."""

def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length using model ID."""

def get_loaded_models((self)) -> list[dict[str, Any]]:
    """Get information about currently loaded models."""

def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
    """Check if a specific model is currently loaded."""

def unload_all_models((self)) -> None:
    """Unload all currently loaded models to free up resources."""

def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
    """Make a completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class LMStudioInstallationNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when the LM Studio installation path cannot be found."""
    def __init__((self)) -> None:
        """Initialize the exception."""

class ValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when data validation fails."""
    def __init__((self, field: str, value: any, reason: str)) -> None:
        """Initialize the exception."""

class InvalidContextLimitError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when context limit is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class InvalidModelSizeError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when model size is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class RegistryValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when registry validation fails."""
    def __init__((self, reason: str)) -> None:
        """Initialize the exception."""

class InvalidModelError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails integrity check."""
    def __init__((self, model_id: str)) -> None:
        """Initialize the exception."""

class InvalidModelCountError(R, e, g, i, s, t, r, y, V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when registry contains invalid models."""
    def __init__((self, count: int)) -> None:
        """Initialize the exception."""

class ModelRegistryError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's an error with the model registry."""

def __init__((self, message: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self)) -> None:
    """Initialize the exception."""

def __init__((self, field: str, value: any, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str)) -> None:
    """Initialize the exception."""

def __init__((self, count: int)) -> None:
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py
# Language: python

import json
import sys
import time
from datetime import datetime
from pathlib import Path
from rich.console import Console
from rich.table import Table
from lmstrix.api.exceptions import APIConnectionError, ModelRegistryError
from lmstrix.core.concrete_config import ConcreteConfigManager
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    scan_and_update_registry,
)
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils import get_context_test_log_path, setup_logging
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file, get_lmstudio_path
from lmstrix.utils.state import StateManager

class LMStrixService:
    """Service layer for LMStrix operations."""
    def scan_models((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list_models((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
        """List all models from the registry with their test status."""
    def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
    )) -> None:
        """Test the context limits for models."""
    def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def check_health((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""
    def show_help((self)) -> None:
        """Show comprehensive help text."""

def _get_models_to_test((
    registry: ModelRegistry,
    test_all: bool,
    ctx: int | None,
    model_id: str | None,
    reset: bool = False,
    fast_mode: bool = False,
)) -> list[Model]:
    """Filter and return a list of models to be tested."""

def _sort_models((models: list[Model], sort_by: str)) -> list[Model]:
    """Sort a list of models based on a given key."""

def _test_single_model((
    tester: ContextTester,
    model: Model,
    ctx: int,
    registry: ModelRegistry,
)) -> None:
    """Test a single model at a specific context size."""

def _test_all_models_at_ctx((
    tester: ContextTester,
    models_to_test: list[Model],
    ctx: int,
    registry: ModelRegistry,
)) -> list[Model]:
    """Test all models at a specific context size."""

def _test_all_models_optimized((
    tester: ContextTester,
    models_to_test: list[Model],
    threshold: int,
    registry: ModelRegistry,
)) -> list[Model]:
    """Run optimized batch testing for multiple models."""

def _print_final_results((updated_models: list[Model])) -> None:
    """Print the final results table."""

def scan_models((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
    """Scan for LM Studio models and update the local registry."""

def list_models((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
    """List all models from the registry with their test status."""

def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
    )) -> None:
    """Test the context limits for models."""

def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def check_health((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def show_help((self)) -> None:
    """Show comprehensive help text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py
# Language: python

import json
from pathlib import Path
from typing import Any
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class ConcreteConfigManager:
    """Manages LM Studio concrete model configurations."""
    def __init__((self, lms_path: Path)) -> None:
        """Initialize the concrete config manager."""
    def _get_config_path((self, model: Model)) -> Path:
        """Get the path for a model's concrete config file."""
    def _create_skeleton_config((self)) -> dict[str, Any]:
        """Create the skeleton structure for a concrete config."""
    def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
        """Update or add a field in the fields list."""
    def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
        """Save a model's tested context limit to its concrete config."""
    def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
        """Save concrete configs for all models with tested contexts."""

def __init__((self, lms_path: Path)) -> None:
    """Initialize the concrete config manager."""

def _get_config_path((self, model: Model)) -> Path:
    """Get the path for a model's concrete config file."""

def _create_skeleton_config((self)) -> dict[str, Any]:
    """Create the skeleton structure for a concrete config."""

def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
    """Update or add a field in the fields list."""

def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
    """Save a model's tested context limit to its concrete config."""

def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
    """Save concrete configs for all models with tested contexts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

from datetime import datetime
from typing import ClassVar
from lmstrix.api import LMStudioClient

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self)) -> bool:
        """Check if we got any response at all (not validating content)."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
    )) -> None:
        """Initialize context tester."""
    def _test_at_context((
        self,
        model_id: str,
        context_size: int,
    )) -> ContextTestResult:
        """Test a model at a specific context size."""
    def test_model((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
        """Test a model to find its maximum working context."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self)) -> bool:
    """Check if we got any response at all (not validating content)."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
    )) -> None:
    """Initialize context tester."""

def _test_at_context((
        self,
        model_id: str,
        context_size: int,
    )) -> ContextTestResult:
    """Test a model at a specific context size."""

def test_model((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
    """Test a model to find its maximum working context."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
import lmstudio
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the inference engine."""
    def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""
    def _find_working_context((self, model_id: str, initial_context: int)) -> int:
        """Find the maximum working context length for a model."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model with automatic context optimization."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the inference engine."""

def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""

def _find_working_context((self, model_id: str, initial_context: int)) -> int:
    """Find the maximum working context length for a model."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model with automatic context optimization."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py
# Language: python

import builtins
import contextlib
import time
from typing import Any
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.logging import logger
import lmstudio
import lmstudio
import lmstudio

class InferenceManager:
    """Unified manager for model inference operations."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the inference manager."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
        """Run inference on a model."""
    def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the inference manager."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
    """Run inference on a model."""

def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from lmstrix.utils.logging import logger
import re
from lmstrix.utils.paths import get_default_models_file

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model:
    """Represents a model in the registry."""
    def __init__((
        self,
        id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
        """Initialize a model with essential fields only."""
    def to_dict((self)) -> dict[str, Any]:
        """Convert model to dictionary for JSON storage."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Alias for to_dict() for backward compatibility."""
    def reset_test_data((self)) -> None:
        """Reset all context testing data."""
    def validate_integrity((self)) -> bool:
        """Validate the model's integrity."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID for filenames."""

class ModelRegistryError(E, x, c, e, p, t, i, o, n):
    """Exception raised for model registry errors."""

class ModelRegistry:
    """Simplified model registry without complex validation."""
    def __init__((self, models_file: Path | None = None)) -> None:
        """Initialize the model registry."""
    def _get_default_models_file((self)) -> Path:
        """Get the default models file path."""
    def load((self)) -> None:
        """Load models from JSON file."""
    def save((self)) -> None:
        """Save models to JSON file."""
    def add_model((self, model: Model)) -> None:
        """Add a model to the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry (adds if not exists for compatibility)."""
    def update_model_by_id((self, model: Model)) -> None:
        """Update a model using its own ID."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def find_model((self, model_identifier: str)) -> Model | None:
        """Find a model by ID or path."""
    def list_models((self)) -> list[Model]:
        """List all models in the registry."""
    def remove_model((self, model_id: str)) -> bool:
        """Remove a model from the registry."""
    def clear((self)) -> None:
        """Clear all models from the registry."""
    def __len__((self)) -> int:
        """Return the number of models."""
    def __contains__((self, model_id: str)) -> bool:
        """Check if a model exists."""

def __init__((
        self,
        id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
    """Initialize a model with essential fields only."""

def to_dict((self)) -> dict[str, Any]:
    """Convert model to dictionary for JSON storage."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Alias for to_dict() for backward compatibility."""

def reset_test_data((self)) -> None:
    """Reset all context testing data."""

def validate_integrity((self)) -> bool:
    """Validate the model's integrity."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID for filenames."""

def __init__((self, models_file: Path | None = None)) -> None:
    """Initialize the model registry."""

def _get_default_models_file((self)) -> Path:
    """Get the default models file path."""

def load((self)) -> None:
    """Load models from JSON file."""

def save((self)) -> None:
    """Save models to JSON file."""

def add_model((self, model: Model)) -> None:
    """Add a model to the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry (adds if not exists for compatibility)."""

def update_model_by_id((self, model: Model)) -> None:
    """Update a model using its own ID."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def find_model((self, model_identifier: str)) -> Model | None:
    """Find a model by ID or path."""

def list_models((self)) -> list[Model]:
    """List all models in the registry."""

def remove_model((self, model_id: str)) -> bool:
    """Remove a model from the registry."""

def clear((self)) -> None:
    """Clear all models from the registry."""

def __len__((self)) -> int:
    """Return the number of models."""

def __contains__((self, model_id: str)) -> bool:
    """Check if a model exists."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""
    def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
        """Resolve placeholders in a single phase."""
    def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
        """Resolve a template with both prompt and runtime contexts."""
    def _count_tokens((self, text: str)) -> int:
        """Count tokens in text."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""

def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
    """Resolve placeholders in a single phase."""

def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
    """Resolve a template with both prompt and runtime contexts."""

def _count_tokens((self, text: str)) -> int:
    """Count tokens in text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path
from lmstrix.utils.logging import logger

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)) -> None:
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""
    def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
        """Sync scanned models with registry."""

def __init__((self)) -> None:
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""

def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
    """Sync scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    out_ctx: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import APIConnectionError
from lmstrix.core.models import (
    ContextTestStatus,
    Model,
    ModelRegistry,
    ModelRegistryError,
)
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def _validate_discovered_model((model_data: dict)) -> bool:
    """Validate that discovered model data is reasonable before processing."""

def _update_existing_model((
    existing_model: Model,
    model_data: dict,
    rescan_all: bool,
    rescan_failed: bool,
)) -> Model:
    """Update an existing model's data and handle rescan options."""

def _add_new_model((model_data: dict)) -> Model | None:
    """Create a new model entry from discovered data."""

def _remove_deleted_models((registry: ModelRegistry, discovered_models: list[dict])) -> None:
    """Remove models from the registry that are no longer discovered."""

def scan_and_update_registry((
    rescan_failed: bool = False,
    rescan_all: bool = False,
    verbose: bool = False,
)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""

def reset_test_data((
    model_identifier: str,
    verbose: bool = False,
)) -> bool:
    """Reset test data for a specific model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import tomllib
import tomli as tomllib
from topl.core import resolve_placeholders
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger
import tomli_w
import tomlkit
import toml

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


<document index="40">
<source>src/lmstrix/py.typed</source>
<document_content>
# This file indicates that the package supports type hints
# as specified in PEP 561.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.logging import logger, setup_logging
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstrix_log_path,
    get_lmstudio_path,
    get_prompts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py
# Language: python

from typing import Any
from lmstrix.utils.logging import logger

def parse_out_ctx((
    out_ctx: int | str,
    max_context: int,
    fallback_context: int | None = None,
)) -> int:
    """Parse out_ctx parameter which can be an integer or percentage string."""

def get_model_max_context((model: Any, use_tested: bool = True)) -> int | None:
    """Get the maximum context for a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/logging.py
# Language: python

import sys
from typing import Any
from loguru import logger

def setup_logging((verbose: bool = False)) -> None:
    """Configure loguru logging based on verbose flag."""

def format_log((record: Any)) -> str:
    """Custom formatter for colored output with level shortcuts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from lmstrix.api.exceptions import LMStudioInstallationNotFoundError
from lmstrix.utils.logging import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""

def get_lmstrix_log_path(()) -> Path:
    """Get the path to the lmstrix.log.txt file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py
# Language: python

import json
from pathlib import Path
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_lmstudio_path

class StateManager:
    """Manages persistent state for LMStrix."""
    def __init__((self)) -> None:
        """Initialize the state manager."""
    def _load_state((self)) -> dict:
        """Load state from file."""
    def _save_state((self)) -> None:
        """Save state to file."""
    def get_last_used_model((self)) -> str | None:
        """Get the last used model ID."""
    def set_last_used_model((self, model_id: str)) -> None:
        """Set the last used model ID."""
    def clear_last_used_model((self)) -> None:
        """Clear the last used model ID."""

def __init__((self)) -> None:
    """Initialize the state manager."""

def _load_state((self)) -> dict:
    """Load state from file."""

def _save_state((self)) -> None:
    """Save state to file."""

def get_last_used_model((self)) -> str | None:
    """Get the last used model ID."""

def set_last_used_model((self, model_id: str)) -> None:
    """Set the last used model ID."""

def clear_last_used_model((self)) -> None:
    """Clear the last used model ID."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from collections.abc import Generator
from pathlib import Path
from typing import Any
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()) -> Mock:
    """Mock LMStudioClient for testing."""

def mock_llm(()) -> Mock:
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()) -> dict[str, Any]:
    """Sample model data for testing."""

def tmp_models_dir((tmp_path: Path)) -> Path:
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path: Path)) -> Path:
    """Create a temporary registry file path."""

def event_loop(()) -> Generator[asyncio.AbstractEventLoop, None, None]:
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()) -> dict[str, Any]:
    """Mock completion response from LM Studio."""

def mock_prompt_template(()) -> dict[str, Any]:
    """Sample prompt template for testing."""

def mock_context_data(()) -> dict[str, str]:
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys
from lmstrix.utils.logging import logger

def run_tests(()) -> int:
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self: "TestLMStudioClient")) -> None:
        """Test client initialization with different verbose settings."""
    def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test successful completion."""
    def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
        """Test completion failure."""
    def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test completion with default parameters."""

def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self: "TestLMStudioClient")) -> None:
    """Test client initialization with different verbose settings."""

def test_list_models_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful list_models call."""

def test_list_models_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test list_models with connection error."""

def test_load_model_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful model loading."""

def test_load_model_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test model loading failure."""

def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test successful completion."""

def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
    """Test completion failure."""

def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

from lmstrix.api.exceptions import (
    APIConnectionError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)) -> None:
        """Test base LMStrixError class."""
    def test_api_connection_error((self)) -> None:
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)) -> None:
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)) -> None:
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)) -> None:
        """Test that all exceptions inherit from lmstrixError."""

def test_api_error_base((self)) -> None:
    """Test base LMStrixError class."""

def test_api_connection_error((self)) -> None:
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)) -> None:
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)) -> None:
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)) -> None:
    """Test that all exceptions inherit from lmstrixError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

from datetime import datetime
from unittest.mock import Mock, patch
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.models import Model

class TestContextTestResult:
    """Test ContextTestResult data class."""
    def test_result_creation((self)) -> None:
        """Test creating a context test result."""
    def test_result_to_dict((self)) -> None:
        """Test converting result to dictionary."""

class TestContextTester:
    """Test ContextTester PUBLIC API."""
    def test_initialization((self)) -> None:
        """Test context tester initialization."""
    def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test initialization with custom client."""
    def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
        """Test testing a model that doesn't exist."""
    def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
        """Test batch testing of models."""

def test_result_creation((self)) -> None:
    """Test creating a context test result."""

def test_result_to_dict((self)) -> None:
    """Test converting result to dictionary."""

def test_initialization((self)) -> None:
    """Test context tester initialization."""

def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test initialization with custom client."""

def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
    """Test testing a model that doesn't exist."""

def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
    """Test batch testing of models."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model

class TestInferenceDict:
    """Test inference result dictionary structure."""
    def test_inference_dict_success((self)) -> None:
        """Test successful inference result dict."""
    def test_inference_dict_failure((self)) -> None:
        """Test failed inference result dict."""
    def test_inference_dict_empty_response((self)) -> None:
        """Test inference result dict with empty response."""

class TestInferenceManager:
    """Test InferenceManager PUBLIC API."""
    def test_manager_initialization((self)) -> None:
        """Test inference manager initialization."""
    def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test manager with custom client."""
    def test_infer_model_not_found((self)) -> None:
        """Test inference with non-existent model."""
    def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
        """Test basic successful inference."""

def test_inference_dict_success((self)) -> None:
    """Test successful inference result dict."""

def test_inference_dict_failure((self)) -> None:
    """Test failed inference result dict."""

def test_inference_dict_empty_response((self)) -> None:
    """Test inference result dict with empty response."""

def test_manager_initialization((self)) -> None:
    """Test inference manager initialization."""

def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test manager with custom client."""

def test_infer_model_not_found((self)) -> None:
    """Test inference with non-existent model."""

def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
    """Test basic successful inference."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from typing import Any
import pytest
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self: "TestContextTestStatus")) -> None:
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self: "TestModel")) -> None:
        """Test model creation using field aliases."""
    def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test model with context testing information."""
    def test_model_path_validation((self: "TestModel")) -> None:
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self: "TestModel")) -> None:
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self: "TestModel")) -> None:
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving and loading models."""
    def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test getting a model by ID."""
    def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test listing all models."""
    def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test removing a model."""
    def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving/loading models with context test information."""
    def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test that the saved JSON has the expected format."""

def test_enum_values((self: "TestContextTestStatus")) -> None:
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self: "TestModel")) -> None:
    """Test model creation using field aliases."""

def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test model with context testing information."""

def test_model_path_validation((self: "TestModel")) -> None:
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self: "TestModel")) -> None:
    """Test sanitized_id method."""

def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self: "TestModel")) -> None:
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving and loading models."""

def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test getting a model by ID."""

def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test listing all models."""

def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test removing a model."""

def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving/loading models with context test information."""

def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)) -> None:
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)) -> None:
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)) -> None:
        """Test resolver initialization."""
    def test_find_placeholders((self)) -> None:
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)) -> None:
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)) -> None:
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)) -> None:
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)) -> None:
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)) -> None:
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)) -> None:
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)) -> None:
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)) -> None:
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)) -> None:
        """Test resolution with empty string values."""
    def test_count_tokens((self)) -> None:
        """Test token counting."""
    def test_resolve_with_special_characters((self)) -> None:
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)) -> None:
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)) -> None:
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)) -> None:
    """Test resolver initialization."""

def test_find_placeholders((self)) -> None:
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)) -> None:
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)) -> None:
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)) -> None:
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)) -> None:
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)) -> None:
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)) -> None:
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)) -> None:
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)) -> None:
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)) -> None:
    """Test resolution with empty string values."""

def test_count_tokens((self)) -> None:
    """Test token counting."""

def test_resolve_with_special_characters((self)) -> None:
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that non-model files are skipped."""

def test_scanner_initialization((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanner initialization."""

def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a single model file."""

def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that non-model files are skipped."""

def test_scan_models((self: "TestModelScanner", mock_get_path: Mock, tmp_path: Path)) -> None:
    """Test scanning for models."""

def test_sync_with_registry((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.__main__ import LMStrixCLI

class TestCLIIntegration:
    """Test CLI integration - basic functionality only."""
    def test_cli_initialization((self)) -> None:
        """Test CLI can be initialized."""
    def test_infer_requires_parameters((self)) -> None:
        """Test infer command validates required parameters."""

def mock_registry((self, tmp_path: Path)) -> Path:
    """Create a mock model registry."""

def test_cli_initialization((self)) -> None:
    """Test CLI can be initialized."""

def test_list_command((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command shows models."""

def test_list_json_format((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command with JSON output."""

def test_infer_requires_parameters((self)) -> None:
    """Test infer command validates required parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import load_context

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path: Path)) -> None:
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path: Path)) -> None:
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path: Path)) -> None:
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path: Path)) -> None:
        """Test loading large context file."""

def test_load_context_simple((self, tmp_path: Path)) -> None:
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path: Path)) -> None:
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path: Path)) -> None:
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path: Path)) -> None:
    """Test loading large context file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry to custom path."""

def test_scan_and_update_models((
        self: "TestModelLoader",
        mock_save_registry: Mock,
        mock_load_registry: Mock,
        mock_client_class: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((
        self: "TestModelLoader",
        mock_load_registry: Mock,
        mock_client_class: Mock,
    )) -> None:
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from empty TOML file."""

def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from empty TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting default models file path."""
    def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context test log path."""
    def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting prompts directory."""
    def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test finding LM Studio in shared location."""

def exists_side_effect((self: Path)) -> bool:

def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting default models file path."""

def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context tests directory."""

def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context test log path."""

def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting prompts directory."""

def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test handling permission errors when creating directories."""


</documents>