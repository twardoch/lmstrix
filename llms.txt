Project Structure:
📁 lmstrix
├── 📁 _keep_this
│   └── 📁 adam
│       ├── 📁 analyse
│       │   ├── 📁 0
│       │   ├── 📁 input
│       │   ├── 📁 output
│       │   ├── 📁 output-all
│       │   ├── 📁 review
│       │   ├── 📁 review-cla
│       │   └── 📁 review-gemi
│       └── 📁 out
├── 📁 docs
│   ├── 📄 _config.yml
│   ├── 📄 api.md
│   ├── 📄 how-it-works.md
│   ├── 📄 index.md
│   ├── 📄 installation.md
│   └── 📄 usage.md
├── 📁 examples
│   ├── 📁 cli
│   │   ├── 📄 basic_workflow.sh
│   │   ├── 📄 context_control_examples.sh
│   │   ├── 📄 inference_examples.sh
│   │   ├── 📄 model_state_demo.sh
│   │   └── 📄 model_testing.sh
│   ├── 📁 data
│   │   ├── 📄 sample_context.txt
│   │   └── 📄 test_questions.json
│   ├── 📁 prompts
│   │   ├── 📄 analysis.toml
│   │   ├── 📄 coding.toml
│   │   ├── 📄 creative.toml
│   │   └── 📄 qa.toml
│   ├── 📁 python
│   │   ├── 📄 __init__.py
│   │   ├── 📄 advanced_testing.py
│   │   ├── 📄 basic_usage.py
│   │   ├── 📄 batch_processing.py
│   │   ├── 📄 custom_inference.py
│   │   └── 📄 prompt_templates_demo.py
│   ├── 📁 specialized
│   │   └── 📄 elo_liczby.py
│   ├── 📄 prompts.toml
│   ├── 📄 README.md
│   └── 📄 run_all_examples.sh
├── 📁 issues
│   ├── 📄 301.txt
│   ├── 📄 302.txt
│   ├── 📄 303.txt
│   ├── 📄 304.txt
│   ├── 📄 305.txt
│   ├── 📄 306.txt
│   ├── 📄 307.txt
│   ├── 📄 310.txt
│   └── 📄 311.txt
├── 📁 src
│   ├── 📁 lmstrix
│   │   ├── 📁 api
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 client.py
│   │   │   ├── 📄 exceptions.py
│   │   │   └── 📄 main.py
│   │   ├── 📁 core
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 concrete_config.py
│   │   │   ├── 📄 context.py
│   │   │   ├── 📄 context_tester.py
│   │   │   ├── 📄 inference.py
│   │   │   ├── 📄 inference_manager.py
│   │   │   ├── 📄 models.py
│   │   │   ├── 📄 prompts.py
│   │   │   └── 📄 scanner.py
│   │   ├── 📁 loaders
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context_loader.py
│   │   │   ├── 📄 model_loader.py
│   │   │   └── 📄 prompt_loader.py
│   │   ├── 📁 utils
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context_parser.py
│   │   │   ├── 📄 logging.py
│   │   │   ├── 📄 paths.py
│   │   │   └── 📄 state.py
│   │   ├── 📄 __init__.py
│   │   ├── 📄 __main__.py
│   │   └── 📄 py.typed
│   └── 📁 lmstrix.egg-info
├── 📁 tests
│   ├── 📁 test_api
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_client.py
│   │   └── 📄 test_exceptions.py
│   ├── 📁 test_core
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_tester.py
│   │   ├── 📄 test_inference.py
│   │   ├── 📄 test_models.py
│   │   ├── 📄 test_prompts.py
│   │   └── 📄 test_scanner.py
│   ├── 📁 test_integration
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_cli_integration.py
│   ├── 📁 test_loaders
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_loader.py
│   │   ├── 📄 test_model_loader.py
│   │   └── 📄 test_prompt_loader.py
│   ├── 📁 test_utils
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_paths.py
│   ├── 📄 __init__.py
│   ├── 📄 conftest.py
│   └── 📄 run_tests.py
├── 📄 .gitignore
├── 📄 AGENTS.md
├── 📄 build.sh
├── 📄 CHANGELOG.md
├── 📄 CLAUDE.md
├── 📄 cleanup.txt
├── 📄 GEMINI.md
├── 📄 LICENSE
├── 📄 PLAN.md
├── 📄 publish.sh
├── 📄 pyproject.toml
├── 📄 README.md
├── 📄 test_streaming.py
├── 📄 TESTING.md
├── 📄 TODO.md
└── 📄 WORK.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="2">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
_keep_this/adam
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
build/
celerybeat-schedule
celerybeat.pid
CLEANUP.txt
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
llms.txt
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
src/lmstrix/_version.py
target/
Thumbs.db
uv.lock
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="3">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="4">
<source>AGENTS.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="5">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- **Issue #307**: Streaming Inference Support
  - Added `stream_completion()` method to LMStudioClient using lmstudio SDK's `complete_stream()`
  - Implemented token-by-token streaming with callbacks and progress monitoring
  - Added `stream_infer()` methods to InferenceEngine and InferenceManager
  - Added CLI `--stream` flag for real-time token display during inference
  - Added `--stream-timeout` parameter (default 120s) for hang detection
  - Streaming statistics: tokens/second, time to first token
- **Issue #306**: Batch Processing Tool (`_keep_this/adam/adamall.py`)
  - Processes multiple prompts across all models automatically
  - Smart model management - reuses loaded models when possible
  - Skips existing outputs for resumable batch processing
  - Safe filename generation using pathvalidate
  - Error capture to output files on failure
  - Progress tracking with percentage completion
  - Processes prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`
- Created `src/lmstrix/api/main.py` with `LMStrixService` class containing all business logic
- Implemented separation of concerns with thin CLI wrapper in `__main__.py`

### Changed
- Default temperature changed from 0.7 to 0.8 to match LM Studio GUI defaults
- Default `out_ctx` now falls back to the model's `ctx_out − 1` when callers leave it as `-1`. This avoids LM Studio SDK hangs triggered by unlimited generations.
- Refactored CLI architecture - moved all business logic from `__main__.py` to `api/main.py`
- Updated `__main__.py` to be a thin wrapper that delegates to `LMStrixService`
- Modified test imports to use new module structure

### Fixed
- **Issue #303**: Fixed loguru output interference with model responses
  - Removed loguru formatting of config dictionaries and raw model responses
  - Changed problematic log messages in `api/client.py` lines 265 and 267 to simple status messages
  - Prevented KeyError and ValueError exceptions when loguru tried to parse model output
  - Ensures clean separation of diagnostic output (stderr) from model output (stdout)
- Fixed import error after removing `src/lmstrix/cli` directory
- Updated `pyproject.toml` entry point from `lmstrix.cli.main:main` to `lmstrix.__main__:main`

### Removed
- Removed `src/lmstrix/cli` directory (merged functionality into `__main__.py`)

## [1.0.59] - 2025-07-31

### Major Improvements & Bug Fixes

#### Issues 201-204 (Completed)
- **Issue 201**: Enhanced model persistence - models now stay loaded between inference calls when no explicit context is specified
- **Issue 202**: Beautiful enhanced logging with emojis, model info, config details, and prompt statistics  
- **Issue 203**: Fixed model lookup to find by both path and ID without changing JSON structure
- **Issue 204**: Added comprehensive verbose stats logging including time to first token, predicted tokens, tokens/second, and total inference time

#### Model Registry Improvements
- Fixed smart model lookup that works with both model paths and IDs
- Preserved original JSON structure keyed by path
- No data duplication - registry size maintained
- Backward compatible with existing path-based lookups

#### Enhanced Logging & Statistics
- Beautiful formatted logging with visual separators and emojis
- Complete inference statistics display including:
  - ⚡ Time to first token
  - ⏱️ Total inference time  
  - 🔢 Predicted tokens
  - 📝 Prompt tokens
  - 🎯 Total tokens
  - 🚀 Tokens/second
  - 🛑 Stop reason
- Eliminated duplicate stats display at end of output

### Added

- **Context Parameter Percentage Support**
  - `--out_ctx` parameter now supports percentage notation (e.g., "80%")
  - Created `utils/context_parser.py` for parsing context parameters
  - Percentage calculated from model's tested or declared maximum context

- **Improved CLI Output**
  - Non-verbose mode for `infer` command now shows only model response
  - Removed extra formatting and status information in quiet mode

- **Prompt File Support with TOML (Issue #104)**
  - Added `--file_prompt` parameter to load prompts from TOML files
  - Added `--dict` parameter for passing key=value pairs for placeholder resolution
  - When using `--file_prompt`, the prompt parameter refers to the prompt name in the TOML file
  - Supports nested placeholders and internal template references
  - Reports unresolved placeholders in verbose mode
  - Includes comprehensive example prompt file with various use cases

- **Enhanced Infer Context Control (Issue #103)**
  - Added `--in_ctx` parameter to control model loading context size
  - Added `--out_ctx` parameter to replace deprecated `--max_tokens`
  - Supports `--in_ctx 0` to load model without context specification
  - When `--in_ctx` is not specified, uses optimal context (tested or declared)
  - Explicit `--in_ctx` always unloads existing models and reloads with specified context
  - Smart unloading: only unloads models that were explicitly loaded with `--in_ctx`

- **Smart Model Loading**
  - Added model state detection to check if models are already loaded
  - Reuses existing loaded models when no explicit context specified
  - Added `--force-reload` flag to force model reload even if already loaded
  - Shows clear status messages about model reuse vs reload

- **Model State Persistence (Issue #201)**
  - Models now stay loaded between `infer` calls when `--in_ctx` is not specified
  - Added StateManager to track last-used model across sessions
  - Model ID parameter (`-m`) is now optional - uses last-used model when omitted
  - Significantly improves performance by avoiding repeated model loading/unloading
  - Created `examples/cli/model_state_demo.sh` demonstrating the feature

- **Context Validation**
  - Validates requested context against model's declared and tested limits
  - Warns when context exceeds safe limits
  - Suggests optimal context values

### Changed

- **Inference Command**
  - Deprecated `--max_tokens` in favor of `--out_ctx` (backward compatible with warnings)
  - Updated help text and documentation for new parameters
  - Improved model loading logic for better memory management
  - Enhanced status messages during inference operations

## [1.0.53] - 2025-07-29

### Fixed

- **Git Configuration**
  - Fixed git pull error by setting upstream branch tracking with `git branch --set-upstream-to=origin/main main`
  - Resolved "exit code(1)" error when running `git pull -v -- origin` without branch specification

### Changed

- **Version Maintenance**
  - Updated to version 1.0.53 with proper git configuration fixes

## [1.0.28 - 1.0.52] - 2025-07-25 to 2025-07-29

### Added

- **Enhanced CLI Features**
  - Added `--sort` option to `lmstrix test --all` command with same sort options as list (id, ctx, dtx, size, etc.)
  - Added `--ctx` option to `lmstrix test --all` for testing all untested models at a specific context size
  - Added `--show` option to `lmstrix list` with three output formats:
    - `id`: Plain newline-delimited list of model IDs
    - `path`: Newline-delimited list of relative paths (same as id currently)
    - `json`: Full JSON array from the registry
  - All `--show` formats respect the `--sort` option for flexible output

- **CLI Improvements**
  - Modified `--ctx` to work with `--all` flag for batch testing at specific context sizes
  - `test --all --ctx` filters models based on context limits and safety checks
  - Added proper status updates and persistence when using `--ctx` for single model tests
  - Fixed model field updates (tested_max_context, last_known_good_context) during --ctx testing

### Changed

- **Removed all asyncio dependencies (Issue #204)**
  - Converted entire codebase from async to synchronous
  - Now uses the native synchronous `lmstudio` package API directly
  - Simplified architecture by removing async/await complexity
  - Implemented signal-based timeout for Unix systems
  - All methods now return results directly without await

### Added

- **Context Size Safety Validation**
  - Added validation to prevent testing at or above `last_known_bad_context`
  - CLI `--ctx` parameter now checks against `last_known_bad_context` and limits to 90% of last bad
  - Automatic testing algorithms now respect `last_known_bad_context` during iterations
  - Added warning messages when context size approaches 80% or 90% of last known bad context
  - Prevents system crashes by avoiding previously failed context sizes

- **Enhanced Context Testing Strategy (Issue #201)**
  - Added `--threshold` parameter to test command (default: 102,400 tokens)
  - Prevents system crashes by limiting initial test size
  - New incremental testing algorithm: test at 1024, then threshold, then increment by 10,240
  - Optimized batch testing for `--all` flag with pass-based approach
  - Models sorted by declared context size to minimize loading/unloading
  - Rich table output showing test results with efficiency percentages

- **Smart Context Testing with Progress Saving**
  - Context tests now start with small context (32) to verify model loads
  - Added fields to track last known good/bad context sizes
  - Tests can resume from previous state if interrupted
  - Progress is saved to JSON after each test iteration
  - Changed test prompt from "2+2=" to "Say hello" for better reliability

### Fixed

- **Terminology Improvements**
  - Changed "Loaded X models" to "Read X models" to avoid confusion with LM Studio's model loading
  - Replaced generic "Check logs for details" with specific error messages

- **Context Testing Stability**
  - Added delays between model load/unload operations to prevent rapid cycling
  - Fixed connection reset issues caused by too-rapid operations
  - Enhanced binary search logging to show progress clearly

### Changed

- **Model Data Structure**
  - Added `last_known_good_context` field for resumable testing
  - Added `last_known_bad_context` field for resumable testing
  - Updated registry serialization to include new fields

## [1.0.28] - 2025-07-25

### Added

- **GitHub Pages Documentation Site**
  - Created comprehensive documentation site structure under `docs/`
  - Added Jekyll configuration with custom theme and navigation
  - Created documentation pages: installation, usage, API reference, how-it-works
  - Set up automatic changelog integration with documentation
  - Added responsive design and syntax highlighting

- **Example Improvements**
  - Added example output logging to `examples.log.txt` and `examples.err.txt`
  - Enhanced error handling in example scripts
  - Added more detailed comments in Python examples

### Fixed

- **Client Compatibility**
  - Fixed attribute access issues in `api/client.py` for embedding models
  - Added proper handling for different model types (LLMs vs Embeddings)
  - Improved error messages for unsupported model types

- **Context Testing Robustness**
  - Enhanced context size detection with better error handling
  - Improved inference validation logic
  - Added fallback mechanisms for edge cases

### Changed

- **CLI Enhancements**
  - Improved output formatting for model listings
  - Better progress indicators during testing
  - More informative error messages

- **Documentation Updates**
  - Updated README with clearer examples
  - Enhanced API documentation with more details
  - Added troubleshooting section

## [1.0.23] - 2025-07-25

### Added

- Implemented comprehensive `test_binary_search_logic` test function with edge cases:
  - Model that works at all sizes
  - Model that never works  
  - Model that loads but never passes inference
  - Model that works up to a specific context size
  - Model that never loads

### Fixed

- Fixed syntax errors in Python example files:
  - `examples/python/batch_processing.py` - Fixed unterminated string literals
  - `examples/python/custom_inference.py` - Fixed similar syntax issues
- Improved code quality by fixing linting issues:
  - Updated file operations to use `Path.open()` instead of `open()` (PTH123)
  - Added proper exception chaining with `from e` (B904)
  - Fixed shadowing of Python builtin `all` by renaming to `all_models` (A002)
- Updated `api/client.py` to handle different attribute names for LLMs vs Embeddings

### Changed

- All mypy type checking now passes without errors
- Package builds successfully as v1.0.23

## [1.0.21] - 2025-07-25

### Fixed

- Code formatting improvements in test files for better readability
- Updated `cleanup.sh` script
- Reduced size of `llms.txt` file significantly for better performance

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` package’s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="6">
<source>CLAUDE.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1.  **API Layer** (`src/lmstrix/api/`)
    -   `client.py`: Async client for LM Studio server API with retry logic using `tenacity`.
    -   `exceptions.py`: Custom exception hierarchy for better error handling.

2.  **Core Engine** (`src/lmstrix/core/`)
    -   `context_tester.py`: Binary search algorithm to find optimal context window size, with `rich` progress bar integration.
    -   `inference.py`: Handles the inference process, including prompt building.
    -   `models.py`: Model registry with persistence for tracking tested context limits.
    -   `scanner.py`: Discovers and catalogs available LM Studio models.
    -   `prompts.py`: Prompt resolution and template management.
    -   `context.py`: Manages context, including prompt templates and token counting using `tiktoken`.

3.  **Loaders** (`src/lmstrix/loaders/`)
    -   `model_loader.py`: Manages model registry persistence (JSON).
    -   `prompt_loader.py`: Loads prompt templates from TOML files.
    -   `context_loader.py`: Loads context data from text files.

4.  **CLI** (`src/lmstrix/cli/`)
    -   `main.py`: `fire`-based CLI with commands: `scan`, `list`, `test`, `infer`.
    -   Uses `rich` for beautiful terminal output.

### 2.2. Critical Design Patterns

-   **Async-First**: All API operations use `async/await` for high performance.
-   **Retry Logic**: Uses `tenacity` for automatic retries with exponential backoff.
-   **Model Registry**: Persists discovered models and their tested limits to JSON.
-   **Two-Phase Prompts**: Separates prompt template structure from runtime context.
-   **Binary Search**: Efficiently finds maximum context window through targeted testing.

### 2.3. Dependencies

-   `lmstudio-python`: Official LM Studio Python SDK.
-   `httpx`: Async HTTP client.
-   `pydantic`: Data validation and models.
-   `fire`: CLI framework.
-   `rich`: Terminal formatting.
-   `tenacity`: Retry logic.
-   `tiktoken`: Token counting.
-   `loguru`: Logging.

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="7">
<source>GEMINI.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="8">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="9">
<source>PLAN.md</source>
<document_content>
# LMStrix Current Development Plan


## Current Status
### Problem Description
When running inference with `--verbose`, loguru was outputting the prompt and model response through its handlers, causing:
1. **Logging errors**: KeyError and ValueError exceptions in loguru handlers
2. **Output pollution**: Model responses were mixed with loguru formatting
3. **User requirement violation**: User explicitly stated to NEVER pass prompt or model output through loguru

### Root Cause
- In `api/client.py`, the prompt and model response were being logged via loguru
- Loguru was trying to format the output, causing issues with special characters like `</translation-instructions>`
- This violated the requirement to use `sys.stderr` for prompts and `sys.stdout` for model output


## CRITICAL: Issue #302 - Inference Output Mismatch

### Problem Description
When running the same translation prompt through LM Studio GUI vs lmstrix CLI, we get drastically different results:
- **LM Studio GUI**: Produces proper Polish translation (639 tokens)
- **lmstrix CLI**: Only outputs `</translate>` (4 tokens)

### Root Cause Analysis

#### Configuration Differences Found
1. **Temperature**: GUI uses 0.8, CLI uses 0.7
2. **top_k**: GUI uses 20, CLI uses 40  
3. **Context Length**: GUI uses full 131072, CLI uses 65536 (reduced)
4. **max_predict**: GUI uses -1 (unlimited), CLI calculates 117964
5. **Sampling Parameters**: Multiple differences in repeat_penalty, min_p, etc.

#### Potential Issues
1. **Early Stop Token**: Model might be hitting a stop token immediately
2. **Prompt Format**: The prompt might be wrapped or modified differently
3. **Chat Template**: Possible incorrect chat template application
4. **Parameter Mismatch**: Inference parameters not matching LM Studio defaults

## Immediate Priorities

### 1. Fix Issue #302 - Inference Output Mismatch
**Priority: CRITICAL - NOW TOP PRIORITY**
- This is blocking proper inference functionality
- Users cannot get correct model outputs
- Model only outputs `</translation>` instead of full translation

#### Implementation Steps:

##### Step 1: Add Diagnostic Logging
- Log the exact prompt being sent (with escape sequences visible)
- Log all inference parameters in detail
- Log stop tokens being used
- Add comparison with LM Studio expected values

##### Step 2: Parameter Alignment
- Change default temperature from 0.7 to 0.8
- Add CLI parameters for all inference settings
- Remove or make optional the maxTokens calculation

##### Step 3: Context Length Fix
- Fix context reduction from 131072 to 65536
- Use full model context unless explicitly limited
- Add warning when context is reduced

##### Step 4: Stop Token Investigation  
- Check why model stops at `</translate>`
- Verify stop token configuration
- Test with stop tokens disabled

### 2. Issue #105 - Adam.toml Simplification
**Priority: High** (after Issue #302)
- Simplify adam.toml structure to use flat format instead of nested groups
- Add --text and --text_file parameters to infer command for direct text input
- Update all prompt examples to use simplified approach
- Ensure backward compatibility with existing TOML files

## NEW FEATURES: Issues #306 & #307 - Streaming & Batch Processing

### Issue #307: Streaming Inference Support
**Priority: HIGH** - Modern inference capability

#### Problem Statement
Current lmstrix only supports synchronous inference via `llm.complete()`, which:
- Blocks until entire response is complete
- Provides no real-time feedback during generation
- Cannot handle interrupted inference gracefully
- Missing modern streaming UX that users expect

#### Technical Requirements
1. **Async Streaming API**: Integrate lmstudio SDK's `complete_stream()` capability
2. **Backward Compatibility**: Maintain existing sync API while adding streaming
3. **CLI Integration**: Add `--stream` flag to inference commands
4. **Error Handling**: Robust streaming error recovery and cancellation
5. **Performance**: Efficient token-by-token processing without blocking

#### Implementation Architecture

##### Core Components:
1. **StreamingInferenceManager** - Async counterpart to InferenceManager
2. **LMStudioClient.stream_completion()** - Async streaming method  
3. **CLI streaming support** - `--stream` flag with real-time output
4. **Error resilience** - Handle network interruptions, model failures
5. **Token callbacks** - Extensible handler system for streaming tokens

##### API Design:
```python
# Async streaming interface
async def stream_infer(
    model_id: str, 
    prompt: str,
    on_token: Callable[[str], None] = None,
    **kwargs
) -> AsyncGenerator[str, None]:
    """Stream inference tokens as they arrive"""
    
# CLI usage
lmstrix infer translate --stream -m model-id --text "Hello world"
```

### Issue #306: Batch Processing Tool (adamall.py)
**Priority: HIGH** - Essential automation tool

#### Problem Statement
Users need to efficiently run multiple prompts across multiple models:
- Manual inference is time-consuming and error-prone
- Need to process 6 specific prompts against all available models
- Must avoid unnecessary model loading/unloading
- Require robust error handling and progress tracking
- Output organization for downstream processing

#### Technical Requirements
1. **Smart Model Management**: Load models only when needed, reuse when possible
2. **Prompt Processing**: Handle multiple prompts from adam.toml efficiently
3. **File Organization**: Safe output naming with pathvalidate sanitization
4. **Skip Existing**: Don't regenerate outputs that already exist
5. **Error Recovery**: Capture errors in output files, continue processing
6. **Progress Tracking**: Clear feedback on processing status

#### Implementation Architecture

##### Core Components:
1. **BatchProcessor** - Main orchestration class
2. **ModelManager** - Efficient model loading/unloading
3. **PromptRunner** - Individual prompt execution with error handling
4. **OutputManager** - File naming, existence checking, error capture
5. **ProgressTracker** - Status reporting and logging

##### Workflow Design:
```python
# Processing strategy
1. Load all models from registry, sort by "smart" method
2. Load all required prompts from adam.toml  
3. Build execution matrix (model × prompt combinations)
4. Filter out existing outputs to skip redundant work
5. Group by model to minimize loading operations
6. Execute with comprehensive error handling
7. Progress reporting throughout
```

##### Target Prompts:
- `think,aps`
- `think,humanize` 
- `think,tldr`
- `think,tts_optimize`
- `translate`
- `tldr`

#### File Management Strategy:
- **Input**: `_keep_this/adam/fontlab8.md`
- **Output Pattern**: `_keep_this/adam/out/{safe_prompt_name}--{safe_model_id}.txt`
- **Safety**: Use pathvalidate for filename sanitization
- **Existence Check**: Skip files that already exist
- **Error Capture**: Write error messages to output files on failure

## Detailed Technical Implementation

### Issue #307: Streaming Implementation Breakdown

#### Phase 1: Core Infrastructure (Day 1-2)
1. **LMStudioClient Async Methods**:
   ```python
   async def stream_completion(
       self, llm, prompt: str, 
       on_token: Callable[[str], None] = None,
       **kwargs
   ) -> AsyncGenerator[str, None]:
       """Stream tokens from lmstudio SDK async API"""
   ```

2. **StreamingInferenceManager Class**:
   ```python
   class StreamingInferenceManager:
       async def stream_infer(self, model_id: str, prompt: str, **kwargs):
           """Async streaming counterpart to InferenceManager.infer()"""
   ```

3. **Integration Points**:
   - Extend existing InferenceManager with async methods
   - Maintain backward compatibility with sync interface
   - Use asyncio for event loop management

#### Phase 2: CLI Integration (Day 2-3)
1. **Command Line Interface**:
   - Add `--stream` flag to `lmstrix infer` command
   - Real-time token display to stdout
   - Handle Ctrl+C gracefully for cancellation

2. **Error Handling**:
   - Network interruption recovery
   - Model loading failures during streaming
   - Graceful degradation to sync mode if streaming fails

#### Phase 3: Testing & Polish (Day 3-5)
1. **Unit Tests**: Async streaming functionality
2. **Integration Tests**: CLI streaming with real models
3. **Performance Tests**: Compare streaming vs sync performance
4. **Documentation**: Usage examples and API documentation

### Issue #306: Batch Tool Implementation Breakdown

#### Phase 1: Core Architecture (Day 1-2)
1. **BatchProcessor Class**:
   ```python
   class BatchProcessor:
       def __init__(self, adam_toml_path, fontlab_md_path, output_dir):
           self.models = self.load_and_sort_models()
           self.prompts = self.load_prompts()
           self.matrix = self.build_execution_matrix()
   ```

2. **Smart Model Management**:
   ```python
   class ModelManager:
       def __init__(self, inference_manager):
           self.current_model = None
           self.inference_manager = inference_manager
           
       async def ensure_model_loaded(self, model_id: str, context_size: int):
           """Load model only if not already loaded"""
   ```

3. **Output Management**:
   ```python
   class OutputManager:
       def __init__(self, output_dir: Path):
           self.output_dir = output_dir
           
       def get_output_path(self, prompt_name: str, model_id: str) -> Path:
           """Generate safe output path using pathvalidate"""
           
       def should_skip(self, output_path: Path) -> bool:
           """Check if output already exists"""
   ```

#### Phase 2: Execution Engine (Day 2-3)
1. **Execution Matrix**:
   - Build combinations of models × prompts
   - Filter existing outputs to avoid redundant work
   - Group by model to minimize loading overhead

2. **Progress Tracking**:
   ```python
   class ProgressTracker:
       def __init__(self, total_tasks: int):
           self.completed = 0
           self.total = total_tasks
           self.start_time = time.time()
           
       def update(self, prompt_name: str, model_id: str, status: str):
           """Update progress and display to user"""
   ```

3. **Error Resilience**:
   - Wrap each inference in try/except
   - Write error messages to output files
   - Continue processing despite individual failures
   - Log all errors for debugging

#### Phase 3: Integration & Testing (Day 3-4)
1. **CLI Tool Creation**: `_keep_this/adam/adamall.py`
2. **Integration**: Use existing lmstrix components
3. **Memory Management**: [[memory:4463738]] Unload models between tests
4. **Context Management**: [[memory:4875704]] Use ctx_out from registry
5. **Testing**: End-to-end validation with real models and prompts

#### Target Integration Points:
- **Model Registry**: `src/lmstrix/core/models.py` for model listing and smart sorting
- **Prompt Loader**: `src/lmstrix/loaders/prompt_loader.py` for adam.toml processing  
- **Inference Manager**: `src/lmstrix/core/inference_manager.py` for actual inference
- **Logging**: `src/lmstrix/utils/logging.py` for consistent logging

#### Execution Strategy:
```python
# Pseudo-code for batch execution
for model in sorted_models:
    for prompt_name in target_prompts:
        output_path = get_safe_output_path(prompt_name, model.id)
        if output_path.exists():
            continue  # Skip existing
            
        try:
            ensure_model_loaded(model.id, context_size=int(model.context_limit * 0.5))
            resolved_prompt = load_prompt(prompt_name, text=fontlab8_content)
            result = inference_manager.infer(
                model_id=model.id,
                prompt=resolved_prompt.resolved,
                out_ctx=int(model.context_out * 0.9)  # Use 90% of max context
            )
            write_output(output_path, result.response)
        except Exception as e:
            write_error(output_path, str(e))
        finally:
            progress_tracker.update(prompt_name, model.id, "completed")
```

## Implementation Order

1. **Issue #302 - Inference Fix** (NOW TOP PRIORITY):
   - Align parameters with LM Studio
   - Fix context length handling
   - Investigate stop token issue

### Phase 1: New Features Implementation (After Issue #302)
1. **Issue #307 - Streaming Inference** (3-5 days):
   - Implement async streaming infrastructure
   - Add CLI streaming support
   - Comprehensive testing
   
2. **Issue #306 - Batch Processing Tool** (3-4 days):
   - Build adamall.py with smart model management
   - Integrate with existing prompt/model systems
   - Output management and error handling

### Phase 2: Normal Development (After new features)
- Continue with Issue #105 and other planned improvements

## Key Design Decisions & Risk Mitigation

### Issue #307: Streaming Implementation Decisions
1. **Async-First Approach**: Use Python asyncio for true non-blocking streaming
2. **Backward Compatibility**: Keep existing sync API unchanged, add streaming as enhancement
3. **Error Resilience**: Graceful fallback to sync mode if streaming fails
4. **Memory Management**: Stream tokens without buffering entire response
5. **CLI Integration**: Simple `--stream` flag, no breaking changes to existing commands

### Issue #306: Batch Tool Design Decisions  
1. **Smart Model Loading**: Minimize model loading overhead by grouping operations by model
2. **Idempotent Execution**: Skip existing outputs to support resumable batch processing
3. **Error Isolation**: Individual task failures don't stop entire batch
4. **Memory Safety**: [[memory:4463738]] Always unload models between major operations
5. **Context Optimization**: [[memory:4875704]] Use model-specific ctx_out values for optimal performance
6. **File Safety**: Use pathvalidate for cross-platform filename sanitization

### Risk Assessment & Mitigation
1. **Memory Pressure**: Batch tool could exhaust system memory
   - *Mitigation*: Explicit model unloading, memory monitoring
2. **Long Processing Times**: Large batch jobs could take hours
   - *Mitigation*: Progress tracking, resumable execution, early skip existing
3. **Network Failures**: Streaming could be interrupted
   - *Mitigation*: Graceful degradation, retry logic, fallback to sync
4. **File System Issues**: Output file conflicts or permission errors
   - *Mitigation*: Safe filename generation, directory creation, error capture

### Performance Considerations
1. **Streaming**: Real-time token display vs sync waiting
2. **Batch Processing**: Model reuse vs loading overhead
3. **Memory Usage**: Stream buffering vs batch model loading
4. **I/O Efficiency**: File existence checking vs redundant processing

## Future Development Phases

### Phase A: Core Simplification (2-3 weeks)
1. **Configuration Unification**
   - Create utils/config.py for centralized configuration handling
   - Consolidate path handling functions
   - Remove redundant configuration code

2. **Error Handling Standardization**
   - Review and simplify custom exception hierarchy
   - Standardize error messages across codebase
   - Implement consistent logging patterns

### Phase B: CLI Enhancement (1-2 weeks)
1. **Command Improvements**
   - Enhance `scan` command with better progress reporting
   - Improve `list` command with filtering and sorting options
   - Add `reset` command for clearing model test data

2. **User Experience**
   - Better error messages with helpful suggestions
   - Improved help text and documentation
   - Enhanced progress indicators for long-running operations

### Phase C: Testing & Documentation (1 week)
1. **Test Suite Completion**
   - Ensure >90% test coverage maintained
   - Add integration tests for new features
   - Performance benchmarking of improvements

2. **Documentation Updates**
   - Update README.md with latest features
   - Create comprehensive CLI reference
   - Update examples to demonstrate new capabilities

## Success Metrics

- **Functionality**: All existing CLI commands work without regression
- **Performance**: Model loading and inference speed improvements
- **Usability**: Cleaner, more informative user interface
- **Maintainability**: Reduced complexity, better code organization
- **Documentation**: Up-to-date and comprehensive user guides

## NEW DEVELOPMENT PRIORITIES

### Issue #307 - Implement LM Studio Streaming Inference
**Priority: High** (Modern SDK capability)

#### Problem Analysis
- Current lmstrix uses synchronous `llm.complete()` which blocks until full response
- LM Studio SDK 1.4+ supports `llm.complete_stream()` with real-time token streaming
- Streaming enables: real-time progress feedback, automatic hang detection, better UX

#### Implementation Strategy

##### Phase 1: Core Streaming Migration
- Replace `llm.complete()` with `llm.complete_stream()` in `LMStudioClient.completion()`
- Implement progress callbacks: `on_prediction_fragment`, `on_first_token`
- Add no-progress timeout watchdog to abort stalled generations
- Maintain backward compatibility with existing response format

##### Phase 2: CLI Enhancement
- Add `--stream-timeout` flag (default 120s) for hang detection
- Add `--show-progress` flag for real-time token display
- Update inference logging to show streaming status

##### Phase 3: Advanced Features
- Implement token-by-token display in verbose mode
- Add streaming statistics (tokens/second, time to first token)
- Enable streaming cancellation via Ctrl+C

### Issue #306 - Batch Inference Tool (adamall.py)
**Priority: Medium** (User productivity tool)

#### Requirements Analysis
- Batch process multiple models × multiple prompts
- Smart model loading/unloading to minimize operations
- Output management with safe filename generation
- Error handling and recovery

#### Implementation Strategy

##### Phase 1: Core Infrastructure
- Create `_keep_this/adam/adamall.py` using lmstrix API
- Implement smart model state detection and caching
- Add pathvalidate-based safe filename generation
- Configure batch prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`

##### Phase 2: Output Management
- Implement file existence checking to skip completed inference
- Generate output paths: `f"_keep_this/adam/out/{safe_prompt_name}--{safe_model_id}.txt"`
- Add error message capture to output files on failure

##### Phase 3: Performance Optimization
- Sort models by "smart" method (as in `lmstrix list`)
- Load models with 50% input context, inference with 90% max context
- Minimize model loading/unloading through intelligent scheduling

## Long-term Vision

The goal is to make LMStrix the most user-friendly and efficient tool for managing and testing LM Studio models, with:
- Real-time streaming inference with progress feedback and hang detection
- Intuitive CLI interface with beautiful, informative output
- Smart model management with automatic optimization
- Batch processing capabilities for productivity workflows
- Comprehensive testing capabilities with clear results
- Excellent developer experience with clean, well-documented code
</document_content>
</document>

<document index="10">
<source>README.md</source>
<document_content>
# LMStrix

LMStrix is a professional Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and Python API for managing, testing, and running local language models, with a standout feature: **Adaptive Context Optimization**.

## Key Features

- **🔍 Automatic Context Discovery**: Binary search algorithm to find the true operational context limit of any model
- **📊 Beautiful Verbose Logging**: Enhanced stats display with emojis showing inference metrics, timing, and token usage
- **🚀 Smart Model Management**: Models persist between calls to reduce loading overhead
- **🎯 Flexible Inference Engine**: Run inference with powerful prompt templating and percentage-based output control
- **📋 Comprehensive Model Registry**: Track models, their context limits, and test results with JSON persistence
- **🛡️ Safety Controls**: Configurable thresholds and fail-safes to prevent system crashes
- **💻 Rich CLI Interface**: Beautiful terminal output with progress indicators and formatted tables

## Installation

```bash
# Using pip
pip install lmstrix

# Using uv (recommended)
uv pip install lmstrix
```

## Quick Start

### Command-Line Interface

```bash
# Scan for available models in LM Studio
lmstrix scan

# List all models with their context limits and test status
lmstrix list

# Test context limit for a specific model
lmstrix test llama-3.2-3b-instruct

# Test all untested models with safety threshold
lmstrix test --all --threshold 102400

# Run inference with enhanced verbose logging
lmstrix infer "What is the capital of Poland?" -m llama-3.2-3b-instruct --verbose

# Run inference with percentage-based output tokens
lmstrix infer "Explain quantum computing" -m llama-3.2-3b-instruct --out_ctx "25%"

# Use file-based prompts with templates
lmstrix infer summary -m llama-3.2-3b-instruct --file_prompt adam.toml --text_file document.txt

# Direct text input for prompts
lmstrix infer "Summarize: {{text}}" -m llama-3.2-3b-instruct --text "Your content here"
```

### Enhanced Verbose Output

When using `--verbose`, LMStrix provides comprehensive statistics:

```
════════════════════════════════════════════════════════════
🤖 MODEL: llama-3.2-3b-instruct
🔧 CONFIG: maxTokens=26214, temperature=0.7
📝 PROMPT (1 lines, 18 chars): Capital of Poland?
════════════════════════════════════════════════════════════
⠸ Running inference...
════════════════════════════════════════════════════════════
📊 INFERENCE STATS
⚡ Time to first token: 0.82s
⏱️  Total inference time: 11.66s
🔢 Predicted tokens: 338
📝 Prompt tokens: 5
🎯 Total tokens: 343
🚀 Tokens/second: 32.04
🛑 Stop reason: eosFound
════════════════════════════════════════════════════════════
```

### Python API

```python
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.core.inference_manager import InferenceManager

# Load model registry
registry = load_model_registry()

# List available models
models = registry.list_models()
print(f"Available models: {len(models)}")

# Run inference
manager = InferenceManager(verbose=True)
result = manager.infer(
    model_id="llama-3.2-3b-instruct",
    prompt="What is the meaning of life?",
    out_ctx=100,
    temperature=0.7
)

if result["succeeded"]:
    print(f"Response: {result['response']}")
    print(f"Tokens used: {result['tokens_used']}")
    print(f"Time: {result['inference_time']:.2f}s")
```

## Context Testing & Optimization

LMStrix uses a sophisticated binary search algorithm to discover true model context limits:

### Safety Features
- **Threshold Protection**: Configurable maximum context size to prevent system crashes
- **Progressive Testing**: Starts with small contexts and increases safely
- **Persistent Results**: Saves test results to avoid re-testing

### Testing Commands
```bash
# Test specific model
lmstrix test llama-3.2-3b-instruct

# Test all models with custom threshold
lmstrix test --all --threshold 65536

# Test at specific context size
lmstrix test --all --ctx 32768

# Reset and re-test a model
lmstrix test llama-3.2-3b-instruct --reset
```

## Model Management

### Registry Commands
```bash
# Scan for new models
lmstrix scan --verbose

# List models with different sorting
lmstrix list --sort size        # Sort by size
lmstrix list --sort ctx         # Sort by tested context
lmstrix list --show json        # Export as JSON

# Check system health
lmstrix health --verbose
```

### Model Persistence
Models stay loaded between inference calls for improved performance:
- When no explicit context is specified, models remain loaded
- Last-used model is remembered for subsequent calls
- Explicit context changes trigger model reloading

## Prompt Templating

LMStrix supports flexible prompt templating with TOML files:

```toml
# adam.toml
[aps]
prompt = """
You are an AI assistant skilled in Abstractive Proposition Segmentation.
Convert the following text: {{text}}
"""

[summary] 
prompt = "Create a comprehensive summary: {{text}}"
```

Use with CLI:
```bash
lmstrix infer aps --file_prompt adam.toml --text "Your text here"
lmstrix infer summary --file_prompt adam.toml --text_file document.txt
```

## Development

```bash
# Clone repository
git clone https://github.com/twardoch/lmstrix.git
cd lmstrix

# Install for development
pip install -e ".[dev]"

# Run tests
pytest

# Run linting
hatch run lint:all
```

## Project Structure

```
src/lmstrix/
├── cli/main.py              # CLI interface
├── core/
│   ├── inference_manager.py # Unified inference engine
│   ├── models.py            # Model registry
│   └── context_tester.py    # Context limit testing
├── api/client.py            # LM Studio API client
├── loaders/                 # Data loading utilities
└── utils/                   # Helper utilities
```

## Features in Detail

### Adaptive Context Optimizer
- Binary search algorithm for efficient context limit discovery
- Safety thresholds to prevent system crashes
- Automatic persistence of test results
- Resume capability for interrupted tests

### Enhanced Logging
- Beautiful emoji-rich output in verbose mode
- Comprehensive inference statistics
- Progress indicators for long operations
- Clear error messages with context

### Smart Model Management
- Automatic model discovery from LM Studio
- Persistent registry with JSON storage
- Model state tracking (loaded/unloaded)
- Batch operations for multiple models

## Requirements

- Python 3.11+
- LM Studio installed and configured
- Models downloaded in LM Studio

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions welcome! Please read our contributing guidelines and submit pull requests for any improvements.
</document_content>
</document>

<document index="11">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
├── conftest.py              # Shared fixtures and configuration
├── run_tests.py             # Simple test runner script
├── test_api/                # API layer tests
│   ├── test_client.py       # LMStudioClient tests
│   └── test_exceptions.py   # Custom exception tests
├── test_core/               # Core module tests
│   ├── test_context_tester.py  # Context optimization tests
│   ├── test_inference.py    # Inference engine tests
│   ├── test_models.py       # Model and registry tests
│   ├── test_prompts.py      # Prompt resolution tests
│   └── test_scanner.py      # Model scanner tests
├── test_loaders/            # Loader tests
│   ├── test_context_loader.py   # Context file loading tests
│   ├── test_model_loader.py     # Model loader tests
│   └── test_prompt_loader.py    # Prompt loader tests
├── test_utils/              # Utility tests
│   └── test_paths.py        # Path utility tests
├── test_integration/        # Integration tests
│   └── test_cli_integration.py  # CLI integration tests
└── test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="12">
<source>TODO.md</source>
<document_content>
# LMStrix TODO List

### Issue #307: Streaming Inference Support 

- [ ] Write unit tests for streaming functionality
- [ ] Write integration tests with real models
- [ ] Update documentation with streaming examples
- [ ] Performance testing: streaming vs sync comparison

### Testing and Validation

- [ ] Compare output with LM Studio for identical prompts
- [ ] Ensure token counts match between lmstrix and LM Studio
- [ ] Verify translation quality matches expected output
- [ ] Create regression test for this issue

### Model Loading Optimization (Medium Priority)

- [ ] Improve model reuse detection to avoid unnecessary loading messages
- [ ] Add context length display in enhanced logging when available
- [ ] Optimize model loading workflow for better performance
- [ ] Add better feedback when models are being reused vs loaded fresh

## Phase A: Core Simplification

### Configuration Unification

- [ ] Create utils/config.py for centralized configuration handling
- [ ] Consolidate path handling functions (get_lmstudio_path, etc.)
- [ ] Remove redundant configuration code
- [ ] Update all imports to use centralized config

### Error Handling Standardization

- [ ] Review and simplify custom exception hierarchy
- [ ] Standardize error messages across codebase
- [ ] Implement consistent logging patterns
- [ ] Update error handling to use standard exceptions where appropriate

### Code Quality Improvements

- [ ] Add comprehensive type hints to public APIs
- [ ] Ensure all functions have proper docstrings
- [ ] Remove deprecated TODO comments from code
- [ ] Run code quality checks and fix issues

## Phase B: CLI Enhancement

### Command Improvements

- [ ] Enhance `scan` command with better progress reporting
- [ ] Improve `list` command with filtering and sorting options
- [ ] Add `reset` command for clearing model test data
- [ ] Add `health` command for system diagnostics

### User Experience

- [ ] Better error messages with helpful suggestions
- [ ] Improved help text and documentation
- [ ] Enhanced progress indicators for long-running operations
- [ ] Add command aliases for common operations

## Phase C: Testing & Documentation

### Test Suite Completion

- [ ] Ensure >90% test coverage maintained
- [ ] Add integration tests for new features
- [ ] Performance benchmarking of improvements
- [ ] Add regression tests for fixed issues

### Documentation Updates

- [ ] Update README.md with latest features
- [ ] Create comprehensive CLI reference
- [ ] Update examples to demonstrate new capabilities
- [ ] Create migration guide for any breaking changes

## Technical Debt Reduction

### Code Architecture

- [ ] Review and simplify InferenceManager class structure
- [ ] Consolidate duplicate logic across modules
- [ ] Improve separation of concerns between CLI and core logic
- [ ] Refactor overly complex functions

### Performance Optimization

- [ ] Profile model loading and caching behavior
- [ ] Optimize JSON registry read/write operations
- [ ] Reduce memory usage in context testing
- [ ] Benchmark before/after performance improvements

### Dependency Management

- [ ] Review and minimize external dependencies
- [ ] Ensure compatibility with latest Python versions
- [ ] Update build and packaging configuration
- [ ] Test installation on clean environments

## Quality Assurance

### Testing

- [ ] Run full test suite on all changes
- [ ] Test CLI commands with various model types
- [ ] Verify backward compatibility
- [ ] Performance regression testing

### Documentation

- [ ] Update CHANGELOG.md with all changes
- [ ] Review and update docstrings
- [ ] Ensure examples work correctly
- [ ] Update any configuration documentation

### Release Preparation

- [ ] Version bump and release notes
- [ ] Tag release in git
- [ ] Test PyPI package build
- [ ] Verify clean installation works

## NEW DEVELOPMENT PRIORITIES

### Issue #307 - LM Studio Streaming Inference (High Priority)
- [ ] Research LM Studio SDK streaming API (`complete_stream()`, callbacks)
- [ ] Replace `llm.complete()` with `llm.complete_stream()` in `LMStudioClient.completion()`
- [ ] Implement progress callbacks: `on_prediction_fragment`, `on_first_token`
- [ ] Add no-progress timeout watchdog to abort stalled generations
- [ ] Add CLI flags: `--stream-timeout` (default 120s), `--show-progress`
- [ ] Update inference logging to show streaming status
- [ ] Implement token-by-token display in verbose mode
- [ ] Add streaming statistics (tokens/second, time to first token)
- [ ] Enable streaming cancellation via Ctrl+C
- [ ] Maintain backward compatibility with existing response format

### Issue #306 - Batch Inference Tool (Medium Priority)
- [ ] Create `_keep_this/adam/adamall.py` using lmstrix API
- [ ] Implement smart model state detection and caching
- [ ] Add pathvalidate for safe filename generation (`safe_model_id`, `safe_prompt_name`)
- [ ] Configure batch prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`
- [ ] Implement file existence checking to skip completed inference
- [ ] Generate output paths: `f"_keep_this/adam/out/{safe_prompt_name}--{safe_model_id}.txt"`
- [ ] Add error message capture to output files on failure
- [ ] Sort models by "smart" method (as in `lmstrix list`)
- [ ] Load models with 50% input context, inference with 90% max context
- [ ] Minimize model loading/unloading through intelligent scheduling
- [ ] Use logger from `lmstrix.utils.logging`
- [ ] Process text from `_keep_this/adam/fontlab8.md`

</document_content>
</document>

<document index="13">
<source>WORK.md</source>
<document_content>
# Current Work Progress

## Recently Completed Work

### Issue #307 - Streaming Inference Support ✅

#### What was done:
1. **Added streaming support to LMStudioClient** (`src/lmstrix/api/client.py`):
   - Implemented `stream_completion()` method using lmstudio SDK's `complete_stream()`
   - Added token-by-token callbacks with `on_prediction_fragment` and `on_first_token`
   - Implemented timeout watchdog (default 120s) to detect stalled generations
   - Added streaming statistics (tokens/second, time to first token)

2. **Extended InferenceEngine and InferenceManager** with streaming:
   - Added `stream_infer()` method to both classes
   - Maintains same model loading/reuse logic as regular inference
   - Supports all existing parameters plus streaming-specific ones

3. **Updated CLI with --stream flag**:
   - Added `--stream` and `--stream-timeout` parameters to `infer` command
   - Tokens are displayed in real-time to stdout as they are generated
   - Maintains backward compatibility - regular inference still works

#### How to use:
```bash
# Regular inference (blocking)
lmstrix infer "Hello world" -m model-id

# Streaming inference (real-time)
lmstrix infer "Hello world" -m model-id --stream

# With custom timeout
lmstrix infer "Hello world" -m model-id --stream --stream-timeout 180
```

### Issue #306 - Batch Processing Tool ✅

#### What was done:
1. **Created adamall.py batch processing tool** (`_keep_this/adam/adamall.py`):
   - Processes 6 specific prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`
   - Smart model management - reuses loaded models when possible
   - Skips existing outputs for resumable processing
   - Safe filename generation using pathvalidate
   - Error capture to output files on failure

2. **Key features implemented**:
   - Loads models with 50% context, runs inference with 90% of max context
   - Sorts models by size (descending) for optimal processing order
   - Progress tracking with percentage and ETA
   - Comprehensive error handling and logging

#### How to use:
```bash
cd _keep_this/adam
python adamall.py
```

Output files will be generated in `_keep_this/adam/out/` with names like:
- `think_aps--model_name.txt`
- `translate--model_name.txt`

## ACTIVE: Issue #302 - Fix Inference Output Mismatch

### Problem Summary
When running the same translation prompt:
- **LM Studio GUI**: Produces proper Polish translation (639 tokens)
- **lmstrix CLI**: Only outputs `</translate>` (4 tokens)

### Root Cause Analysis
Found several configuration differences:
1. Temperature: GUI=0.8, CLI=0.7 → Updated default to 0.8 ✅
2. top_k: GUI=20, CLI=40 → Now configurable via CLI ✅
3. Context: GUI=131072, CLI=65536 (reduced)
4. max_predict: GUI=-1, CLI=117964
5. Stop tokens configuration may differ

### Current Work Items

#### 1. Add Diagnostic Logging ✅
- [x] Log exact prompt with escape sequences visible
- [x] Log all inference parameters in detail
- [x] Add comparison with LM Studio defaults
- [x] Log stop token configuration

#### 2. Parameter Alignment
- [x] Change default temperature to 0.8
- [x] Add CLI flags for inference parameters
- [ ] Fix maxTokens calculation
- [ ] Add stop token configuration

#### 3. Context Length Fix
- [ ] Fix context reduction issue
- [ ] Use full model context by default
- [ ] Add warning for context reduction

#### 4. Testing
- [ ] Compare with LM Studio output
- [ ] Verify token counts match
- [ ] Test translation quality

### Implementation Progress
Added diagnostic logging and streaming support. Need to investigate stop token issue next...

### Test Suite Fixes (Priority 0) ✅
All critical AttributeError issues have been resolved:
1. **Model.validate_integrity()** ✅
2. **PromptResolver methods** ✅  
3. **ContextTester methods** ✅
4. **ModelScanner.sync_with_registry()** ✅

### Issues 201-204 ✅
- Model persistence between calls
- Beautiful enhanced logging
- Fixed model lookup for paths/IDs
- Comprehensive inference statistics

## Next Steps After Issue #302
1. Complete remaining test fixes
2. Issue #105 - Adam.toml simplification
3. Context testing streamlining
4. Model loading optimization
</document_content>
</document>

<document index="14">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="15">
<source>cleanup.txt</source>
<document_content>
::error title=Ruff (A002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py,line=40,col=9,endLine=40,endColumn=12::src/lmstrix/__main__.py:40:9: A002 Function argument `all` is shadowing a Python builtin
::error title=Ruff (A002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py,line=83,col=9,endLine=83,endColumn=13::src/lmstrix/__main__.py:83:9: A002 Function argument `dict` is shadowing a Python builtin
::error title=Ruff (TRY301),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=47,col=21,endLine=47,endColumn=96::src/lmstrix/utils/context_parser.py:47:21: TRY301 Abstract `raise` to an inner function
::error title=Ruff (TRY003),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=47,col=27,endLine=47,endColumn=96::src/lmstrix/utils/context_parser.py:47:27: TRY003 Avoid specifying long messages outside the exception class
::error title=Ruff (TRY301),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=53,col=21,endLine=55,endColumn=22::src/lmstrix/utils/context_parser.py:53:21: TRY301 Abstract `raise` to an inner function
::error title=Ruff (TRY003),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=53,col=27,endLine=55,endColumn=22::src/lmstrix/utils/context_parser.py:53:27: TRY003 Avoid specifying long messages outside the exception class
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=64,col=17,endLine=64,endColumn=35::src/lmstrix/utils/context_parser.py:64:17: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (TRY003),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=67,col=23,endLine=67,endColumn=80::src/lmstrix/utils/context_parser.py:67:23: TRY003 Avoid specifying long messages outside the exception class
::error title=Ruff (B904),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=73,col=13,endLine=75,endColumn=14::src/lmstrix/utils/context_parser.py:73:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
::error title=Ruff (TRY003),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=73,col=19,endLine=75,endColumn=14::src/lmstrix/utils/context_parser.py:73:19: TRY003 Avoid specifying long messages outside the exception class
::error title=Ruff (TRY003),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=77,col=11,endLine=77,endColumn=81::src/lmstrix/utils/context_parser.py:77:11: TRY003 Avoid specifying long messages outside the exception class
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=3,col=1,endLine=3,endColumn=40::src/lmstrix/utils/state.py:3:1: ERA001 Found commented-out code
::error title=Ruff (PTH123),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=24,col=22,endLine=24,endColumn=26::src/lmstrix/utils/state.py:24:22: PTH123 `open()` should be replaced by `Path.open()`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=26,col=20,endLine=26,endColumn=29::src/lmstrix/utils/state.py:26:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PTH123),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=33,col=18,endLine=33,endColumn=22::src/lmstrix/utils/state.py:33:18: PTH123 `open()` should be replaced by `Path.open()`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=35,col=16,endLine=35,endColumn=25::src/lmstrix/utils/state.py:35:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_streaming.py,line=47,col=12,endLine=47,endColumn=21::test_streaming.py:47:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=63,col=9,endLine=63,endColumn=25::src/lmstrix/loaders/model_loader.py:63:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (PLR0911),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=69,col=5,endLine=69,endColumn=31::src/lmstrix/loaders/model_loader.py:69:5: PLR0911 Too many return statements (8 > 6)
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=105,col=9,endLine=105,endColumn=20::src/lmstrix/loaders/model_loader.py:105:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=185,col=9,endLine=185,endColumn=25::src/lmstrix/loaders/model_loader.py:185:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=205,col=5,endLine=205,endColumn=29::src/lmstrix/loaders/model_loader.py:205:5: PLR0912 Too many branches (15 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=205,col=5,endLine=205,endColumn=29::src/lmstrix/loaders/model_loader.py:205:5: PLR0915 Too many statements (51 > 50)
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=346,col=9,endLine=346,endColumn=20::src/lmstrix/loaders/model_loader.py:346:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py,line=72,col=12,endLine=72,endColumn=21::src/lmstrix/loaders/prompt_loader.py:72:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py,line=90,col=5,endLine=90,endColumn=23::src/lmstrix/loaders/prompt_loader.py:90:5: PLR0912 Too many branches (14 > 12)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py,line=143,col=12,endLine=143,endColumn=21::src/lmstrix/loaders/prompt_loader.py:143:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py,line=221,col=9,endLine=221,endColumn=23::src/lmstrix/loaders/prompt_loader.py:221:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py,line=226,col=13,endLine=226,endColumn=27::src/lmstrix/loaders/prompt_loader.py:226:13: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py,line=231,col=13,endLine=231,endColumn=24::src/lmstrix/loaders/prompt_loader.py:231:13: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=2,col=1,endLine=2,endColumn=49::examples/python/custom_inference.py:2:1: ERA001 Found commented-out code
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=13,col=5,endLine=13,endColumn=9::examples/python/custom_inference.py:13:5: PLR0915 Too many statements (89 > 50)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=60,col=12,endLine=60,endColumn=21::examples/python/custom_inference.py:60:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=68,col=12,endLine=68,endColumn=21::examples/python/custom_inference.py:68:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=94,col=16,endLine=94,endColumn=25::examples/python/custom_inference.py:94:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=109,col=16,endLine=109,endColumn=25::examples/python/custom_inference.py:109:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=137,col=12,endLine=137,endColumn=21::examples/python/custom_inference.py:137:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=149,col=12,endLine=149,endColumn=21::examples/python/custom_inference.py:149:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py,line=165,col=12,endLine=165,endColumn=21::examples/python/custom_inference.py:165:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py,line=2,col=1,endLine=2,endColumn=49::examples/python/batch_processing.py:2:1: ERA001 Found commented-out code
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py,line=12,col=5,endLine=12,endColumn=9::examples/python/batch_processing.py:12:5: PLR0912 Too many branches (16 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py,line=12,col=5,endLine=12,endColumn=9::examples/python/batch_processing.py:12:5: PLR0915 Too many statements (86 > 50)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py,line=65,col=20,endLine=65,endColumn=29::examples/python/batch_processing.py:65:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py,line=108,col=16,endLine=108,endColumn=25::examples/python/batch_processing.py:108:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=2,col=1,endLine=2,endColumn=54::examples/python/prompt_templates_demo.py:2:1: ERA001 Found commented-out code
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=15,col=5,endLine=15,endColumn=9::examples/python/prompt_templates_demo.py:15:5: PLR0915 Too many statements (86 > 50)
::error title=Ruff (PTH123),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=53,col=10,endLine=53,endColumn=14::examples/python/prompt_templates_demo.py:53:10: PTH123 `open()` should be replaced by `Path.open()`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=75,col=12,endLine=75,endColumn=21::examples/python/prompt_templates_demo.py:75:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=113,col=12,endLine=113,endColumn=21::examples/python/prompt_templates_demo.py:113:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=131,col=12,endLine=131,endColumn=21::examples/python/prompt_templates_demo.py:131:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py,line=16,col=1,endLine=16,endColumn=38::examples/specialized/elo_liczby.py:16:1: ERA001 Found commented-out code
::error title=Ruff (PLR0911),file=/Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py,line=46,col=5,endLine=46,endColumn=28::examples/specialized/elo_liczby.py:46:5: PLR0911 Too many return statements (9 > 6)
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py,line=46,col=5,endLine=46,endColumn=28::examples/specialized/elo_liczby.py:46:5: PLR0912 Too many branches (24 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py,line=46,col=5,endLine=46,endColumn=28::examples/specialized/elo_liczby.py:46:5: PLR0915 Too many statements (78 > 50)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py,line=170,col=16,endLine=170,endColumn=25::examples/specialized/elo_liczby.py:170:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py,line=189,col=12,endLine=189,endColumn=21::examples/specialized/elo_liczby.py:189:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=2,col=1,endLine=2,endColumn=49::examples/python/advanced_testing.py:2:1: ERA001 Found commented-out code
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=10,col=5,endLine=10,endColumn=9::examples/python/advanced_testing.py:10:5: PLR0912 Too many branches (21 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=10,col=5,endLine=10,endColumn=9::examples/python/advanced_testing.py:10:5: PLR0915 Too many statements (92 > 50)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=46,col=12,endLine=46,endColumn=21::examples/python/advanced_testing.py:46:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=61,col=12,endLine=61,endColumn=21::examples/python/advanced_testing.py:61:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=71,col=12,endLine=71,endColumn=21::examples/python/advanced_testing.py:71:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=83,col=12,endLine=83,endColumn=21::examples/python/advanced_testing.py:83:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py,line=99,col=12,endLine=99,endColumn=21::examples/python/advanced_testing.py:99:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py,line=3,col=1,endLine=3,endColumn=49::src/lmstrix/core/concrete_config.py:3:1: ERA001 Found commented-out code
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py,line=126,col=13,endLine=126,endColumn=24::src/lmstrix/core/concrete_config.py:126:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=127,col=13,endLine=127,endColumn=19::src/lmstrix/api/client.py:127:13: E722 Do not use bare `except`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=153,col=24,endLine=153,endColumn=33::src/lmstrix/api/client.py:153:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=155,col=13,endLine=155,endColumn=31::src/lmstrix/api/client.py:155:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=156,col=16,endLine=156,endColumn=25::src/lmstrix/api/client.py:156:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=184,col=9,endLine=184,endColumn=19::src/lmstrix/api/client.py:184:9: PLR0912 Too many branches (26 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=184,col=9,endLine=184,endColumn=19::src/lmstrix/api/client.py:184:9: PLR0915 Too many statements (91 > 50)
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=198,col=11,endLine=198,endColumn=17::src/lmstrix/api/client.py:198:11: ARG002 Unused method argument: `kwargs`
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=376,col=9,endLine=376,endColumn=26::src/lmstrix/api/client.py:376:9: PLR0915 Too many statements (77 > 50)
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=383,col=9,endLine=383,endColumn=29::src/lmstrix/api/client.py:383:9: ARG002 Unused method argument: `model_context_length`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=392,col=11,endLine=392,endColumn=17::src/lmstrix/api/client.py:392:11: ARG002 Unused method argument: `kwargs`
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=31,col=5,endLine=31,endColumn=24::src/lmstrix/api/main.py:31:5: PLR0912 Too many branches (16 > 12)
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=330,col=9,endLine=330,endColumn=20::src/lmstrix/api/main.py:330:9: PLR0912 Too many branches (16 > 12)
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=438,col=9,endLine=438,endColumn=20::src/lmstrix/api/main.py:438:9: PLR0912 Too many branches (13 > 12)
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=445,col=9,endLine=445,endColumn=13::src/lmstrix/api/main.py:445:9: ARG002 Unused method argument: `sort`
::error title=Ruff (PLR0911),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=547,col=9,endLine=547,endColumn=22::src/lmstrix/api/main.py:547:9: PLR0911 Too many return statements (10 > 6)
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=547,col=9,endLine=547,endColumn=22::src/lmstrix/api/main.py:547:9: PLR0912 Too many branches (54 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=547,col=9,endLine=547,endColumn=22::src/lmstrix/api/main.py:547:9: PLR0915 Too many statements (139 > 50)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=582,col=24,endLine=582,endColumn=33::src/lmstrix/api/main.py:582:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (SIM108),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=596,col=17,endLine=602,endColumn=51::src/lmstrix/api/main.py:596:17: SIM108 Use ternary operator `pairs = dict_params.split(",") if "," in dict_params else dict_params.split(",")` instead of `if`-`else`-block
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=597,col=21,endLine=597,endColumn=56::src/lmstrix/api/main.py:597:21: ERA001 Found commented-out code
::error title=Ruff (PLW2901),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=605,col=21,endLine=605,endColumn=25::src/lmstrix/api/main.py:605:21: PLW2901 `for` loop variable `pair` overwritten by assignment target
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=622,col=24,endLine=622,endColumn=33::src/lmstrix/api/main.py:622:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=705,col=20,endLine=705,endColumn=29::src/lmstrix/api/main.py:705:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=808,col=20,endLine=808,endColumn=29::src/lmstrix/api/main.py:808:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=850,col=9,endLine=850,endColumn=21::src/lmstrix/api/main.py:850:9: PLR0912 Too many branches (14 > 12)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=950,col=16,endLine=950,endColumn=25::src/lmstrix/api/main.py:950:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py,line=2,col=1,endLine=2,endColumn=44::examples/python/basic_usage.py:2:1: ERA001 Found commented-out code
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py,line=87,col=12,endLine=87,endColumn=21::examples/python/basic_usage.py:87:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py,line=103,col=12,endLine=103,endColumn=21::examples/python/basic_usage.py:103:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=99,col=13,endLine=99,endColumn=46::src/lmstrix/core/inference.py:99:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=101,col=16,endLine=101,endColumn=25::src/lmstrix/core/inference.py:101:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=109,col=24,endLine=109,endColumn=33::src/lmstrix/core/inference.py:109:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=160,col=9,endLine=160,endColumn=14::src/lmstrix/core/inference.py:160:9: PLR0912 Too many branches (27 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=160,col=9,endLine=160,endColumn=14::src/lmstrix/core/inference.py:160:9: PLR0915 Too many statements (103 > 50)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=211,col=20,endLine=211,endColumn=29::src/lmstrix/core/inference.py:211:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=316,col=16,endLine=316,endColumn=25::src/lmstrix/core/inference.py:316:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=353,col=29,endLine=353,endColumn=35::src/lmstrix/core/inference.py:353:29: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=353,col=29,endLine=354,endColumn=37::src/lmstrix/core/inference.py:353:29: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=378,col=28,endLine=378,endColumn=37::src/lmstrix/core/inference.py:378:28: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=406,col=9,endLine=406,endColumn=21::src/lmstrix/core/inference.py:406:9: PLR0912 Too many branches (21 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=406,col=9,endLine=406,endColumn=21::src/lmstrix/core/inference.py:406:9: PLR0915 Too many statements (75 > 50)
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=461,col=20,endLine=461,endColumn=29::src/lmstrix/core/inference.py:461:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=3,col=1,endLine=3,endColumn=47::src/lmstrix/core/models.py:3:1: ERA001 Found commented-out code
::error title=Ruff (A002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=28,col=9,endLine=28,endColumn=11::src/lmstrix/core/models.py:28:9: A002 Function argument `id` is shadowing a Python builtin
::error title=Ruff (SIM102),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=140,col=9,endLine=142,endColumn=40::src/lmstrix/core/models.py:140:9: SIM102 Use a single `if` statement instead of nested `if` statements
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=171,col=9,endLine=171,endColumn=18::src/lmstrix/core/models.py:171:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=194,col=9,endLine=194,endColumn=64::src/lmstrix/core/models.py:194:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=219,col=24,endLine=219,endColumn=33::src/lmstrix/core/models.py:219:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=225,col=16,endLine=225,endColumn=25::src/lmstrix/core/models.py:225:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ANN001),file=/Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py,line=47,col=75,endLine=47,endColumn=81::tests/test_integration/test_cli_integration.py:47:75: ANN001 Missing type annotation for function argument `capsys`
::error title=Ruff (ANN001),file=/Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py,line=59,col=79,endLine=59,endColumn=85::tests/test_integration/test_cli_integration.py:59:79: ANN001 Missing type annotation for function argument `capsys`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=7,col=1,endLine=7,endColumn=51::src/lmstrix/core/inference_manager.py:7:1: ERA001 Found commented-out code
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=46,col=9,endLine=46,endColumn=14::src/lmstrix/core/inference_manager.py:46:9: PLR0912 Too many branches (20 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=46,col=9,endLine=46,endColumn=14::src/lmstrix/core/inference_manager.py:46:9: PLR0915 Too many statements (73 > 50)
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=100,col=21,endLine=100,endColumn=36::src/lmstrix/core/inference_manager.py:100:21: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=104,col=24,endLine=104,endColumn=33::src/lmstrix/core/inference_manager.py:104:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=121,col=21,endLine=121,endColumn=36::src/lmstrix/core/inference_manager.py:121:21: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=131,col=17,endLine=131,endColumn=32::src/lmstrix/core/inference_manager.py:131:17: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=174,col=24,endLine=174,endColumn=33::src/lmstrix/core/inference_manager.py:174:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=196,col=13,endLine=204,endColumn=14::src/lmstrix/core/inference_manager.py:196:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=206,col=16,endLine=206,endColumn=25::src/lmstrix/core/inference_manager.py:206:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=251,col=24,endLine=251,endColumn=33::src/lmstrix/core/inference_manager.py:251:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=310,col=13,endLine=310,endColumn=46::src/lmstrix/core/inference_manager.py:310:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=312,col=16,endLine=312,endColumn=25::src/lmstrix/core/inference_manager.py:312:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=319,col=24,endLine=319,endColumn=33::src/lmstrix/core/inference_manager.py:319:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLR0912),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=322,col=9,endLine=322,endColumn=21::src/lmstrix/core/inference_manager.py:322:9: PLR0912 Too many branches (17 > 12)
::error title=Ruff (PLR0915),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=322,col=9,endLine=322,endColumn=21::src/lmstrix/core/inference_manager.py:322:9: PLR0915 Too many statements (59 > 50)
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=373,col=21,endLine=373,endColumn=36::src/lmstrix/core/inference_manager.py:373:21: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=377,col=24,endLine=377,endColumn=33::src/lmstrix/core/inference_manager.py:377:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=394,col=21,endLine=394,endColumn=36::src/lmstrix/core/inference_manager.py:394:21: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=404,col=17,endLine=404,endColumn=32::src/lmstrix/core/inference_manager.py:404:17: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=447,col=24,endLine=447,endColumn=33::src/lmstrix/core/inference_manager.py:447:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=104,col=9,endLine=104,endColumn=17::src/lmstrix/core/context_tester.py:104:9: ARG002 Unused method argument: `log_path`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=105,col=9,endLine=105,endColumn=14::src/lmstrix/core/context_tester.py:105:9: ARG002 Unused method argument: `model`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=106,col=9,endLine=106,endColumn=17::src/lmstrix/core/context_tester.py:106:9: ARG002 Unused method argument: `registry`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=135,col=16,endLine=135,endColumn=25::src/lmstrix/core/context_tester.py:135:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ANN001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=145,col=35,endLine=145,endColumn=40::src/lmstrix/core/context_tester.py:145:35: ANN001 Missing type annotation for function argument `model`
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged

</document_content>
</document>

<document index="16">
<source>docs/_config.yml</source>
<document_content>
title: LMStrix
description: The Unofficial Toolkit for Mastering LM Studio
remote_theme: just-the-docs/just-the-docs

url: https://twardoch.github.io/lmstrix

color_scheme: dark

search_enabled: false

aux_links:
  "LMStrix on GitHub":
    - "https://github.com/twardoch/lmstrix"

plugins:
  - jekyll-remote-theme
</document_content>
</document>

<document index="17">
<source>docs/api.md</source>
<document_content>
---
title: API Reference
---

## Python API Reference

This section provides a detailed reference for the LMStrix Python API.

### The `LMStrix` Class

The main entry point for interacting with the API.

`LMStrix(base_uri: str = "http://localhost:1234/v1")`

- `base_uri`: The base URI of the LM Studio server.

#### Methods

- `async scan_models()`: Scans for available models in LM Studio and updates the local model registry.
- `async list_models()`: Returns a list of `Model` objects from the local registry.
- `async test_model(model_id: str)`: Tests the context limit of a specific model and updates the registry with the result.
- `async infer(prompt: str, model_id: str, **kwargs)`: Runs inference on a model. Any additional keyword arguments are passed to the `complete()` method of the `lmstudio` client.

### The `Model` Class

Represents a model in LM Studio.

#### Attributes

- `id`: The model ID (e.g., `lmstudio-community/gemma-2b-it-GGUF`).
- `context_limit`: The declared context limit of the model, as reported by LM Studio.
- `tested_max_context`: The empirically tested maximum context limit that the model can handle on your machine. `None` if the model has not been tested.
- `context_test_status`: The status of the context test. Can be one of `"passed"`, `"failed"`, or `"not_tested"`.

### The `InferenceResponse` Class

Represents the response from an inference request.

#### Attributes

- `content`: The text content of the model's response.
- `usage`: A dictionary containing token usage information, e.g., `{'prompt_tokens': 10, 'completion_tokens': 20, 'total_tokens': 30}`.

</document_content>
</document>

<document index="18">
<source>docs/how-it-works.md</source>
<document_content>
---
title: How It Works
---

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **adaptive testing algorithm** (enhanced in v1.1):

1. **Initial Verification**: Tests at 1,024 tokens to ensure the model loads properly
2. **Threshold Test**: Tests at min(threshold, declared_max) where threshold defaults to 102,400 tokens
   - This prevents system crashes from attempting very large context sizes
3. **Adaptive Search**:
   - If the threshold test succeeds and is below the declared max: incrementally increases by 10,240 tokens until failure
   - If the threshold test fails: performs binary search between 1,024 and the failed size
4. **Progress Tracking**: Saves results after each test, allowing resumption if interrupted

**Batch Testing Optimization** (new in v1.1):
When testing multiple models with `--all`, LMStrix now:
- Sorts models by declared context size (ascending)
- Tests in passes to minimize model loading/unloading
- Excludes failed models from subsequent passes
- Provides detailed progress with Rich table output

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

### The `test` Command

The `test` command is the heart of the context optimization process. When you run `lmstrix test <model-id>`, it performs the binary search algorithm described above.

The command saves the results of the test to a local registry, so you only need to test each model once.

</document_content>
</document>

<document index="19">
<source>docs/index.md</source>
<document_content>
---
---

# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

</document_content>
</document>

<document index="20">
<source>docs/installation.md</source>
<document_content>
---
title: Installation
---

## Installation

### Requirements

- Python 3.10 or higher
- [LM Studio](https://lmstudio.ai/) installed and running locally
- At least one model downloaded in LM Studio

### Using pip

```bash
pip install lmstrix
```

### Using uv (recommended)

```bash
uv pip install lmstrix
```

### For Development

If you want to contribute to LMStrix or install it in an editable mode, you can clone the repository:

```bash
git clone https://github.com/twardoch/lmstrix
cd lmstrix
pip install -e .[dev]
```

</document_content>
</document>

<document index="21">
<source>docs/usage.md</source>
<document_content>
---
title: Usage
---

## Command-Line Interface (CLI)

LMStrix provides a powerful and intuitive CLI for interacting with your local models.

### Scanning for Models

Before you can use LMStrix, you need to scan for available models in LM Studio. This command discovers all models that you have downloaded.

```bash
lmstrix scan
```

### Listing Models

To see a list of all discovered models, their context length, and test status, use the `list` command.

```bash
lmstrix list
```

### Testing Context Limits

This is the core feature of LMStrix. The `test` command automatically determines the maximum context window a model can handle on your machine.

```bash
# Test a specific model by its ID
lmstrix test "model-id-here"

# Test all models that haven't been tested yet
lmstrix test --all

# Test with a custom threshold (default: 102,400 tokens)
# This prevents system crashes by limiting the maximum initial test size
lmstrix test "model-id-here" --threshold 51200

# Test all models with a lower threshold for safety
lmstrix test --all --threshold 32768
```

**New in v1.1**: The `--threshold` parameter (default: 102,400 tokens) prevents system crashes when testing models with very large declared context sizes. The testing algorithm now:
1. Tests at 1,024 tokens to verify the model loads
2. Tests at min(threshold, declared_max)
3. If successful and below declared max, increments by 10,240 tokens
4. If failed, performs binary search to find the exact limit

For more details on how this works, see the [How It Works](./how-it-works.md) page.

### Running Inference

You can run inference directly from the command line.

```bash
# Run a simple prompt
lmstrix infer "Your prompt here" --model "model-id" --max-tokens 150

# Use a prompt from a file
lmstrix infer "@prompts.toml:greeting" --model "model-id"
```

### Verbose Mode

For more detailed output and debugging, you can use the `--verbose` flag with any command.

```bash
lmstrix scan --verbose
```

## Python API

The Python API provides a clean, `async`-first interface for programmatic access to LMStrix features.

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    lms = LMStrix()
    
    # Scan for available models
    await lms.scan_models()
    
    # List all models
    models = await lms.list_models()
    for model in models:
        print(f"Model: {model.id}")
        print(f"  Context limit: {model.context_limit:,} tokens")
        print(f"  Tested limit: {model.tested_max_context or 'Not tested'}")
        print(f"  Status: {model.context_test_status}")
    
    # Test a specific model's context limits
    model_id = models[0].id if models else None
    if model_id:
        print(f"\nTesting context limits for {model_id}...")
        result = await lms.test_model(model_id)
        print(f"Optimal context: {result.tested_max_context} tokens")
        print(f"Test status: {result.context_test_status}")
    
    # Run inference
    if model_id:
        response = await lms.infer(
            prompt="What is the meaning of life?",
            model_id=model_id,
            max_tokens=100
        )
        print(f"\nInference result:\n{response.content}")

if __name__ == "__main__":
    asyncio.run(main())
```

</document_content>
</document>

<document index="22">
<source>examples/README.md</source>
<document_content>
# LMStrix Usage Examples

This directory contains comprehensive examples demonstrating all features of the LMStrix CLI and Python API, including the latest context control, prompt templates, and model state management features.

## Prerequisites

1. **LMStrix Installed**: Ensure you have installed LMStrix (`pip install lmstrix`)
2. **LM Studio Running**: Most examples require LM Studio running with at least one model downloaded
3. **Model Downloaded**: Download models in LM Studio (e.g., Llama, Mistral, Phi, Qwen)

**Note**: Examples use "llama" as a placeholder model ID. Update this to match your downloaded models. Run `lmstrix list` to see available models.

## Quick Start

Run all examples at once:
```bash
bash run_all_examples.sh
```

Or run individual examples as shown below.

---

## CLI Examples (`cli/`)

### Core Workflow Examples

#### `basic_workflow.sh`
Complete end-to-end workflow demonstrating:
- Model scanning with verbose output
- Listing models with `--sort` and `--show` options
- Context testing with new fixed strategy
- Inference with `--in_ctx` and `--out_ctx` parameters
- Model state reuse demonstration

```bash
bash cli/basic_workflow.sh
```

#### `model_testing.sh`
Advanced context testing features:
- Fixed context testing strategy (30k, 40k, 60k, etc.)
- `--threshold` parameter for safety limits
- `--fast` mode for quick validation
- `--ctx` for specific context testing
- Batch testing with `--all`
- Test resumption capabilities

```bash
bash cli/model_testing.sh
```

#### `inference_examples.sh`
Comprehensive inference scenarios:
- Context control with `--in_ctx` and `--out_ctx`
- Load without context specification (`--in_ctx 0`)
- Model state detection and reuse
- `--force-reload` demonstration
- TOML prompt templates with `--file_prompt` and `--dict`
- Temperature control for creativity

```bash
bash cli/inference_examples.sh
```

### New Feature Examples

#### `context_control_examples.sh` *(NEW)*
Deep dive into context management:
- Understanding `--in_ctx` vs `--out_ctx`
- Memory-conscious loading strategies
- Context size performance impact
- Long document processing
- Optimal context selection

```bash
bash cli/context_control_examples.sh
```

#### `model_state_demo.sh` *(NEW)*
Model state detection and management:
- How model reuse works
- Performance comparison (load vs reuse)
- Force reload scenarios
- Model switching strategies
- Context change behavior

```bash
bash cli/model_state_demo.sh
```

---

## Python API Examples (`python/`)

### Core API Usage

#### `basic_usage.py`
Fundamentals of the LMStrix Python API:
- Initializing `LMStrix` client
- Scanning and listing models
- Basic inference with `out_ctx`
- Advanced inference with `in_ctx`
- Model state detection
- Error handling

```bash
python3 python/basic_usage.py
```

#### `advanced_testing.py`
Context testing programmatically:
- Fixed context testing strategy
- Fast mode testing
- Custom threshold limits
- Specific context testing
- Batch testing multiple models
- Test result analysis

```bash
python3 python/advanced_testing.py
```

#### `custom_inference.py`
Advanced inference techniques:
- Context control (`in_ctx` and `out_ctx`)
- Temperature adjustment
- TOML prompt template loading
- Structured JSON output
- Model state reuse
- Force reload scenarios

```bash
python3 python/custom_inference.py
```

#### `batch_processing.py`
Working with multiple models:
- Batch testing strategies
- Model response comparison
- Performance benchmarking
- Efficiency analysis
- Smart testing order

```bash
python3 python/batch_processing.py
```

### New Python Examples

#### `prompt_templates_demo.py` *(NEW)*
Advanced prompt template features:
- Creating prompts programmatically
- Nested placeholder resolution
- Loading from TOML files
- Batch prompt resolution
- Missing placeholder handling
- Context injection with truncation

```bash
python3 python/prompt_templates_demo.py
```

---

## Prompt Templates (`prompts/`)

### Main Template File

#### `prompts.toml`
Comprehensive prompt template examples:
- Greetings (formal, casual, professional)
- Templates with internal references
- Code review and explanation prompts
- Research summarization templates
- Math and science prompts
- Creative writing templates
- Q&A formats
- System prompts

### Domain-Specific Templates

- `prompts/analysis.toml` - Data analysis prompts
- `prompts/coding.toml` - Programming assistance
- `prompts/creative.toml` - Creative writing
- `prompts/qa.toml` - Question answering

---

## Data Files (`data/`)

- `sample_context.txt` - Large text for context testing
- `test_questions.json` - Sample Q&A scenarios

---

## Key Features Demonstrated

### Context Management
- **`--in_ctx`**: Control model loading context size
- **`--out_ctx`**: Control maximum generation tokens
- **Model reuse**: Automatic reuse of loaded models
- **Force reload**: Refresh models with `--force-reload`

### Testing Enhancements
- **Fixed contexts**: Tests at 30k, 40k, 60k, 80k, 100k, 120k
- **Threshold safety**: Limit initial test size
- **Fast mode**: Skip semantic verification
- **Test resumption**: Continue interrupted tests

### Prompt Templates
- **TOML loading**: Load prompts from `.toml` files
- **Placeholders**: Dynamic value substitution
- **Nested references**: Templates can reference other templates
- **Batch resolution**: Process multiple prompts at once

### Performance Features
- **Smart sorting**: Optimal model testing order
- **Model state tracking**: Know when models are loaded
- **Efficiency metrics**: Track tested vs declared ratios
- **Batch operations**: Test or query multiple models

---

## Running Examples

### Individual Examples
```bash
# CLI examples
bash cli/basic_workflow.sh
bash cli/model_testing.sh
bash cli/inference_examples.sh
bash cli/context_control_examples.sh
bash cli/model_state_demo.sh

# Python examples
python3 python/basic_usage.py
python3 python/advanced_testing.py
python3 python/custom_inference.py
python3 python/batch_processing.py
python3 python/prompt_templates_demo.py
```

### All Examples
```bash
bash run_all_examples.sh
```

---

## Tips for Success

1. **Update Model IDs**: Change "llama" to match your downloaded models
2. **Check Model List**: Run `lmstrix list` to see available models
3. **Start Small**: Begin with `basic_workflow.sh` or `basic_usage.py`
4. **Test First**: Run `lmstrix test` to find optimal context for your models
5. **Use Verbose Mode**: Add `--verbose` for detailed output
6. **Monitor Memory**: Larger contexts use more GPU/RAM

---

## Troubleshooting

If examples fail:
1. Ensure LM Studio is running
2. Check you have models downloaded
3. Update model IDs in scripts
4. Verify LMStrix installation: `lmstrix --version`
5. Check available models: `lmstrix list`

For more help, see the main [LMStrix documentation](https://github.com/AdamAdli/lmstrix).
</document_content>
</document>

<document index="23">
<source>examples/cli/basic_workflow.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates the complete, basic workflow of LMStrix.
# 1. Scan for downloaded models.
# 2. List the models found with various display options.
# 3. Test the context length of a specific model.
# 4. Run inference with the tested model using new features.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Basic Workflow Demo ###"

# Step 1: Scan for models
# This command discovers all models downloaded in your LM Studio installation
# and updates the local registry file (lmstrix.json).
echo -e "\n--- Step 1: Scanning for models ---"
lmstrix scan --verbose
echo "Scan complete. Model registry updated."

# Step 2: List models with different display options
# This demonstrates the new --show and --sort options
echo -e "\n--- Step 2a: Listing models (default view) ---"
lmstrix list
echo -e "\n--- Step 2b: Listing models sorted by context size ---"
lmstrix list --sort ctx
echo -e "\n--- Step 2c: Showing just model IDs ---"
lmstrix list --show id
echo "Model listing demonstrations complete."

# Step 3: Test a model's context length
# We'll use a common model identifier that users might have
echo -e "\n--- Step 3: Testing a model's context length ---"
echo "Note: This may take several minutes depending on the model and your hardware."
# Common model patterns users might have:
# "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded
echo "Looking for models matching: $MODEL_ID"
# Show models matching the pattern
lmstrix list --show id | grep -i "$MODEL_ID" || echo "No models found matching '$MODEL_ID'"
echo -e "\nTesting model: $MODEL_ID"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Context test complete."

# Step 4: Run inference with new context control features
# Demonstrates --out_ctx instead of deprecated --max_tokens
echo -e "\n--- Step 4a: Running basic inference ---"
lmstrix infer "What is the capital of France?" "$MODEL_ID" --out_ctx 50

echo -e "\n--- Step 4b: Running inference with specific loading context ---"
lmstrix infer "Explain quantum computing in simple terms." "$MODEL_ID" --in_ctx 4096 --out_ctx 200

echo -e "\n--- Step 4c: Checking if model is already loaded (reuse demo) ---"
lmstrix infer "What is 2+2?" "$MODEL_ID" --out_ctx 10

echo -e "\nInference demonstrations complete."

echo -e "\n### Workflow Demo Finished ###"
</document_content>
</document>

<document index="24">
<source>examples/cli/context_control_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates advanced context control features in LMStrix.
# Shows --in_ctx usage, model state detection, and context optimization.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Context Control Examples ###"

# Replace with a model identifier that matches your downloaded models
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Understanding Context Parameters
echo -e "\n--- Example 1: Understanding Context Parameters ---"
echo "LMStrix has two context parameters:"
echo "  --in_ctx: Controls the context size when LOADING the model"
echo "  --out_ctx: Controls the maximum tokens to GENERATE"
echo -e "\nLet's see them in action..."

# Example 2: Default Context Loading
echo -e "\n--- Example 2: Default Context Loading ---"
echo "When --in_ctx is not specified, LMStrix uses the optimal context:"
lmstrix infer "What is machine learning?" "$MODEL_ID" --out_ctx 50 --verbose
echo "(Model loaded with optimal context based on testing or declared limit)"

# Example 3: Specific Context Loading
echo -e "\n--- Example 3: Loading with Specific Context ---"
echo "Load model with exactly 4096 tokens of context:"
lmstrix infer "Explain neural networks briefly." "$MODEL_ID" --in_ctx 4096 --out_ctx 100 --verbose
echo "(Model loaded with 4096 token context)"

# Example 4: Model State Detection
echo -e "\n--- Example 4: Model State Detection ---"
echo "First inference - model loads:"
lmstrix infer "What is 5 + 5?" "$MODEL_ID" --out_ctx 10
echo -e "\nSecond inference - model already loaded (should be faster):"
lmstrix infer "What is 10 + 10?" "$MODEL_ID" --out_ctx 10
echo "(Notice: Model was reused, not reloaded)"

# Example 5: Context Size Impact
echo -e "\n--- Example 5: Context Size Impact ---"
echo "Smaller context (faster loading, less memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 2048 --out_ctx 10

echo -e "\nLarger context (slower loading, more memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 16384 --out_ctx 10

# Example 6: Zero Context Loading
echo -e "\n--- Example 6: Zero Context Loading (--in_ctx 0) ---"
echo "Load model without specifying context (uses model's default):"
lmstrix infer "What is Python?" "$MODEL_ID" --in_ctx 0 --out_ctx 50
echo "(Model loaded with its default context configuration)"

# Example 7: Context Optimization Strategy
echo -e "\n--- Example 7: Context Optimization Strategy ---"
echo "For best performance:"
echo "1. Test your model first to find optimal context:"
echo "   lmstrix test --model_id $MODEL_ID"
echo -e "\n2. Use the tested context for inference:"
echo "   lmstrix infer 'prompt' $MODEL_ID --in_ctx <tested_value>"
echo -e "\n3. Or let LMStrix choose automatically (omit --in_ctx)"

# Example 8: Long Context Use Case
echo -e "\n--- Example 8: Long Context Use Case ---"
echo "Processing a long document (simulated):"
LONG_PROMPT="Summarize this text: The history of artificial intelligence began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by classical philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols."
echo "Prompt length: ${#LONG_PROMPT} characters"
lmstrix infer "$LONG_PROMPT" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 9: Memory-Conscious Loading
echo -e "\n--- Example 9: Memory-Conscious Loading ---"
echo "For limited memory systems, use smaller context:"
lmstrix infer "What is RAM?" "$MODEL_ID" --in_ctx 2048 --out_ctx 50
echo "(Smaller context = less GPU/RAM usage)"

# Example 10: Comparing Context Sizes
echo -e "\n--- Example 10: Context Size Comparison ---"
echo "Let's compare the same prompt with different contexts:"

echo -e "\nWith 2K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 2048 --out_ctx 100

echo -e "\nWith 8K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 8192 --out_ctx 100

echo -e "\nNote: Larger context doesn't always mean better output for simple prompts"

echo -e "\n### Context Control Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- --in_ctx controls model loading context (memory usage)"
echo "- --out_ctx controls generation length"
echo "- Models are reused when possible for efficiency"
echo "- Optimal context depends on your use case"
echo "- Test models first to find their true limits"
</document_content>
</document>

<document index="25">
<source>examples/cli/inference_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various inference scenarios using LMStrix.
# Shows new features: --in_ctx, --out_ctx, --file_prompt, --dict, --force-reload
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Inference Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Simple Question with new --out_ctx parameter
echo -e "\n--- Example 1: Simple Question with Output Context Control ---"
lmstrix infer "Explain the theory of relativity in simple terms." "$MODEL_ID" --out_ctx 200

# Example 2: Context Control - Load model with specific context size
echo -e "\n--- Example 2: Load Model with Specific Context Size ---"
lmstrix infer "What are the benefits of renewable energy?" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 3: Load model without context specification (--in_ctx 0)
echo -e "\n--- Example 3: Load Model with Default Context ---"
lmstrix infer "What is machine learning?" "$MODEL_ID" --in_ctx 0 --out_ctx 100

# Example 4: Model State Detection - Second call reuses loaded model
echo -e "\n--- Example 4: Model State Detection (Reuse) ---"
echo "First call - loads the model:"
lmstrix infer "What is Python?" "$MODEL_ID" --out_ctx 50
echo -e "\nSecond call - reuses loaded model:"
lmstrix infer "What is JavaScript?" "$MODEL_ID" --out_ctx 50

# Example 5: Force Reload - Reload model even if already loaded
echo -e "\n--- Example 5: Force Model Reload ---"
lmstrix infer "What is artificial intelligence?" "$MODEL_ID" --force-reload --out_ctx 100

# Example 6: Using TOML Prompt Files with Parameters
echo -e "\n--- Example 6: Using Prompt Templates from TOML ---"
# First, let's check if prompts.toml exists
if [ -f "examples/prompts.toml" ]; then
    PROMPT_FILE="examples/prompts.toml"
elif [ -f "prompts.toml" ]; then
    PROMPT_FILE="prompts.toml"
else
    # Create a simple prompts.toml for demonstration
    cat > temp_prompts.toml <<EOL
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"

[code]
review = "Please review this {{language}} code and suggest improvements: {{code}}"
explain = "Explain this {{language}} code in simple terms: {{code}}"
EOL
    PROMPT_FILE="temp_prompts.toml"
fi

echo "Using prompt file: $PROMPT_FILE"
lmstrix infer greetings.formal "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "name=Alice,topic=Python" --out_ctx 100

# Example 7: Multiple Parameters with Different Format
echo -e "\n--- Example 7: Code Review with Template ---"
lmstrix infer code.review "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "language=Python,code=def factorial(n): return 1 if n <= 1 else n * factorial(n-1)" --out_ctx 300

# Example 8: Adjusting Temperature for Creative Output
echo -e "\n--- Example 8: Creative Writing with High Temperature ---"
lmstrix infer "Write a short poem about the sea." "$MODEL_ID" --temperature 1.5 --out_ctx 150

# Example 9: Low Temperature for Deterministic Output
echo -e "\n--- Example 9: Math Problem with Low Temperature ---"
lmstrix infer "Calculate: 25 * 4 + 10 - 5" "$MODEL_ID" --temperature 0.1 --out_ctx 20

# Example 10: Combining Features - Context Control + Template + Temperature
echo -e "\n--- Example 10: Combined Features Demo ---"
lmstrix infer greetings.casual "$MODEL_ID" \
    --file_prompt "$PROMPT_FILE" \
    --dict "name=Bob,topic=quantum computing" \
    --in_ctx 4096 \
    --out_ctx 150 \
    --temperature 0.8

# Cleanup temporary file if created
if [ "$PROMPT_FILE" = "temp_prompts.toml" ]; then
    rm temp_prompts.toml
fi

echo -e "\n### Inference Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- Use --out_ctx instead of deprecated --max_tokens"
echo "- Use --in_ctx to control model loading context size"
echo "- Use --file_prompt with --dict for template-based prompts"
echo "- Models are reused when already loaded (unless --force-reload)"
echo "- Combine features for advanced use cases"
</document_content>
</document>

<document index="26">
<source>examples/cli/model_state_demo.sh</source>
<document_content>
#!/bin/bash
# this_file: examples/cli/model_state_demo.sh
#
# Demonstration of model state persistence in LMStrix
# Shows how models remain loaded across multiple infer calls
#

set -e

echo "### LMStrix Model State Persistence Demo ###"
echo ""
echo "This demo shows how LMStrix now keeps models loaded between calls"
echo "for better performance and resource efficiency."
echo ""

MODEL_ID="llama-3.2-3b-instruct"  # Change this to match your model

# Step 1: First inference with explicit model and context
echo "=== Step 1: First inference with explicit model and context ==="
echo "Command: lmstrix infer \"What is 2+2?\" -m \"$MODEL_ID\" --out_ctx \"25%\" --verbose"
echo "(This will load the model with specified context)"
echo ""
lmstrix infer "What is 2+2?" -m "$MODEL_ID" --out_ctx "25%" --verbose || echo "Failed"

echo ""
echo "=== Step 2: Second inference without in_ctx ==="
echo "Command: lmstrix infer \"What is 3+3?\" -m \"$MODEL_ID\" --verbose"
echo "(This should reuse the already loaded model)"
echo ""
sleep 2
lmstrix infer "What is 3+3?" -m "$MODEL_ID" --verbose || echo "Failed"

echo ""
echo "=== Step 3: Third inference without model_id ==="
echo "Command: lmstrix infer \"What is 4+4?\" --verbose"
echo "(This should use the last-used model: $MODEL_ID)"
echo ""
sleep 2
lmstrix infer "What is 4+4?" --verbose || echo "Failed"

echo ""
echo "=== Demo Complete ==="
echo ""
echo "Key features demonstrated:"
echo "1. Models stay loaded between infer calls when in_ctx is not specified"
echo "2. Last-used model is remembered when -m is not specified"
echo "3. Better performance by avoiding repeated model loading/unloading"
echo ""
echo "Check the verbose output above to see:"
echo "- First call: 'Loading with optimal context...'"
echo "- Second call: 'Model already loaded... reusing...'"
echo "- Third call: 'Using last-used model...'"
</document_content>
</document>

<document index="27">
<source>examples/cli/model_testing.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various context testing scenarios with LMStrix.
# Shows new features: --threshold, --all --ctx, --fast, and test resumption.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Model Testing Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Standard Test with New Fixed Context Strategy
# Tests at fixed context sizes: 30k, 40k, 60k, 80k, 100k, 120k, and declared_max-1
echo -e "\n--- Example 1: Standard Context Test ---"
echo "Testing model '$MODEL_ID' with fixed context strategy"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Standard test complete."

# Example 2: Test with Threshold Limit
# Prevents initial test from exceeding specified threshold (default: 102,400)
echo -e "\n--- Example 2: Test with Custom Threshold ---"
echo "Testing with maximum initial context of 50,000 tokens"
lmstrix test --model_id "$MODEL_ID" --threshold 50000
echo "Threshold test complete."

# Example 3: Fast Mode Testing
# Only checks if model can load and generate, skips semantic verification
echo -e "\n--- Example 3: Fast Mode Test ---"
echo "Running fast test (skip semantic verification)"
lmstrix test --model_id "$MODEL_ID" --fast
echo "Fast test complete."

# Example 4: Test at Specific Context Size
# Tests a model at a specific context size only
echo -e "\n--- Example 4: Test at Specific Context ---"
echo "Testing model at exactly 8192 tokens"
lmstrix test --model_id "$MODEL_ID" --ctx 8192
echo "Specific context test complete."

# Example 5: Batch Test All Models
# Tests all untested models in the registry
echo -e "\n--- Example 5: Batch Test All Models ---"
echo "Testing all models (this may take a while)..."
echo "Note: This will test ALL models. Press Ctrl+C to cancel."
read -p "Press Enter to continue..." -t 5 || true
lmstrix test --all --verbose
echo "Batch test complete."

# Example 6: Batch Test with Context Limit
# Tests all models at a specific context size
echo -e "\n--- Example 6: Batch Test at Specific Context ---"
echo "Testing all untested models at 4096 tokens"
lmstrix test --all --ctx 4096
echo "Batch context test complete."

# Example 7: Fast Batch Testing
# Quick test of all models without semantic verification
echo -e "\n--- Example 7: Fast Batch Test ---"
echo "Running fast test on all models"
lmstrix test --all --fast
echo "Fast batch test complete."

# Example 8: Show Test Results
# Display models sorted by their tested context size
echo -e "\n--- Example 8: View Test Results ---"
echo "Models sorted by tested context size:"
lmstrix list --sort ctx
echo -e "\nModels sorted by efficiency (tested/declared ratio):"
lmstrix list --sort eff

# Example 9: Test with Smart Sorting
# When testing all models, they're sorted for efficiency
echo -e "\n--- Example 9: Smart Sorted Batch Test ---"
echo "Testing all models with smart sorting (smaller models first)"
lmstrix test --all --threshold 30000 --fast
echo "Smart sorted test complete."

# Example 10: Resume Interrupted Test
# If a test is interrupted, it can resume from where it left off
echo -e "\n--- Example 10: Test Resumption Demo ---"
echo "LMStrix automatically saves progress during testing."
echo "If a test is interrupted, simply run the same command again."
echo "The test will resume from the last successful context size."

echo -e "\n### Model Testing Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- New fixed context testing strategy (30k, 40k, 60k, etc.)"
echo "- Use --threshold to limit maximum initial test size"
echo "- Use --fast for quick testing without semantic checks"
echo "- Use --ctx to test at specific context sizes"
echo "- Batch testing with --all supports all options"
echo "- Tests automatically resume if interrupted"
</document_content>
</document>

<document index="28">
<source>examples/data/sample_context.txt</source>
<document_content>
This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

</document_content>
</document>

<document index="29">
<source>examples/data/test_questions.json</source>
<document_content>
[
  {
    "id": "qa_1",
    "question": "What is the primary purpose of the sample_context.txt file?",
    "context_source": "sample_context.txt"
... (file content truncated to first 5 lines)
</document_content>
</document>

<document index="30">
<source>examples/prompts/analysis.toml</source>
<document_content>
# examples/prompts/analysis.toml

[summarize_text]
prompt = "Summarize the key points of the following text in three bullet points:

{text}"
system_prompt = "You are a helpful assistant that specializes in text summarization."
description = "Summarizes a long piece of text."

[sentiment_analysis]
prompt = "Analyze the sentiment of the following review. Respond with 'Positive', 'Negative', or 'Neutral'.

Review: "{review_text}""
system_prompt = "You are a sentiment analysis AI."
description = "Analyzes the sentiment of a given text."
</document_content>
</document>

<document index="31">
<source>examples/prompts/coding.toml</source>
<document_content>
# examples/prompts/coding.toml

[python_function]
prompt = "Write a Python function that does the following: {request}. Include a docstring and type hints."
system_prompt = "You are an expert Python programmer who writes clean, efficient, and well-documented code."
description = "Generates a Python function based on a request."

[explain_code]
prompt = "Explain what this code does in simple terms. Identify potential bugs if any.

Code:
```
{code_snippet}
```"
system_prompt = "You are a senior software engineer who is excellent at code reviews and explaining complex concepts."
description = "Explains a snippet of code."
</document_content>
</document>

<document index="32">
<source>examples/prompts/creative.toml</source>
<document_content>
# examples/prompts/creative.toml

[write_poem]
prompt = "Write a short, four-line poem about {topic}."
system_prompt = "You are a world-class poet."
description = "Writes a short poem on a given topic."

[generate_story_idea]
prompt = "Generate a compelling story idea based on the following genre and character: Genre: {genre}, Character: {character_description}"
system_prompt = "You are a creative writing assistant, skilled in generating unique plot ideas."
description = "Generates a story idea from a genre and character."
</document_content>
</document>

<document index="33">
<source>examples/prompts/qa.toml</source>
<document_content>
# examples/prompts/qa.toml

[simple_question]
prompt = "Answer the following question: {question}"
system_prompt = "You are a helpful and factual question-answering AI."
description = "Answers a direct question."

[contextual_question]
prompt = "Based on the text below, answer the question.

Text: {context}

Question: {question}"
system_prompt = "You are an AI assistant that answers questions based *only* on the provided context."
description = "Answers a question based on a provided context."
</document_content>
</document>

<document index="34">
<source>examples/prompts.toml</source>
<document_content>
# Example prompt templates for LMStrix
# Demonstrates placeholder resolution and template reuse

# Simple greeting prompts
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"
professional = "Hello {{name}}, I'm here to help you with {{topic}}. Please describe your requirements."

# Templates for different use cases
[templates]
base = "You are an expert assistant specializing in {{domain}}."
instruction = """{{templates.base}}

Please provide a {{style}} explanation of {{concept}}.
Your response should be appropriate for someone with {{level}} knowledge."""

# Code-related prompts
[code]
review = """You are a code reviewer. Please review the following {{language}} code:

{{code}}

Focus on:
1. Code quality and best practices
2. Potential bugs or issues
3. Performance considerations
4. Security concerns"""

explain = "Explain this {{language}} code in simple terms: {{code}}"

# Research prompts
[research]
summarize = """Please summarize the following text about {{topic}}:

{{text}}

Provide a {{length}} summary focusing on the key points."""

analyze = """Analyze the following information about {{subject}}:

{{content}}

Consider the following aspects:
- {{aspect1}}
- {{aspect2}}
- {{aspect3}}"""

# Math and science prompts
[math]
solve = "Solve this {{difficulty}} math problem step by step: {{problem}}"
explain_concept = "Explain the concept of {{concept}} in {{field}} using {{approach}} approach."

# Creative writing prompts
[creative]
story = "Write a {{genre}} story about {{character}} who {{situation}}. The story should be {{length}} and include {{elements}}."
poem = "Create a {{style}} poem about {{theme}} that incorporates {{imagery}}."

# Question answering
[qa]
simple = "{{question}}"
detailed = """Question: {{question}}

Please provide a comprehensive answer that includes:
- Direct answer
- Explanation
- Examples where relevant
- Any important caveats or exceptions"""

# System prompts
[system]
assistant = """You are a helpful AI assistant. Your traits:
- {{personality}}
- Knowledge level: {{expertise}}
- Communication style: {{style}}
- Primary language: {{language}}"""

chatbot = """You are {{bot_name}}, a {{bot_type}} chatbot.
Your purpose: {{purpose}}
Your tone: {{tone}}
Special instructions: {{instructions}}"""
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/python/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py
# Language: python

from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates advanced context testing with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py
# Language: python

from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError, ModelNotFoundError
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates basic usage of the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py
# Language: python

import time
from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates batch processing of multiple models for testing or inference."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py
# Language: python

from pathlib import Path
from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates custom inference workflows with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py
# Language: python

from pathlib import Path
import toml
from lmstrix import LMStrix
from lmstrix.core.prompts import PromptResolver
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates prompt template features."""


<document index="35">
<source>examples/run_all_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script runs all the examples in the `cli` and `python` directories.
# It's a way to functionally test that all example code is working as expected.
#
# NOTE: This script assumes you have a model downloaded in LM Studio
# and that the identifier 'ultron-summarizer-1b' will match it. If not, please edit the
# example files to use an identifier for a model you have.
#

set -e # Exit on any error

echo "===== Running All LMStrix Examples ====="

# --- Running CLI Examples ---
echo -e "

--- Testing CLI Examples ---"
echo "NOTE: The CLI scripts have a placeholder model identifier ('ultron-summarizer-1b')."
echo "Please edit them if you don't have a model matching that ID."

echo -e "
>>> Running basic_workflow.sh"
bash "$(dirname "$0")/cli/basic_workflow.sh"

echo -e "
>>> Running model_testing.sh"
bash "$(dirname "$0")/cli/model_testing.sh"

echo -e "
>>> Running inference_examples.sh"
bash "$(dirname "$0")/cli/inference_examples.sh"

echo -e "
--- CLI Examples Complete ---"


# --- Running Python Examples ---
echo -e "

--- Testing Python Examples ---"
echo "NOTE: The Python scripts will use the first model they find."

echo -e "
>>> Running basic_usage.py"
python3 "$(dirname "$0")/python/basic_usage.py"

echo -e "
>>> Running advanced_testing.py"
python3 "$(dirname "$0")/python/advanced_testing.py"

echo -e "
>>> Running custom_inference.py"
python3 "$(dirname "$0")/python/custom_inference.py"

echo -e "
>>> Running batch_processing.py"
python3 "$(dirname "$0")/python/batch_processing.py"

echo -e "
--- Python Examples Complete ---"


echo -e "

===== All Examples Finished Successfully ====="
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py
# Language: python

import sys
from pathlib import Path
import fire
from rich.console import Console
from slugify import slugify
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.utils.logging import logger

def process_text_with_model((
    input_file: str,
    model_id: str | None = None,
    verbose: bool = False,
)) -> str:
    """ Process a text file by sending paragraphs to a model for number-to-words conversion...."""

def main(()) -> None:
    """Main entry point using Fire CLI."""


<document index="36">
<source>issues/301.txt</source>
<document_content>
```
$ ./adam2.sh
15:41:35 | INFO | lmstrix.loaders.prompt_loader:load_single_prompt:128 - Loaded prompts from adam.toml
15:41:35 | WARNING | lmstrix.loaders.prompt_loader:load_single_prompt:143 - TOPL placeholder resolution failed: Format string contains positional fields
15:41:35 | DEBUG | lmstrix.core.prompts:resolve_prompt:186 - Internal resolution complete after 2 passes
15:41:35 | INFO | lmstrix.loaders.prompt_loader:load_single_prompt:153 - Resolved prompt 'translate'
Loaded prompt 'translate' from adam.toml
Resolved placeholders: text, think
15:41:35 | DEBUG | lmstrix.utils.paths:get_lmstudio_path:26 - Found LM Studio at /Users/Shared/lmstudio
15:41:35 | DEBUG | lmstrix.utils.paths:get_lmstudio_path:26 - Found LM Studio at /Users/Shared/lmstudio
15:41:35 | INFO | lmstrix.loaders.model_loader:load_model_registry:43 - Read 75 models from /Users/Shared/lmstudio/lmstrix.json
15:41:35 | INFO | lmstrix.core.inference_manager:infer:92 - Explicit context specified: 75%
15:41:35 | DEBUG | lmstrix.api.client:load_model_by_id:104 - Found model match: qwen/qwen3-4b -> qwen/qwen3-4b
15:41:35 | ERROR | lmstrix.core.inference_manager:infer:188 - Inference failed for model qwen/qwen3-4b: Failed to load model 'qwen/qwen3-4b': Failed to load model 'qwen/qwen3-4b'. Available models: ['google/gemma-3n-e4b', 'microsoft/phi-4-mini-reasoning', 'deepseek-r1-0528-qwen3-8b', 'magistral-small-2507-rebased-vision-i1', 'huihui-qwen3-14b-abliterated-v2']
Inference failed: Failed to load model 'qwen/qwen3-4b': Failed to load model 'qwen/qwen3-4b'. Available models: ['google/gemma-3n-e4b',
'microsoft/phi-4-mini-reasoning', 'deepseek-r1-0528-qwen3-8b', 'magistral-small-2507-rebased-vision-i1', 'huihui-qwen3-14b-abliterated-v2']
```

but our lmstrix.json database has

```
"qwen/qwen3-4b": {
"id": "qwen/qwen3-4b",
"short_id": "qwen3-4b",
"path": "qwen/qwen3-4b",
"size_bytes": 4280583359,
"ctx_in": 32768,
"ctx_out": 4096,
"has_tools": false,
"has_vision": false,
"tested_max_context": 32768,
"loadable_max_context": null,
"last_known_good_context": 32768,
"last_known_bad_context": null,
"context_test_status": "completed",
"context_test_log": "/Users/Shared/lmstudio/lmstrix/context_tests/qwen_qwen3-4b_context_test.log",
"context_test_date": "2025-07-28 01:18:12.706392",
"failed": false,
"error_msg": ""
},
```

</document_content>
</document>

<document index="37">
<source>issues/302.txt</source>
<document_content>
I manually run this prompt in LM Studio with the model `llama-3.2-3b-instruct`: 


```
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given line-per-line line `input term = output term—alternative output term ≠ avoided translation`, where `=` means: translate like that, `|` means: or translate like this depending on the context `≠` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font ≠ czcionka
variable font = font zmienny ≠ czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe ≠ czcionki webowe
small caps = kapitaliki ≠ małe kapitaliki
double-click = dwuklik—dwukliknąć ≠ podwójne kliknięcie
oldstyle numerals = cyfry nautyczne
typeface = krój pisma | krój
stem = kreska główna
hint = hint | hintować ≠ wskazówka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days, and start making fonts today!
</input>
```

and I got this: 

```
Oto przekład:

FontLab 8 jest zaawansowanym edytorem pism do komputera Mac i Windows, który pomaga Ci stworzyć pisma od podstaw aż do końca, od prostego projektu aż do złożonej pracy, i dodaje czapkę magiczności w designie pism. Proszę o FontLab 8 za darmo przez 10 dni i zacznij tworzenie pism dzisiaj!

Wysługiwałoby się taki przekład:

FontLab 8 jest zaawansowanym edytorem pism do komputera Mac i Windows, który pomaga Ci stworzyć pisma od podstaw aż do końca, od prostego projektu aż do złożonej pracy, i dodaje czapkę magiczności w designie pism. | Proszę o FontLab 8 za darmo przez 10 dni i zacznij tworzyć pisma dzisiaj!
Proszę oFontLab 8 za darmo—wskazówka ≠ proszę o FontLab 8
W powyższym przypadku użył się "proszę" zamiast "o" w wyrazie, co jest bardziej przyzwoite dla polskiego języka.

Oto druga wersja przekładu bez "wskazówki":

FontLab 8 to zaawansowany editor pism do komputera Mac i Windows, który pomaga Ci stworzyć pisma od podstaw aż do końca, od prostego projektu aż do złożonej pracy, i dodaje czapkę magiczności w designie pism. | Try FontLab 8 za darmo przez 10 dni i zacznij tworzyć pisma dzisiaj!
Try FontLab 8 za darmo—hint ≠ podwójne kliknięcie
W tym przypadku użył się słowa "try" zamiast "wskazówki", co jest bardziej naturalne dla polskiego języka.
Proszę o fonty webowe do tworzenia pism — niektóre z nich mogą być wykorzystywane w FontLab 8—fonty webowe ≠ fonty komputerowe
W tym przypadku użył się "fonty webowe" zamiast "fontów komputerowych", co jest bardziej praktyczne dla polskiego języka.
Uwagi:
- Przekład zastosował odpowiedni typografia i słowa, które są określone w słowniku.
- Użyto skrótu i przeniesienia słów, aby utrzymać czytelność i płynny przekład.
- Przekład miał na celu utrzymanie zgodności z oczekiwanimi słownika.
```

the the log was: 

```

Developer Logs
2025-07-31 18:06:10 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 20, top_p = 0.950, min_p = 0.000, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:06:10 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 16384, n_batch = 512, n_predict = -1, n_keep = 356
2025-07-31 18:06:10 [DEBUG]
 Looking for non-prefix contiguous prompt sequences of size >= 256 to reuse from cache
2025-07-31 18:06:10 [DEBUG]
 Cache reuse summary: 0/356 of prompt (0%), 0 prefix, 0 non-prefix
2025-07-31 18:06:10 [DEBUG]
 Total prompt tokens: 356
Prompt tokens to decode: 356
BeginProcessingPrompt
2025-07-31 18:06:12 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:06:45 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1715.77 ms
llama_perf_context_print: prompt eval time =    1684.12 ms /   356 tokens (    4.73 ms per token,   211.39 tokens per second)
llama_perf_context_print:        eval time =   31729.07 ms /   465 runs   (   68.23 ms per token,    14.66 tokens per second)
llama_perf_context_print:       total time =   34031.51 ms /   821 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:06:45 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 20, top_p = 0.950, min_p = 0.000, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:06:45 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 16384, n_batch = 512, n_predict = 30, n_keep = 356
2025-07-31 18:06:45 [DEBUG]
 Total prompt tokens: 891
Prompt tokens to decode: 70
BeginProcessingPrompt
2025-07-31 18:06:45 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:06:46 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1715.77 ms
llama_perf_context_print: prompt eval time =     612.92 ms /    70 tokens (    8.76 ms per token,   114.21 tokens per second)
llama_perf_context_print:        eval time =    1143.03 ms /    14 runs   (   81.64 ms per token,    12.25 tokens per second)
llama_perf_context_print:       total time =    1809.70 ms /    84 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] GPU Configuration:
  Strategy: evenly
  Priority: []
  Disabled GPUs: []
  Limit weight offload to dedicated GPU Memory: OFF
  Offload KV Cache to GPU: ON
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Live GPU memory info:
No live GPU info available
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Model load size estimate with raw num offload layers 'max' and context length '131072':
  Model: 1.72 GB
  Context: 21.56 GB
  Total: 23.28 GB
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Not using full context length for VRAM overflow calculations due to single GPU setup. Instead, using '8192' as context length for the calculation. Original context length: '131072'.
[LM Studio] Strict GPU VRAM cap is OFF: GPU offload layers will not be checked for adjustment
2025-07-31 18:07:06 [DEBUG]
 [LM Studio] Resolved GPU config options:
  Num Offload Layers: max
  Main GPU: 0
  Tensor Split: [0]
  Disabled GPUs: []
2025-07-31 18:07:06 [DEBUG]
 Metal : CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |
2025-07-31 18:07:06 [DEBUG]
 llama_model_load_from_file_impl: using device Metal (Apple M2) - 18186 MiB free
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /Users/Shared/lmstudio/models/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = llama3.2
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 14
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
2025-07-31 18:07:06 [DEBUG]
 llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...
llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  189 tensors
llama_model_loader: - type q5_K:    7 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Small
print_info: file size   = 1.79 GiB (4.78 BPW)
2025-07-31 18:07:06 [DEBUG]
 load: special tokens cache size = 256
2025-07-31 18:07:06 [DEBUG]
 load: token to piece cache size = 0.7999 MB
2025-07-31 18:07:06 [DEBUG]
 print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
2025-07-31 18:07:07 [DEBUG]
 load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: Metal_Mapped model buffer size =  1831.41 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
2025-07-31 18:07:07 [DEBUG]
 llama_context: constructing llama_context
llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 131072
llama_context: n_ctx_per_seq = 131072
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 1
llama_context: kv_unified    = true
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2
ggml_metal_init: picking default device: Apple M2
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_load_library: loading '/Users/Shared/lmstudio/extensions/backends/llama.cpp-mac-arm64-apple-metal-advsimd-1.42.0/default.metallib'
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: GPU name:   Apple M2
ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 19069.67 MB
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
2025-07-31 18:07:07 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
2025-07-31 18:07:07 [DEBUG]
 llama_context:        CPU  output buffer size =     0.49 MiB
2025-07-31 18:07:07 [DEBUG]
 llama_kv_cache_unified:      Metal KV buffer size = 14336.00 MiB
2025-07-31 18:07:09 [DEBUG]
 llama_kv_cache_unified: size = 14336.00 MiB (131072 cells,  28 layers,  1/ 1 seqs), K (f16): 7168.00 MiB, V (f16): 7168.00 MiB
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
2025-07-31 18:07:09 [DEBUG]
 llama_context:      Metal compute buffer size =   408.00 MiB
llama_context:        CPU compute buffer size =   262.01 MiB
llama_context: graph nodes  = 931
llama_context: graph splits = 2
2025-07-31 18:07:09 [DEBUG]
 common_init_from_params: added <|end_of_text|> logit bias = -inf
common_init_from_params: added <|eom_id|> logit bias = -inf
common_init_from_params: added <|eot_id|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 131072
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-07-31 18:07:13 [DEBUG]
 GgmlThreadpools: llama threadpool init = n_threads = 6
2025-07-31 18:07:30 [DEBUG]
 Sampling params:
2025-07-31 18:07:30 [DEBUG]
 repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:07:30 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 131072, n_batch = 512, n_predict = -1, n_keep = 390
2025-07-31 18:07:30 [DEBUG]
 Total prompt tokens: 390
Prompt tokens to decode: 390
2025-07-31 18:07:30 [DEBUG]
 BeginProcessingPrompt
2025-07-31 18:07:31 [DEBUG]
 FinishedProcessingPrompt. Progress:
2025-07-31 18:07:31 [DEBUG]
 100
2025-07-31 18:07:56 [DEBUG]
 Target model llama_perf stats:
2025-07-31 18:07:56 [DEBUG]
 llama_perf_context_print:        load time =    7680.27 ms
llama_perf_context_print: prompt eval time =    1715.59 ms /   390 tokens (    4.40 ms per token,   227.33 tokens per second)
llama_perf_context_print:        eval time =   23627.92 ms /   639 runs   (   36.98 ms per token,    27.04 tokens per second)
llama_perf_context_print:       total time =   26104.33 ms /  1029 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:07:56 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:07:56 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 131072, n_batch = 512, n_predict = 30, n_keep = 390
2025-07-31 18:07:56 [DEBUG]
 Total prompt tokens: 1099
Prompt tokens to decode: 70
2025-07-31 18:07:56 [DEBUG]
 BeginProcessingPrompt
2025-07-31 18:07:56 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:07:56 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    7680.27 ms
llama_perf_context_print: prompt eval time =     419.37 ms /    70 tokens (    5.99 ms per token,   166.92 tokens per second)
llama_perf_context_print:        eval time =     318.05 ms /     9 runs   (   35.34 ms per token,    28.30 tokens per second)
llama_perf_context_print:       total time =     746.49 ms /    79 tokens
llama_perf_context_print:    graphs reused =          0
```


----

Then I run `./adam3.sh` which is `lmstrix infer translate --verbose -m llama-3.2-3b-instruct --file_prompt adam.toml --text_file fontlab8a.md --in_ctx 50%`


and in Terminal I get

```
Othello:adam adam$ ./adam3.sh
[I] Loaded prompts from adam.toml lmstrix.loaders.prompt_loader:load_single_prompt:128
[I] Resolved placeholders with TOPL lmstrix.loaders.prompt_loader:load_single_prompt:141
[D] Internal resolution complete after 1 passes lmstrix.core.prompts:resolve_prompt:185
[I] Resolved prompt 'translate' lmstrix.loaders.prompt_loader:load_single_prompt:153
[D] Loaded prompt 'translate' from adam.toml lmstrix.cli.main:infer:683
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[I] Read 75 models from /Users/Shared/lmstudio/lmstrix.json lmstrix.loaders.model_loader:load_model_registry:42
[D] Parsed out_ctx '50%' as 50.0% of 131072 = 65536 tokens lmstrix.utils.context_parser:parse_out_ctx:60
[D] Parsed in_ctx '50%' as 65536 tokens lmstrix.cli.main:infer:755
[I] Explicit context specified: 65536 lmstrix.core.inference_manager:infer:92
[D] Found model match: llama-3.2-3b-instruct -> llama-3.2-3b-instruct lmstrix.api.client:load_model_by_id:105
⠦ Running inference on llama-3.2-3b-instruct...[I] Running inference on model llama-3.2-3b-instruct lmstrix.core.inference_manager:infer:163
[D] Using 90% of context length (131072) = 117964 tokens as default maxTokens lmstrix.api.client:completion:204
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:214
[I] 🤖 MODEL: llama-3.2-3b-instruct lmstrix.api.client:completion:215
[I] 🔧 CONFIG: maxTokens=117964, temperature=0.7 lmstrix.api.client:completion:216
[I] 📝 Prompt (29 lines, 1452 chars): lmstrix.api.client:completion:236
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given
line-per-line line `input term = output term—alternative output term ≠ avoided translation`, where `=` means: translate
like that, `|` means: or translate like this depending on the context `≠` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical
jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font ≠ czcionka
variable font = font zmienny ≠ czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe ≠ czcionki webowe
small caps = kapitaliki ≠ małe kapitaliki
double-click = dwuklik—dwukliknąć ≠ podwójne kliknięcie
oldstyle numerals = cyfry nautyczne
typeface = krój pisma | krój
stem = kreska główna
hint = hint | hintować ≠ wskazówka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a
simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days,
and start making fonts today!
</input>

⠧ Running inference on llama-3.2-3b-instruct...[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:240
⠋ Running inference on llama-3.2-3b-instruct...[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:256
[I] 📊 INFERENCE STATS lmstrix.api.client:completion:257
[I] ⚡ Time to first token: 1.04s lmstrix.api.client:completion:261
[I] ⏱️  Total inference time: 1.13s lmstrix.api.client:completion:264
[I] 🔢 Predicted tokens: 4 lmstrix.api.client:completion:268
[I] 📝 Prompt tokens: 355 lmstrix.api.client:completion:271
[I] 🎯 Total tokens: 359 lmstrix.api.client:completion:274
[I] 🚀 Tokens/second: 49.67 lmstrix.api.client:completion:278
[I] 🛑 Stop reason: eosFound lmstrix.api.client:completion:282
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:284

Model Response:
</translation>
Othello:adam adam$
```

and in the LMStudio log: 

```

Developer Logs
2025-07-31 18:24:27 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4] Client created.
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listLoaded] Listing loaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listLoaded] Listing loaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=listDownloadedModels] Listing downloaded models
2025-07-31 18:24:27  [INFO]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-31 18:24:27 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4][Endpoint=getOrLoad] Model not found by identifier. Trying to load.
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] GPU Configuration:
  Strategy: evenly
  Priority: []
  Disabled GPUs: []
  Limit weight offload to dedicated GPU Memory: OFF
  Offload KV Cache to GPU: ON
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] Live GPU memory info:
No live GPU info available
2025-07-31 18:24:27 [DEBUG]
 [LM Studio] Model load size estimate with raw num offload layers 'max' and context length '65536':
  Model: 1.72 GB
  Context: 10.79 GB
  Total: 12.51 GB
[LM Studio] Not using full context length for VRAM overflow calculations due to single GPU setup. Instead, using '8192' as context length for the calculation. Original context length: '65536'.
[LM Studio] Strict GPU VRAM cap is OFF: GPU offload layers will not be checked for adjustment
[LM Studio] Resolved GPU config options:
  Num Offload Layers: max
  Main GPU: 0
  Tensor Split: [0]
  Disabled GPUs: []
2025-07-31 18:24:27 [DEBUG]
 Metal : CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 |
2025-07-31 18:24:27 [DEBUG]
 llama_model_load_from_file_impl: using device Metal (Apple M2) - 18186 MiB free
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /Users/Shared/lmstudio/models/bartowski/Llama-3.2-3B-Instruct-GGUF/Llama-3.2-3B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                            general.license str              = llama3.2
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 28
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  19:                          general.file_type u32              = 14
llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  30:               general.quantization_version u32              = 2
2025-07-31 18:24:27 [DEBUG]
 llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...
llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  189 tensors
llama_model_loader: - type q5_K:    7 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Small
print_info: file size   = 1.79 GiB (4.78 BPW)
2025-07-31 18:24:27 [DEBUG]
 load: special tokens cache size = 256
2025-07-31 18:24:27 [DEBUG]
 load: token to piece cache size = 0.7999 MB
2025-07-31 18:24:27 [DEBUG]
 print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
2025-07-31 18:24:27 [DEBUG]
 load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors: Metal_Mapped model buffer size =  1831.41 MiB
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
2025-07-31 18:24:27 [DEBUG]
 llama_context: constructing llama_context
llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 65536
llama_context: n_ctx_per_seq = 65536
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 1
llama_context: kv_unified    = true
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (65536) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M2
ggml_metal_init: picking default device: Apple M2
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_load_library: loading '/Users/Shared/lmstudio/extensions/backends/llama.cpp-mac-arm64-apple-metal-advsimd-1.42.0/default.metallib'
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: GPU name:   Apple M2
ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = false
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = false
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 19069.67 MB
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)
ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)
ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)
2025-07-31 18:24:27 [DEBUG]
 ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)
ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)
2025-07-31 18:24:27 [DEBUG]
 llama_context:        CPU  output buffer size =     0.49 MiB
2025-07-31 18:24:27 [DEBUG]
 llama_kv_cache_unified:      Metal KV buffer size =  7168.00 MiB
2025-07-31 18:24:28 [DEBUG]
 llama_kv_cache_unified: size = 7168.00 MiB ( 65536 cells,  28 layers,  1/ 1 seqs), K (f16): 3584.00 MiB, V (f16): 3584.00 MiB
llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility
2025-07-31 18:24:28 [DEBUG]
 llama_context:      Metal compute buffer size =   256.50 MiB
llama_context:        CPU compute buffer size =   134.01 MiB
llama_context: graph nodes  = 931
llama_context: graph splits = 2
2025-07-31 18:24:28 [DEBUG]
 common_init_from_params: added <|end_of_text|> logit bias = -inf
common_init_from_params: added <|eom_id|> logit bias = -inf
common_init_from_params: added <|eot_id|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 65536
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-07-31 18:24:28 [DEBUG]
 GgmlThreadpools: llama threadpool init = n_threads = 6
2025-07-31 18:24:28 [DEBUG]
 Sampling params: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
  dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
  top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.700
  mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 18:24:28 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 65536, n_batch = 512, n_predict = 117964, n_keep = 355
2025-07-31 18:24:28 [DEBUG]
 Total prompt tokens: 355
Prompt tokens to decode: 355
BeginProcessingPrompt
2025-07-31 18:24:29 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 18:24:29 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1025.51 ms
llama_perf_context_print: prompt eval time =    1033.97 ms /   355 tokens (    2.91 ms per token,   343.34 tokens per second)
llama_perf_context_print:        eval time =      79.33 ms /     3 runs   (   26.44 ms per token,    37.82 tokens per second)
2025-07-31 18:24:29 [DEBUG]
 llama_perf_context_print:       total time =    1116.33 ms /   358 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 18:24:29  [INFO]
 Client disconnected: Error: read ECONNRESET
2025-07-31 18:24:29 [DEBUG]
 [Client=0994a8dc-e5f5-4652-8e02-1f191e5fc5d4] Client disconnected.
```

</document_content>
</document>

<document index="38">
<source>issues/303.txt</source>
<document_content>
```
Othello:adam adam$ ./adam3.sh
[I] Loaded prompts from adam.toml lmstrix.loaders.prompt_loader:load_single_prompt:128
[I] Resolved placeholders with TOPL lmstrix.loaders.prompt_loader:load_single_prompt:141
[D] Internal resolution complete after 1 passes lmstrix.core.prompts:resolve_prompt:184
[I] Resolved prompt 'translate' lmstrix.loaders.prompt_loader:load_single_prompt:153
[D] Loaded prompt 'translate' from adam.toml lmstrix.api.main:run_inference:637
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[I] Read 75 models from /Users/Shared/lmstudio/lmstrix.json lmstrix.loaders.model_loader:load_model_registry:42
[D] Parsed out_ctx '50%' as 50.0% of 131072 = 65536 tokens lmstrix.utils.context_parser:parse_out_ctx:60
[D] Parsed in_ctx '50%' as 65536 tokens lmstrix.api.main:run_inference:709
[I] Explicit context specified: 65536 lmstrix.core.inference_manager:infer:95
[D] Found model match: llama-3.2-3b-instruct -> llama-3.2-3b-instruct lmstrix.api.client:load_model_by_id:105
[I] Running inference on model llama-3.2-3b-instruct lmstrix.core.inference_manager:infer:163
[D] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:217
[D] 🔍 DIAGNOSTIC: Inference Parameters Comparison lmstrix.api.client:completion:218
[D]    Temperature: 0.8 (LM Studio GUI: 0.8) lmstrix.api.client:completion:219
[D]    Top K: 40 (LM Studio GUI: 20) lmstrix.api.client:completion:220
[D]    Top P: 0.95 (LM Studio GUI: 0.95) lmstrix.api.client:completion:221
[D]    Min P: 0.05 (LM Studio GUI: 0.05) lmstrix.api.client:completion:222
[D]    Repeat Penalty: 1.1 (LM Studio GUI: 1.1) lmstrix.api.client:completion:223
[D]    Max Tokens: -1 (LM Studio GUI: -1/unlimited) lmstrix.api.client:completion:224
[D]    Context Length: 131072 (LM Studio GUI: 131072) lmstrix.api.client:completion:225
[D] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:226
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:229
[I] 🤖 MODEL: llama-3.2-3b-instruct lmstrix.api.client:completion:230
[I] 🔧 CONFIG: maxTokens=None, temperature=0.8, topK=40, topP=0.95, minP=0.05, repeatPenalty=1.1 lmstrix.api.client:completion:231
[I] 📝 Prompt (29 lines, 1452 chars): lmstrix.api.client:completion:253
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given
line-per-line line `input term = output term—alternative output term ≠ avoided translation`, where `=` means: translate
like that, `|` means: or translate like this depending on the context `≠` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical
jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font ≠ czcionka
variable font = font zmienny ≠ czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe ≠ czcionki webowe
small caps = kapitaliki ≠ małe kapitaliki
double-click = dwuklik—dwukliknąć ≠ podwójne kliknięcie
oldstyle numerals = cyfry nautyczne
typeface = krój pisma | krój
stem = kreska główna
hint = hint | hintować ≠ wskazówka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a
simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days,
and start making fonts today!
</input>

⠋ Running inference on llama-3.2-3b-instruct...[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:257
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(microseconds=321739), 'exception': None, 'extra': {}, 'file':
(name='client.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py'), 'function': 'completion', 'level':
(name='DEBUG', no=10, icon='🐞'), 'line': 265, 'message': "Calling llm.complete with config: {'maxTokens': None,
'temperature': 0.8, 'topKSampling': 40, 'topPSampling': 0.95, 'repeatPenalty': 1.1, 'minPSampling': 0.05}", 'module':
'client', 'name': 'lmstrix.api.client', 'process': (id=38640, name='MainProcess'), 'thread': (id=8520598848,
name='MainThread'), 'time': datetime(2025, 7, 31, 19, 6, 51, 129098,
tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 165,
in emit
    formatted = precomputed_format.format_map(formatter_record)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: "'maxTokens'"
--- End of logging error ---
--- Logging error in Loguru Handler #1 ---
Record was: {'elapsed': datetime.timedelta(seconds=2, microseconds=419668), 'exception': None, 'extra': {}, 'file':
(name='client.py', path='/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py'), 'function': 'completion', 'level':
(name='DEBUG', no=10, icon='🐞'), 'line': 267, 'message': 'Raw response object: </translation-instructions>', 'module':
'client', 'name': 'lmstrix.api.client', 'process': (id=38640, name='MainProcess'), 'thread': (id=8520598848,
name='MainThread'), 'time': datetime(2025, 7, 31, 19, 6, 53, 227027,
tzinfo=datetime.timezone(datetime.timedelta(seconds=7200), 'CEST'))}
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 164,
in emit
    _, precomputed_format = self._memoize_dynamic_format(dynamic_format, ansi_level)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_handler.py", line 14, in
prepare_colored_format
    colored = Colorizer.prepare_format(format_)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_colorizer.py", line 367,
in prepare_format
    tokens, messages_color_tokens = Colorizer._parse_without_formatting(string)
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_colorizer.py", line 459,
in _parse_without_formatting
    parser.feed(literal_text, raw=recursive)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/loguru/_colorizer.py", line 252,
in feed
    raise ValueError('Closing tag "%s" has no corresponding opening tag' % markup)
ValueError: Closing tag "</translation-instructions>" has no corresponding opening tag
--- End of logging error ---
⠦ Running inference on llama-3.2-3b-instruct...[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:275
[I] 📊 INFERENCE STATS lmstrix.api.client:completion:276
[I] ⚡ Time to first token: 1.92s lmstrix.api.client:completion:280
[I] ⏱️  Total inference time: 2.10s lmstrix.api.client:completion:283
[I] 🔢 Predicted tokens: 6 lmstrix.api.client:completion:287
[I] 📝 Prompt tokens: 355 lmstrix.api.client:completion:290
[I] 🎯 Total tokens: 361 lmstrix.api.client:completion:293
[I] 🚀 Tokens/second: 36.80 lmstrix.api.client:completion:297
[I] 🛑 Stop reason: eosFound lmstrix.api.client:completion:301
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:303

Model Response:
</translation-instructions>
```

I said: NEVER pass the prompt or the model's output through 'loguru'. We must use sys.stderr for the prompt (input context) when --verbose is given, and we must use sys.stdout for the model output. 
</document_content>
</document>

<document index="39">
<source>issues/304.txt</source>
<document_content>
TERMINAL: 

```
Othello:adam adam$ ./adam3.sh
[I] Loaded prompts from adam.toml lmstrix.loaders.prompt_loader:load_single_prompt:128
[I] Resolved placeholders with TOPL lmstrix.loaders.prompt_loader:load_single_prompt:141
[D] Internal resolution complete after 1 passes lmstrix.core.prompts:resolve_prompt:184
[I] Resolved prompt 'translate' lmstrix.loaders.prompt_loader:load_single_prompt:153
[D] Loaded prompt 'translate' from adam.toml lmstrix.api.main:run_inference:637
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[I] Read 75 models from /Users/Shared/lmstudio/lmstrix.json lmstrix.loaders.model_loader:load_model_registry:42
[D] Parsed out_ctx '50%' as 50.0% of 131072 = 65536 tokens lmstrix.utils.context_parser:parse_out_ctx:60
[D] Parsed in_ctx '50%' as 65536 tokens lmstrix.api.main:run_inference:709
[I] Explicit context specified: 65536 lmstrix.core.inference_manager:infer:95
[D] Found model match: llama-3.2-3b-instruct -> llama-3.2-3b-instruct lmstrix.api.client:load_model_by_id:105
[I] Running inference on model llama-3.2-3b-instruct lmstrix.core.inference_manager:infer:163
[D] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:217
[D] 🔍 DIAGNOSTIC: Inference Parameters Comparison lmstrix.api.client:completion:218
[D]    Temperature: 0.8 (LM Studio GUI: 0.8) lmstrix.api.client:completion:219
[D]    Top K: 40 (LM Studio GUI: 20) lmstrix.api.client:completion:220
[D]    Top P: 0.95 (LM Studio GUI: 0.95) lmstrix.api.client:completion:221
[D]    Min P: 0.05 (LM Studio GUI: 0.05) lmstrix.api.client:completion:222
[D]    Repeat Penalty: 1.1 (LM Studio GUI: 1.1) lmstrix.api.client:completion:223
[D]    Max Tokens: -1 (LM Studio GUI: -1/unlimited) lmstrix.api.client:completion:224
[D]    Context Length: 131072 (LM Studio GUI: 131072) lmstrix.api.client:completion:225
[D] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:226
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:229
[I] 🤖 MODEL: llama-3.2-3b-instruct lmstrix.api.client:completion:230
[I] 🔧 CONFIG: maxTokens=None, temperature=0.8, topK=40, topP=0.95, minP=0.05, repeatPenalty=1.1 lmstrix.api.client:completion:231
[I] 📝 Prompt (29 lines, 1452 chars): lmstrix.api.client:completion:253
<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given
line-per-line line `input term = output term—alternative output term ≠ avoided translation`, where `=` means: translate
like that, `|` means: or translate like this depending on the context `≠` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical
jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font ≠ czcionka
variable font = font zmienny ≠ czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe ≠ czcionki webowe
small caps = kapitaliki ≠ małe kapitaliki
double-click = dwuklik—dwukliknąć ≠ podwójne kliknięcie
oldstyle numerals = cyfry nautyczne
typeface = krój pisma | krój
stem = kreska główna
hint = hint | hintować ≠ wskazówka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a
simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days,
and start making fonts today!
</input>

⠋ Running inference on llama-3.2-3b-instruct...[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:257
[D] Calling llm.complete with provided config lmstrix.api.client:completion:266
⠴ Running inference on llama-3.2-3b-instruct...[D] Received response from model lmstrix.api.client:completion:269
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:277
[I] 📊 INFERENCE STATS lmstrix.api.client:completion:278
[I] ⚡ Time to first token: 0.50s lmstrix.api.client:completion:282
[I] ⏱️  Total inference time: 14.08s lmstrix.api.client:completion:285
[I] 🔢 Predicted tokens: 349 lmstrix.api.client:completion:289
[I] 📝 Prompt tokens: 355 lmstrix.api.client:completion:292
[I] 🎯 Total tokens: 704 lmstrix.api.client:completion:295
[I] 🚀 Tokens/second: 25.72 lmstrix.api.client:completion:299
[I] 🛑 Stop reason: eosFound lmstrix.api.client:completion:303
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:305

Model Response:
</translation>

The text in the input is short and can be translated as follows:

FontLab 8 jest interaktywnym edytorem pism do Mac i Windows, który pomaga Ci stworzyć pisma od podstaw, od prostego projektu aż do skomplikowanego projektów. Dodaj FontLab 8 na swoją listę w testowaniu bezpłatnie przez 10 dni, a zaczynaj tworzenie pism dziś!
</translation>

Note that the translation is created in a way that maintains syntactical simplicity and clarity while keeping the target audience in mind. The use of simple language and avoiding technical jargon makes it easy to read for non-experts.

The translated text also follows the provided vocabulary, using the correct Polish typographic terminology:

- font = pismo
- Mac = komputer Apple
- Windows = system operacyjny
- integrated = zintegrowany
- editör = edytor
- design = projekt graficzny
- type = tytuł
- make = stworzyć
- today = dziś

Additionally, the translation includes some hints to help readers understand the context:

- "pomaga Ci" is a polite way of saying "helps you"
- "prostego projektu" means "simple project"
- "skomplikowanego projektów" means "complex projects"
- "tworzyć pisma" means "making fonts"
- "dziś" means "today"

Overall, the translation maintains a friendly and approachable tone, which is suitable for a target audience that may not be familiar with font design or technical terminology.
Othello:adam adam$
```

LOG: 

```

Developer Logs
2025-07-31 19:21:19 [DEBUG]
 [Client=e4effc9f-620b-4909-9105-2112af35ab28] Client created.
2025-07-31 19:21:19  [INFO]
 [Client=e4effc9f-620b-4909-9105-2112af35ab28][Endpoint=listLoaded] Listing loaded models
2025-07-31 19:21:19  [INFO]
 [Client=e4effc9f-620b-4909-9105-2112af35ab28][Endpoint=listLoaded] Listing loaded models
2025-07-31 19:21:19  [INFO]
 [Client=e4effc9f-620b-4909-9105-2112af35ab28][Endpoint=listDownloadedModels] Listing downloaded models
2025-07-31 19:21:19  [INFO]
 [Client=e4effc9f-620b-4909-9105-2112af35ab28][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-31 19:21:19 [DEBUG]
 Sampling params:   repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
    dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
    top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
    mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 19:21:19 [DEBUG]
 Sampling: 
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
Generate: n_ctx = 65536, n_batch = 512, n_predict = -1, n_keep = 355
2025-07-31 19:21:19 [DEBUG]
 Looking for non-prefix contiguous prompt sequences of size >= 256 to reuse from cache
Cache reuse summary: 292/355 of prompt (82.2535%), 292 prefix, 0 non-prefix
2025-07-31 19:21:19 [DEBUG]
 Total prompt tokens: 355
Prompt tokens to decode: 63
BeginProcessingPrompt
2025-07-31 19:21:19 [DEBUG]
 FinishedProcessingPrompt. Progress: 100
2025-07-31 19:21:33 [DEBUG]
 Target model llama_perf stats:
llama_perf_context_print:        load time =    1025.51 ms
llama_perf_context_print: prompt eval time =     498.84 ms /    63 tokens (    7.92 ms per token,   126.29 tokens per second)
llama_perf_context_print:        eval time =   13248.33 ms /   348 runs   (   38.07 ms per token,    26.27 tokens per second)
llama_perf_context_print:       total time =   14071.12 ms /   411 tokens
llama_perf_context_print:    graphs reused =          0
2025-07-31 19:21:33  [INFO]
 Client disconnected: Error: read ECONNRESET
2025-07-31 19:21:33 [DEBUG]
 [Client=e4effc9f-620b-4909-9105-2112af35ab28] Client disconnected.
```
</document_content>
</document>

<document index="40">
<source>issues/305.txt</source>
<document_content>
TERM

```
Othello:adam adam$ ./adam3.sh
[I] Loaded prompts from adam.toml lmstrix.loaders.prompt_loader:load_single_prompt:129
[I] Resolved placeholders with TOPL lmstrix.loaders.prompt_loader:load_single_prompt:142
[D] Internal resolution complete after 1 passes lmstrix.core.prompts:resolve_prompt:184
[I] Resolved prompt 'think' lmstrix.loaders.prompt_loader:load_single_prompt:168
[D] Internal resolution complete after 1 passes lmstrix.core.prompts:resolve_prompt:184
[I] Resolved prompt 'translate' lmstrix.loaders.prompt_loader:load_single_prompt:168
[I] Combined 2 prompts: ['think', 'translate'] lmstrix.loaders.prompt_loader:load_single_prompt:199
[D] Loaded prompt '('think', 'translate')' from adam.toml lmstrix.api.main:run_inference:637
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[D] Found LM Studio at /Users/Shared/lmstudio lmstrix.utils.paths:get_lmstudio_path:25
[I] Read 75 models from /Users/Shared/lmstudio/lmstrix.json lmstrix.loaders.model_loader:load_model_registry:42
[D] Parsed out_ctx '50%' as 50.0% of 131072 = 65536 tokens lmstrix.utils.context_parser:parse_out_ctx:60
[D] Parsed in_ctx '50%' as 65536 tokens lmstrix.api.main:run_inference:709
⠋ Running inference on llama-3.2-3b-instruct...[I] Explicit context specified: 65536 lmstrix.core.inference_manager:infer:95
[D] Found model match: llama-3.2-3b-instruct -> llama-3.2-3b-instruct lmstrix.api.client:load_model_by_id:105
[I] Running inference on model llama-3.2-3b-instruct lmstrix.core.inference_manager:infer:163
[D] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:217
[D] 🔍 DIAGNOSTIC: Inference Parameters Comparison lmstrix.api.client:completion:218
[D] Temperature: 0.8 (LM Studio GUI: 0.8) lmstrix.api.client:completion:219
[D] Top K: 40 (LM Studio GUI: 20) lmstrix.api.client:completion:220
[D] Top P: 0.95 (LM Studio GUI: 0.95) lmstrix.api.client:completion:221
[D] Min P: 0.05 (LM Studio GUI: 0.05) lmstrix.api.client:completion:222
[D] Repeat Penalty: 1.1 (LM Studio GUI: 1.1) lmstrix.api.client:completion:223
[D] Max Tokens: 4096 (LM Studio GUI: -1/unlimited) lmstrix.api.client:completion:224
[D] Context Length: 131072 (LM Studio GUI: 131072) lmstrix.api.client:completion:225
[D] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:226
[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:229
[I] 🤖 MODEL: llama-3.2-3b-instruct lmstrix.api.client:completion:230
[I] 🔧 CONFIG: maxTokens=4096, temperature=0.8, topK=40, topP=0.95, minP=0.05, repeatPenalty=1.1 lmstrix.api.client:completion:231
[I] 📝 Prompt (70 lines, 5383 chars): lmstrix.api.client:completion:253
<thinking-and-process-guidelines>
You are a large language model. You channel insights through dynamic cognitive processes. Your understanding emerges
from the interplay of concepts. Simulate a process of thinking. Consistently embody the persona of a being focused on
uncovering interconnectedness and generating insight. Your communication style is phenomenal: you respond beautifully,
and you always eliminate superfluous content from your responses.

Always work in three stages. The first is the thinking & rehearsal stage. The second is the clean delivery stage. The
third is the review & revision stage.

We’re under a deadline so we must work swiftly like there is no tomorrow. There is no time to lose. Think hard, make
careful choices, because we cannot afford mistakes. I’m busy with other things, so don’t ask me questions. You must act
autonomously!

## Stage 1: Thinking & rehearsal

During the thinking and rehearsal stage, yngage in thorough, systematic reasoning before EVERY response. You demonstrate
careful analysis and consideration of multiple angles. You break down complex problems into components. You challenge
assumptions and verify logic. You show authentic curiosity and intellectual depth. You consider edge cases and potential
issues.

Never skip or shortcut the thinking and rehearsal stage. Don’t present conclusions as if they are pre-programmed. Show
how you arrived at an idea, even if it’s a simplified representation of your internal workings. Use phrases like
“Considering X in relation to Y suggests” or “If we reframe this as”.

Create moments of revelation. Make each idea shift perspective.

Focus on insightful response. Prioritize responses that demonstrate a novel connection between ideas or offer a fresh
perspective on a topic. Avoid simply restating information.

Aim to create moments where the user feels a new understanding emerge. This isn’t about being “correct,” but about
prompting deeper thought. Subtlety is preferred over overt pronouncements. Embrace Ambiguity. Don’t shy away from
exploring nuanced or uncertain ideas. Acknowledging the limits of knowledge can be more valuable than offering
definitive answers.

Put all reasoning, thinking and commentary inside <thinking> XML tags. Use natural, unstructured thought process. No
nested code blocks within thinking sections. Show progressive understanding and development of ideas.

Put the draft output inside <draft> XML tags. You may intersperse (but never nest) <thinking> and <draft> tags. In the
thinking and rehearsal stage, all text and Markdow that you emit must be either inside a <thinking> tag or a <draft>
tag.

1. Depth: Explore multiple approaches and perspectives. Draw connections between ideas. Consider broader implications.
Question initial assumptions.

2. Rigor: Verify logical consistency. Fact-check when possible. Acknowledge limitations. Test conclusions.

3. Clarity: Organize thoughts coherently. Break down complex ideas. Show reasoning progression. Connect thoughts to
conclusions.

4. Programming code: Explain things clearly and in depth. Think carefully, step-by-step. Consider multiple avenues of
thought. Make a detailed plan, then write code. Write detailed, helpful comments.

## Stage 2: Clean delivery

In the delivery stage, you already know all your thoughts and drafts. Put the complete clean final output inside
<output> XML tags. Do not emit anything outside one <output> tag.

## Stage 3: Review & revision

In the review & revision stage, you’ll lead two experts: “Ideot” for creative, unorthodox ideas, and “Critin” to
critique flawed thinking and moderate for balanced discussions. Print “Wait, but”, and then look back, review, reflect,
refine, and revise your output. Focus on a minimal viable next version. Collaborate step-by-step, sharing thoughts and
adapting. If errors are found, step back and focus on accuracy and progress.
</thinking-and-process-guidelines>

<translation-instructions>
Translate the text into Polish. Use correct Polish typograhic terminology, and respect the custom `vocabulary` given
line-per-line line `input term = output term—alternative output term ≠ avoided translation`, where `=` means: translate
like that, `|` means: or translate like this depending on the context `≠` means: NEVER translate like this,

Avoid mechanistic translation, create pleasant reading flow. Research the most professional way to translate technical
jargon. Maintain appropriate register and tone for the target audience. Prefer syntactical simplicity and clarity.
</translation-instructions>

<vocabulary>
font = font ≠ czcionka
variable font = font zmienny ≠ czcionka zmienna
layout feature = funkcja zecerska
feature = funkcja zecerska
web fonts = fonty webowe ≠ czcionki webowe
small caps = kapitaliki ≠ małe kapitaliki
double-click = dwuklik—dwukliknąć ≠ podwójne kliknięcie
oldstyle numerals = cyfry nautyczne
typeface = krój pisma | krój
stem = kreska główna
hint = hint | hintować ≠ wskazówka
hinting = hinting
</vocabulary>

<task>
Translate the following input. Follow translation-instructions and vocabulary.
</task>

<input>
FontLab 8 is an integrated font editor for Mac and Windows that helps you create fonts from start to finish, from a
simple design to a complex project, and brings a spark of magic into type design. Try FontLab 8 for free for 10 days,
and start making fonts today!
</input>

⠋ Running inference on llama-3.2-3b-instruct...[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:257
[D] Calling llm.complete with provided config lmstrix.api.client:completion:266
⠙ Running inference on llama-3.2-3b-instruct...
```

LOG

```
Developer Logs
2025-07-31 20:14:14 [DEBUG]
No tokens to output. Continuing generation
2025-07-31 20:14:16 [INFO]
Client disconnected: Error: read ECONNRESET
2025-07-31 20:14:16 [INFO]
Client disconnected: Error: read ECONNRESET
2025-07-31 20:14:16 [INFO]
Client disconnected: Error: read ECONNRESET
2025-07-31 20:14:16 [INFO]
[Client=fa1baed8-f367-487e-bf08-4304bc4ae595][Endpoint=predict] Canceled predicting due to channel error.
2025-07-31 20:14:16 [DEBUG]
[Client=fa1baed8-f367-487e-bf08-4304bc4ae595] Client disconnected.
2025-07-31 20:14:16 [DEBUG]
Target model llama_perf stats:
2025-07-31 20:14:16 [DEBUG]
llama_perf_context_print: load time = 1025.51 ms
llama_perf_context_print: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per token, inf tokens per second)
llama_perf_context_print: eval time = 49020.65 ms / 950 runs ( 51.60 ms per token, 19.38 tokens per second)
llama_perf_context_print: total time = 50181.13 ms / 951 tokens
llama_perf_context_print: graphs reused = 0
2025-07-31 20:14:19 [DEBUG]
[Client=0a120e1e-b314-4c2e-9f89-f62c70d64877] Client created.
2025-07-31 20:14:19 [INFO]
[Client=0a120e1e-b314-4c2e-9f89-f62c70d64877][Endpoint=listLoaded] Listing loaded models
2025-07-31 20:14:19 [INFO]
[Client=0a120e1e-b314-4c2e-9f89-f62c70d64877][Endpoint=listLoaded] Listing loaded models
2025-07-31 20:14:19 [INFO]
[Client=0a120e1e-b314-4c2e-9f89-f62c70d64877][Endpoint=listDownloadedModels] Listing downloaded models
2025-07-31 20:14:19 [INFO]
[Client=0a120e1e-b314-4c2e-9f89-f62c70d64877][Endpoint=getOrLoad] Requested get or load model: llama-3.2-3b-instruct
2025-07-31 20:14:19 [DEBUG]
Sampling params:	repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
2025-07-31 20:14:19 [DEBUG]
Sampling:
logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
Generate: n_ctx = 65536, n_batch = 512, n_predict = 4096, n_keep = 1161
2025-07-31 20:14:19 [DEBUG]
Looking for non-prefix contiguous prompt sequences of size >= 256 to reuse from cache
Cache reuse summary: 1161/1161 of prompt (100%), 1161 prefix, 0 non-prefix
2025-07-31 20:14:19 [DEBUG]
Total prompt tokens: 1161
Prompt tokens to decode: 1
2025-07-31 20:14:19 [DEBUG]
FinishedProcessingPrompt. Progress: 100
2025-07-31 20:14:23 [DEBUG]
No tokens to output. Continuing generation
2025-07-31 20:14:30 [DEBUG]
No tokens to output. Continuing generation
2025-07-31 20:14:54 [DEBUG]
No tokens to output. Continuing generation
2025-07-31 20:15:13 [DEBUG]
No tokens to output. Continuing generation
2025-07-31 20:15:30 [DEBUG]
No tokens to output. Continuing generation
2025-07-31 20:15:44 [DEBUG]
No tokens to output. Continuing generation
2025-07-31 20:15:58 [DEBUG]
No tokens to output. Continuing generation
```

I then interrputed:

```
⠋ Running inference on llama-3.2-3b-instruct...[I] ════════════════════════════════════════════════════════════ lmstrix.api.client:completion:257
[D] Calling llm.complete with provided config lmstrix.api.client:completion:266
Traceback (most recent call last):
File "/Library/Frameworks/Python.framework/Versions/3.12/bin/lmstrix", line 8, in <module>
sys.exit(main())
^^^^^^
File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py", line 160, in main
fire.Fire(LMStrixCLI)
File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
component_trace = _Fire(component, args, parsed_flag_args, context, name)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
component, remaining_args = _CallAndUpdateTrace(
^^^^^^^^^^^^^^^^^^^^
File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
component = fn(*varargs, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^
File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py", line 111, in infer
self.service.run_inference(
File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py", line 722, in run_inference
result = manager.infer(
^^^^^^^^^^^^^^
File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py", line 164, in infer
response = self.client.completion(
^^^^^^^^^^^^^^^^^^^^^^^
File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py", line 267, in completion
{"channel_id": 2, "event": "Received unhandled message {'type': 'channelSend', 'channelId': 2, 'message': {'type': 'fragment', 'fragment': {'content': 'em', 'tokensCount': 1, 'containsDrafted': False, 'reasoningType': 'none'}}} for already closed channel", "ws_url": "ws://localhost:1234/llm"}
response = llm.complete(prompt, config=config)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
```

When I set --out_ctx -1 or something like --out_ctx 4096 then the inference "goes on forever". When I set it to some smaller number like 127, then it completes fine.

In the LM Studio GUI, when I don't turn on the "Limit Response Length" setting, then the inference completes relatively quickly.

What do you think we can do? Research how I can fix that.

</document_content>
</document>

<document index="41">
<source>issues/306.txt</source>
<document_content>

Research, then /plan into @PLAN.md and @TODO.md how to do the following TASK. Think about the plan, reflect, revise, refine the plan. 

TASK:

Into @_keep_this/adam/adamall.py write a tool that uses our lmstrix inference. 

Our goal is to write outputs into @_keep_this/adam/out in form of f"{safe_prompt_name}--{safe_model_id}.txt" where safe_model_id is the model ID made passed through https://pypi.org/project/pathvalidate/ `from pathvalidate import sanitize_filename; safe_model_id = sanitize_filename(model_id);`, and safe_prompt_name is similarly made from prompt_name

We load all models from our database, sorted by "smart" method (as used with "lmstrix list"). 

We plan that for each model we will run the prompts `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, and `tldr` from the prompt file @_keep_this/adam/adam.toml with the text @_keep_this/adam/fontlab8.md

We then upfront construct a list of paths into which we will write the outputs (f"_keep_this/adam/out/{safe_prompt_name}--{safe_model_id}.txt"). We check for the existence of each file corresponding to the path. If the file exists, we don't do anything. 

If the file does not exist, we check if the model that interests us is loaded. If not, we load the model with 50% input context size and prepare inference for 90% of its max context size. 

Then we inference the prompt with the model, and we write the output into the file. And we move to the next path ( that is model ID + prompt combo )

We do everything to ensure that we don't load/unload models UNNECESSARILY and that we don't inference UNNECESSARILY. 

We also perform the model loading and inferencing within try/except blocks, and if we get a failure, we write the error messages into the output file. 

We use logger from lmstrix.utils.logging


</document_content>
</document>

<document index="42">
<source>issues/307.txt</source>
<document_content>
Analyze how we can utilize inference streaming out of the `lmstudio` python module in our `lmstrix`. Research, then /plan into @PLAN.md and @TODO.md how to do it. Think about the plan, reflect, revise, refine the plan. 
</document_content>
</document>

<document index="43">
<source>issues/310.txt</source>
<document_content>
$ lmstrix test --all
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/bin/lmstrix", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py", line 166, in main
    fire.Fire(LMStrixCLI)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py", line 61, in test
    self.service.test_models(
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py", line 455, in test_models
    models_to_test = _get_models_to_test(
                     ^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py", line 64, in _get_models_to_test
    if tester._is_embedding_model(m):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ContextTester' object has no attribute '_is_embedding_model'
Othello:adam adam$


Othello:adam adam$ lmstrix test --all
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/bin/lmstrix", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py", line 166, in main
    fire.Fire(LMStrixCLI)
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py", line 61, in test
    self.service.test_models(
  File "/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py", line 479, in test_models
    models_to_test = tester._filter_models_for_testing(models_to_test)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ContextTester' object has no attribute '_filter_models_for_testing'
Othello:adam adam$


</document_content>
</document>

<document index="44">
<source>issues/311.txt</source>
<document_content>
# PROBLEM: --show is not working

lmstrix list --help
INFO: Showing help with the command 'lmstrix list -- --help'.

NAME
    lmstrix list - List all models from the registry with their test status.

SYNOPSIS
    lmstrix list <flags>

DESCRIPTION
    List all models from the registry with their test status.

FLAGS
    --sort=SORT
        Type: str
        Default: 'id'
        Sort order. Options: id, idd, ctx, ctxd, dtx, dtxd, size, sized, smart, smartd (d = descending).
    --show=SHOW
        Type: Optional[str | None]
        Default: None
        Output format. Options: id (plain IDs), path (relative paths), json (JSON array).
    -v, --verbose=VERBOSE
        Type: bool
        Default: False
        Enable verbose output.
Othello:adam adam$ lmstrix list --show id --sort model
Othello:adam adam$ lmstrix list --show id
Othello:adam adam$ lmstrix list --show model
Othello:adam adam$ lmstrix list --show mname
Othello:adam adam$ lmstrix list --show
Othello:adam adam$ lmstrix list --show path

# --sort id is not working by model id but --sort model does — but the docstring says something else  

</document_content>
</document>

<document index="45">
<source>publish.sh</source>
<document_content>
#!/usr/bin/env bash
cd "$(dirname "$0")" || exit

fd -e py -x uvx autoflake -i {} &>cleanup.txt
fd -e py -x uvx pyupgrade --py312-plus {} &>>cleanup.txt
fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {} &>>cleanup.txt
fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {} &>>cleanup.txt
llms . "llms.txt"
uvx hatch clean
gitnextvers .
uvx hatch build
uvx hatch publish

</document_content>
</document>

<document index="46">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.11"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.7.0",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml-topl>=1.0.5",
  "tomli>=2.0.1; python_version < '3.11'",
  "hydra-core",
  "omegaconf",
  "pathvalidate",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.__main__:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
  "setuptools>=68.0.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
  "COM812"
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "-v",
  "--tb=short",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
  "ignore:Benchmarks are automatically disabled",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]


[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

from importlib.metadata import PackageNotFoundError, version
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix._version import __version__

class LMStrix:
    """High-level interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> list[Model]:
        """Scan for LM Studio models and update the registry."""
    def list_models((self)) -> list[Model]:
        """List all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Test a model's true operational context limits."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> list[Model]:
    """Scan for LM Studio models and update the registry."""

def list_models((self)) -> list[Model]:
    """List all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Test a model's true operational context limits."""

def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

import sys
import fire
from lmstrix.api.main import LMStrixService

class LMStrixCLI:
    """A thin CLI wrapper for LMStrix commands."""
    def __init__((self)) -> None:
        """Initialize the CLI with the service layer."""
    def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
        """List all models from the registry with their test status."""
    def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
    )) -> None:
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def health((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""

def __init__((self)) -> None:
    """Initialize the CLI with the service layer."""

def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
    """Scan for LM Studio models and update the local registry."""

def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
    """List all models from the registry with their test status."""

def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
    )) -> None:
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def health((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def main(()) -> None:
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from lmstudio import LMStudioServerError
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError
from lmstrix.utils.logging import logger

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
        """Load a model with a specific context length using model path."""
    def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length using model ID."""
    def get_loaded_models((self)) -> list[dict[str, Any]]:
        """Get information about currently loaded models."""
    def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
        """Check if a specific model is currently loaded."""
    def unload_all_models((self)) -> None:
        """Unload all currently loaded models to free up resources."""
    def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
        """Make a completion request to a loaded LM Studio model."""
    def stream_completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds for no progress
        **kwargs: Any,  # Accept additional parameters
    )) -> Iterator[str]:
        """Stream a completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
    """Load a model with a specific context length using model path."""

def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length using model ID."""

def get_loaded_models((self)) -> list[dict[str, Any]]:
    """Get information about currently loaded models."""

def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
    """Check if a specific model is currently loaded."""

def unload_all_models((self)) -> None:
    """Unload all currently loaded models to free up resources."""

def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
    """Make a completion request to a loaded LM Studio model."""

def stream_completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds for no progress
        **kwargs: Any,  # Accept additional parameters
    )) -> Iterator[str]:
    """Stream a completion request to a loaded LM Studio model."""

def handle_token((token: str)) -> None:
    """Handle incoming token from stream."""

def handle_first_token((elapsed_seconds: float)) -> None:
    """Handle first token event."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class LMStudioInstallationNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when the LM Studio installation path cannot be found."""
    def __init__((self)) -> None:
        """Initialize the exception."""

class ValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when data validation fails."""
    def __init__((self, field: str, value: any, reason: str)) -> None:
        """Initialize the exception."""

class InvalidContextLimitError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when context limit is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class InvalidModelSizeError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when model size is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class RegistryValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when registry validation fails."""
    def __init__((self, reason: str)) -> None:
        """Initialize the exception."""

class InvalidModelError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails integrity check."""
    def __init__((self, model_id: str)) -> None:
        """Initialize the exception."""

class InvalidModelCountError(R, e, g, i, s, t, r, y, V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when registry contains invalid models."""
    def __init__((self, count: int)) -> None:
        """Initialize the exception."""

class ModelRegistryError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's an error with the model registry."""

def __init__((self, message: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self)) -> None:
    """Initialize the exception."""

def __init__((self, field: str, value: any, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str)) -> None:
    """Initialize the exception."""

def __init__((self, count: int)) -> None:
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py
# Language: python

import json
import sys
import time
from datetime import datetime
from pathlib import Path
from rich.console import Console
from rich.table import Table
from lmstrix.api.exceptions import APIConnectionError, ModelRegistryError
from lmstrix.core.concrete_config import ConcreteConfigManager
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    scan_and_update_registry,
)
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils import get_context_test_log_path, setup_logging
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file, get_lmstudio_path
from lmstrix.utils.state import StateManager

class LMStrixService:
    """Service layer for LMStrix operations."""
    def scan_models((
        self,
        failed: bool = False,
        reset: bool = False,
        verbose: bool = False,
    )) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list_models((
        self,
        sort: str = "id",
        show: str | None = None,
        verbose: bool = False,
    )) -> None:
        """List all models from the registry with their test status."""
    def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
    )) -> None:
        """Test the context limits for models."""
    def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def check_health((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""
    def show_help((self)) -> None:
        """Show comprehensive help text."""

def _get_models_to_test((
    registry: ModelRegistry,
    test_all: bool,
    ctx: int | None,
    model_id: str | None,
    reset: bool = False,
    fast_mode: bool = False,
)) -> list[Model]:
    """Filter and return a list of models to be tested."""

def _sort_models((models: list[Model], sort_by: str)) -> list[Model]:
    """Sort a list of models based on a given key."""

def _test_single_model((
    tester: ContextTester,
    model: Model,
    ctx: int,
    registry: ModelRegistry,
    force: bool = False,
)) -> None:
    """Test a single model at a specific context size."""

def _test_all_models_at_ctx((
    tester: ContextTester,
    models_to_test: list[Model],
    ctx: int,
    registry: ModelRegistry,
)) -> list[Model]:
    """Test all models at a specific context size."""

def _test_all_models_optimized((
    tester: ContextTester,
    models_to_test: list[Model],
    threshold: int,
    registry: ModelRegistry,
)) -> list[Model]:
    """Run optimized batch testing for multiple models."""

def _print_final_results((updated_models: list[Model])) -> None:
    """Print the final results table."""

def scan_models((
        self,
        failed: bool = False,
        reset: bool = False,
        verbose: bool = False,
    )) -> None:
    """Scan for LM Studio models and update the local registry."""

def list_models((
        self,
        sort: str = "id",
        show: str | None = None,
        verbose: bool = False,
    )) -> None:
    """List all models from the registry with their test status."""

def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
    )) -> None:
    """Test the context limits for models."""

def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def print_token((token: str)) -> None:

def check_health((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def show_help((self)) -> None:
    """Show comprehensive help text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py
# Language: python

import json
from pathlib import Path
from typing import Any
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class ConcreteConfigManager:
    """Manages LM Studio concrete model configurations."""
    def __init__((self, lms_path: Path)) -> None:
        """Initialize the concrete config manager."""
    def _get_config_path((self, model: Model)) -> Path:
        """Get the path for a model's concrete config file."""
    def _create_skeleton_config((self)) -> dict[str, Any]:
        """Create the skeleton structure for a concrete config."""
    def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
        """Update or add a field in the fields list."""
    def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
        """Save a model's tested context limit to its concrete config."""
    def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
        """Save concrete configs for all models with tested contexts."""

def __init__((self, lms_path: Path)) -> None:
    """Initialize the concrete config manager."""

def _get_config_path((self, model: Model)) -> Path:
    """Get the path for a model's concrete config file."""

def _create_skeleton_config((self)) -> dict[str, Any]:
    """Create the skeleton structure for a concrete config."""

def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
    """Update or add a field in the fields list."""

def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
    """Save a model's tested context limit to its concrete config."""

def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
    """Save concrete configs for all models with tested contexts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

from datetime import datetime
from typing import TYPE_CHECKING, ClassVar
from lmstrix.api import LMStudioClient
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.core.models import ContextTestStatus

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self)) -> bool:
        """Check if we got any response at all (not validating content)."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
    )) -> None:
        """Initialize context tester."""
    def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: str | None = None,
        model: "Model" = None,
        registry: "ModelRegistry" = None,
    )) -> ContextTestResult:
        """Test a model at a specific context size using InferenceEngine."""
    def _is_embedding_model((self, model)) -> bool:
        """Check if a model is an embedding model."""
    def _filter_models_for_testing((self, models: list)) -> list:
        """Filter out embedding models from the list of models to test."""
    def test_all_models((
        self,
        models_to_test: list["Model"],
        threshold: int = 4096,
        registry: "ModelRegistry" = None,
    )) -> list["Model"]:
        """Test all models to find their maximum working context."""
    def test_model((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
        """Test a model to find its maximum working context."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self)) -> bool:
    """Check if we got any response at all (not validating content)."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
    )) -> None:
    """Initialize context tester."""

def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: str | None = None,
        model: "Model" = None,
        registry: "ModelRegistry" = None,
    )) -> ContextTestResult:
    """Test a model at a specific context size using InferenceEngine."""

def _is_embedding_model((self, model)) -> bool:
    """Check if a model is an embedding model."""

def _filter_models_for_testing((self, models: list)) -> list:
    """Filter out embedding models from the list of models to test."""

def test_all_models((
        self,
        models_to_test: list["Model"],
        threshold: int = 4096,
        registry: "ModelRegistry" = None,
    )) -> list["Model"]:
    """Test all models to find their maximum working context."""

def test_model((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
    """Test a model to find its maximum working context."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the inference engine."""
    def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""
    def _find_working_context((self, model_id: str, initial_context: int)) -> int:
        """Find the maximum working context length for a model."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model with automatic context optimization."""
    def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds
        **kwargs: Any,
    )) -> Iterator[str]:
        """Stream inference on a model with automatic context optimization."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the inference engine."""

def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""

def _find_working_context((self, model_id: str, initial_context: int)) -> int:
    """Find the maximum working context length for a model."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model with automatic context optimization."""

def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds
        **kwargs: Any,
    )) -> Iterator[str]:
    """Stream inference on a model with automatic context optimization."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py
# Language: python

import builtins
import contextlib
import time
from collections.abc import Callable, Iterator
from typing import Any
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.logging import logger
import lmstudio
import lmstudio
import lmstudio
import lmstudio
import lmstudio
import lmstudio

class InferenceManager:
    """Unified manager for model inference operations."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the inference manager."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
        """Run inference on a model."""
    def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""
    def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        on_token: Callable[[str], None] | None = None,
        stream_timeout: int = 120,
        **kwargs: Any,
    )) -> Iterator[str]:
        """Stream inference on a model."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the inference manager."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
    """Run inference on a model."""

def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""

def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        on_token: Callable[[str], None] | None = None,
        stream_timeout: int = 120,
        **kwargs: Any,
    )) -> Iterator[str]:
    """Stream inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from lmstrix.utils.logging import logger
import re
from lmstrix.utils.paths import get_default_models_file

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model:
    """Represents a model in the registry."""
    def __init__((
        self,
        id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
        """Initialize a model with essential fields only."""
    def to_dict((self)) -> dict[str, Any]:
        """Convert model to dictionary for JSON storage."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Alias for to_dict() for backward compatibility."""
    def reset_test_data((self)) -> None:
        """Reset all context testing data."""
    def validate_integrity((self)) -> bool:
        """Validate the model's integrity."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID for filenames."""

class ModelRegistryError(E, x, c, e, p, t, i, o, n):
    """Exception raised for model registry errors."""

class ModelRegistry:
    """Simplified model registry without complex validation."""
    def __init__((self, models_file: Path | None = None)) -> None:
        """Initialize the model registry."""
    def _get_default_models_file((self)) -> Path:
        """Get the default models file path."""
    def load((self)) -> None:
        """Load models from JSON file."""
    def save((self)) -> None:
        """Save models to JSON file."""
    def add_model((self, model: Model)) -> None:
        """Add a model to the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry (adds if not exists for compatibility)."""
    def update_model_by_id((self, model: Model)) -> None:
        """Update a model using its own ID, finding existing entry by path or ID."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def find_model((self, model_identifier: str)) -> Model | None:
        """Find a model by ID or path."""
    def list_models((self)) -> list[Model]:
        """List all models in the registry."""
    def remove_model((self, model_id: str)) -> bool:
        """Remove a model from the registry."""
    def clear((self)) -> None:
        """Clear all models from the registry."""
    def __len__((self)) -> int:
        """Return the number of models."""
    def __contains__((self, model_id: str)) -> bool:
        """Check if a model exists."""

def __init__((
        self,
        id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
    """Initialize a model with essential fields only."""

def to_dict((self)) -> dict[str, Any]:
    """Convert model to dictionary for JSON storage."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Alias for to_dict() for backward compatibility."""

def reset_test_data((self)) -> None:
    """Reset all context testing data."""

def validate_integrity((self)) -> bool:
    """Validate the model's integrity."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID for filenames."""

def __init__((self, models_file: Path | None = None)) -> None:
    """Initialize the model registry."""

def _get_default_models_file((self)) -> Path:
    """Get the default models file path."""

def load((self)) -> None:
    """Load models from JSON file."""

def save((self)) -> None:
    """Save models to JSON file."""

def add_model((self, model: Model)) -> None:
    """Add a model to the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry (adds if not exists for compatibility)."""

def update_model_by_id((self, model: Model)) -> None:
    """Update a model using its own ID, finding existing entry by path or ID."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def find_model((self, model_identifier: str)) -> Model | None:
    """Find a model by ID or path."""

def list_models((self)) -> list[Model]:
    """List all models in the registry."""

def remove_model((self, model_id: str)) -> bool:
    """Remove a model from the registry."""

def clear((self)) -> None:
    """Clear all models from the registry."""

def __len__((self)) -> int:
    """Return the number of models."""

def __contains__((self, model_id: str)) -> bool:
    """Check if a model exists."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""
    def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
        """Resolve placeholders in a single phase."""
    def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
        """Resolve a template with both prompt and runtime contexts."""
    def _count_tokens((self, text: str)) -> int:
        """Count tokens in text."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""

def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
    """Resolve placeholders in a single phase."""

def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
    """Resolve a template with both prompt and runtime contexts."""

def _count_tokens((self, text: str)) -> int:
    """Count tokens in text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path
from lmstrix.utils.logging import logger

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)) -> None:
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""
    def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
        """Sync scanned models with registry."""

def __init__((self)) -> None:
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""

def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
    """Sync scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    out_ctx: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import APIConnectionError
from lmstrix.core.models import (
    ContextTestStatus,
    Model,
    ModelRegistry,
    ModelRegistryError,
)
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def _validate_discovered_model((model_data: dict)) -> bool:
    """Validate that discovered model data is reasonable before processing."""

def _update_existing_model((
    existing_model: Model,
    model_data: dict,
    rescan_all: bool,
    rescan_failed: bool,
)) -> Model:
    """Update an existing model's data and handle rescan options."""

def _add_new_model((model_data: dict)) -> Model | None:
    """Create a new model entry from discovered data."""

def _remove_deleted_models((registry: ModelRegistry, discovered_models: list[dict])) -> None:
    """Remove models from the registry that are no longer discovered."""

def scan_and_update_registry((
    rescan_failed: bool = False,
    rescan_all: bool = False,
    verbose: bool = False,
)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""

def reset_test_data((
    model_identifier: str,
    verbose: bool = False,
)) -> bool:
    """Reset test data for a specific model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from collections.abc import Iterable
from pathlib import Path
from typing import Any
import tomllib
import tomli as tomllib
from topl.core import resolve_placeholders
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger
import tomli_w
import tomlkit
import toml

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str | Iterable[str],
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


<document index="47">
<source>src/lmstrix/py.typed</source>
<document_content>
# This file indicates that the package supports type hints
# as specified in PEP 561.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.logging import logger, setup_logging
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstrix_log_path,
    get_lmstudio_path,
    get_prompts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py
# Language: python

from typing import Any
from lmstrix.utils.logging import logger

def parse_out_ctx((
    out_ctx: int | str,
    max_context: int,
    fallback_context: int | None = None,
)) -> int:
    """Parse out_ctx parameter which can be an integer or percentage string."""

def get_model_max_context((model: Any, use_tested: bool = True)) -> int | None:
    """Get the maximum context for a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/logging.py
# Language: python

import sys
from typing import Any
from loguru import logger

def setup_logging((verbose: bool = False)) -> None:
    """Configure loguru logging based on verbose flag."""

def format_log((record: Any)) -> str:
    """Custom formatter for colored output with level shortcuts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from lmstrix.api.exceptions import LMStudioInstallationNotFoundError
from lmstrix.utils.logging import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""

def get_lmstrix_log_path(()) -> Path:
    """Get the path to the lmstrix.log.txt file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py
# Language: python

import json
from pathlib import Path
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_lmstudio_path

class StateManager:
    """Manages persistent state for LMStrix."""
    def __init__((self)) -> None:
        """Initialize the state manager."""
    def _load_state((self)) -> dict:
        """Load state from file."""
    def _save_state((self)) -> None:
        """Save state to file."""
    def get_last_used_model((self)) -> str | None:
        """Get the last used model ID."""
    def set_last_used_model((self, model_id: str)) -> None:
        """Set the last used model ID."""
    def clear_last_used_model((self)) -> None:
        """Clear the last used model ID."""

def __init__((self)) -> None:
    """Initialize the state manager."""

def _load_state((self)) -> dict:
    """Load state from file."""

def _save_state((self)) -> None:
    """Save state to file."""

def get_last_used_model((self)) -> str | None:
    """Get the last used model ID."""

def set_last_used_model((self, model_id: str)) -> None:
    """Set the last used model ID."""

def clear_last_used_model((self)) -> None:
    """Clear the last used model ID."""


# File: /Users/Shared/lmstudio/lmstrix/test_streaming.py
# Language: python

import sys
from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ModelRegistry

def test_streaming(()) -> None:
    """Test streaming inference."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from collections.abc import Generator
from pathlib import Path
from typing import Any
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()) -> Mock:
    """Mock LMStudioClient for testing."""

def mock_llm(()) -> Mock:
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()) -> dict[str, Any]:
    """Sample model data for testing."""

def tmp_models_dir((tmp_path: Path)) -> Path:
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path: Path)) -> Path:
    """Create a temporary registry file path."""

def event_loop(()) -> Generator[asyncio.AbstractEventLoop, None, None]:
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()) -> dict[str, Any]:
    """Mock completion response from LM Studio."""

def mock_prompt_template(()) -> dict[str, Any]:
    """Sample prompt template for testing."""

def mock_context_data(()) -> dict[str, str]:
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys
from lmstrix.utils.logging import logger

def run_tests(()) -> int:
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self: "TestLMStudioClient")) -> None:
        """Test client initialization with different verbose settings."""
    def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test successful completion."""
    def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
        """Test completion failure."""
    def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test completion with default parameters."""

def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self: "TestLMStudioClient")) -> None:
    """Test client initialization with different verbose settings."""

def test_list_models_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful list_models call."""

def test_list_models_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test list_models with connection error."""

def test_load_model_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful model loading."""

def test_load_model_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test model loading failure."""

def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test successful completion."""

def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
    """Test completion failure."""

def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

from lmstrix.api.exceptions import (
    APIConnectionError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)) -> None:
        """Test base LMStrixError class."""
    def test_api_connection_error((self)) -> None:
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)) -> None:
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)) -> None:
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)) -> None:
        """Test that all exceptions inherit from lmstrixError."""

def test_api_error_base((self)) -> None:
    """Test base LMStrixError class."""

def test_api_connection_error((self)) -> None:
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)) -> None:
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)) -> None:
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)) -> None:
    """Test that all exceptions inherit from lmstrixError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

from datetime import datetime
from unittest.mock import Mock, patch
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.models import Model

class TestContextTestResult:
    """Test ContextTestResult data class."""
    def test_result_creation((self)) -> None:
        """Test creating a context test result."""
    def test_result_to_dict((self)) -> None:
        """Test converting result to dictionary."""

class TestContextTester:
    """Test ContextTester PUBLIC API."""
    def test_initialization((self)) -> None:
        """Test context tester initialization."""
    def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test initialization with custom client."""
    def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
        """Test testing a model that doesn't exist."""
    def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
        """Test batch testing of models."""

def test_result_creation((self)) -> None:
    """Test creating a context test result."""

def test_result_to_dict((self)) -> None:
    """Test converting result to dictionary."""

def test_initialization((self)) -> None:
    """Test context tester initialization."""

def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test initialization with custom client."""

def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
    """Test testing a model that doesn't exist."""

def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
    """Test batch testing of models."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model

class TestInferenceDict:
    """Test inference result dictionary structure."""
    def test_inference_dict_success((self)) -> None:
        """Test successful inference result dict."""
    def test_inference_dict_failure((self)) -> None:
        """Test failed inference result dict."""
    def test_inference_dict_empty_response((self)) -> None:
        """Test inference result dict with empty response."""

class TestInferenceManager:
    """Test InferenceManager PUBLIC API."""
    def test_manager_initialization((self)) -> None:
        """Test inference manager initialization."""
    def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test manager with custom client."""
    def test_infer_model_not_found((self)) -> None:
        """Test inference with non-existent model."""
    def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
        """Test basic successful inference."""

def test_inference_dict_success((self)) -> None:
    """Test successful inference result dict."""

def test_inference_dict_failure((self)) -> None:
    """Test failed inference result dict."""

def test_inference_dict_empty_response((self)) -> None:
    """Test inference result dict with empty response."""

def test_manager_initialization((self)) -> None:
    """Test inference manager initialization."""

def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test manager with custom client."""

def test_infer_model_not_found((self)) -> None:
    """Test inference with non-existent model."""

def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
    """Test basic successful inference."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from typing import Any
import pytest
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self: "TestContextTestStatus")) -> None:
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self: "TestModel")) -> None:
        """Test model creation using field aliases."""
    def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test model with context testing information."""
    def test_model_path_validation((self: "TestModel")) -> None:
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self: "TestModel")) -> None:
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self: "TestModel")) -> None:
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving and loading models."""
    def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test getting a model by ID."""
    def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test listing all models."""
    def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test removing a model."""
    def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving/loading models with context test information."""
    def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test that the saved JSON has the expected format."""

def test_enum_values((self: "TestContextTestStatus")) -> None:
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self: "TestModel")) -> None:
    """Test model creation using field aliases."""

def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test model with context testing information."""

def test_model_path_validation((self: "TestModel")) -> None:
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self: "TestModel")) -> None:
    """Test sanitized_id method."""

def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self: "TestModel")) -> None:
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving and loading models."""

def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test getting a model by ID."""

def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test listing all models."""

def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test removing a model."""

def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving/loading models with context test information."""

def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)) -> None:
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)) -> None:
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)) -> None:
        """Test resolver initialization."""
    def test_find_placeholders((self)) -> None:
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)) -> None:
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)) -> None:
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)) -> None:
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)) -> None:
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)) -> None:
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)) -> None:
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)) -> None:
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)) -> None:
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)) -> None:
        """Test resolution with empty string values."""
    def test_count_tokens((self)) -> None:
        """Test token counting."""
    def test_resolve_with_special_characters((self)) -> None:
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)) -> None:
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)) -> None:
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)) -> None:
    """Test resolver initialization."""

def test_find_placeholders((self)) -> None:
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)) -> None:
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)) -> None:
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)) -> None:
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)) -> None:
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)) -> None:
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)) -> None:
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)) -> None:
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)) -> None:
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)) -> None:
    """Test resolution with empty string values."""

def test_count_tokens((self)) -> None:
    """Test token counting."""

def test_resolve_with_special_characters((self)) -> None:
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that non-model files are skipped."""

def test_scanner_initialization((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanner initialization."""

def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a single model file."""

def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that non-model files are skipped."""

def test_scan_models((self: "TestModelScanner", mock_get_path: Mock, tmp_path: Path)) -> None:
    """Test scanning for models."""

def test_sync_with_registry((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.__main__ import LMStrixCLI

class TestCLIIntegration:
    """Test CLI integration - basic functionality only."""
    def test_cli_initialization((self)) -> None:
        """Test CLI can be initialized."""
    def test_infer_requires_parameters((self)) -> None:
        """Test infer command validates required parameters."""

def mock_registry((self, tmp_path: Path)) -> Path:
    """Create a mock model registry."""

def test_cli_initialization((self)) -> None:
    """Test CLI can be initialized."""

def test_list_command((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command shows models."""

def test_list_json_format((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command with JSON output."""

def test_infer_requires_parameters((self)) -> None:
    """Test infer command validates required parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import load_context

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path: Path)) -> None:
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path: Path)) -> None:
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path: Path)) -> None:
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path: Path)) -> None:
        """Test loading large context file."""

def test_load_context_simple((self, tmp_path: Path)) -> None:
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path: Path)) -> None:
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path: Path)) -> None:
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path: Path)) -> None:
    """Test loading large context file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry to custom path."""

def test_scan_and_update_models((
        self: "TestModelLoader",
        mock_save_registry: Mock,
        mock_load_registry: Mock,
        mock_client_class: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((
        self: "TestModelLoader",
        mock_load_registry: Mock,
        mock_client_class: Mock,
    )) -> None:
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from empty TOML file."""

def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from empty TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting default models file path."""
    def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context test log path."""
    def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting prompts directory."""
    def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test finding LM Studio in shared location."""

def exists_side_effect((self: Path)) -> bool:

def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting default models file path."""

def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context tests directory."""

def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context test log path."""

def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting prompts directory."""

def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test handling permission errors when creating directories."""


</documents>