Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ examples
â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ basic_workflow.sh
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference_examples.sh
â”‚   â”‚   â””â”€â”€ ğŸ“„ model_testing.sh
â”‚   â”œâ”€â”€ ğŸ“ data
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sample_context.txt
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_questions.json
â”‚   â”œâ”€â”€ ğŸ“ prompts
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ analysis.toml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ coding.toml
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ creative.toml
â”‚   â”‚   â””â”€â”€ ğŸ“„ qa.toml
â”‚   â”œâ”€â”€ ğŸ“ python
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ advanced_testing.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ basic_usage.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ batch_processing.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ custom_inference.py
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â””â”€â”€ ğŸ“„ run_all_examples.sh
â”œâ”€â”€ ğŸ“ issues
â”‚   â”œâ”€â”€ ğŸ“„ 101.txt
â”‚   â””â”€â”€ ğŸ“„ 102.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __main__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ py.typed
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_context_tester.py
â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_model_loader.py
â”‚   â”œâ”€â”€ ğŸ“ test_api
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_client.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_exceptions.py
â”‚   â”œâ”€â”€ ğŸ“ test_core
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_tester.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_inference.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_models.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_prompts.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_scanner.py
â”‚   â”œâ”€â”€ ğŸ“ test_e2e
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ test_integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_cli_integration.py
â”‚   â”œâ”€â”€ ğŸ“ test_loaders
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_model_loader.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_prompt_loader.py
â”‚   â”œâ”€â”€ ğŸ“ test_utils
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_paths.py
â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_paths.py
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py
â”‚   â””â”€â”€ ğŸ“„ run_tests.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ build.sh
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ MANIFEST.in
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ pytest.ini
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TESTING.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="2">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="3">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="4">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="5">
<source>MANIFEST.in</source>
<document_content>
include LICENSE
include README.md
include CHANGELOG.md
include pyproject.toml
include src/lmstrix/py.typed
recursive-include tests *.py
recursive-include tests *.toml
recursive-include tests *.json
recursive-exclude * __pycache__
recursive-exclude * *.py[co]
</document_content>
</document>

<document index="6">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Next Steps

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The core functionality is complete. The project has been successfully refactored to use the native `lmstudio` package for all model interactions, and all data is stored in the correct, centralized location. The CLI and Python API are functional.

The immediate next steps are to build a comprehensive test suite to ensure reliability and to prepare the package for its initial public release.

## 2. Phase 2: Testing & Quality Assurance

This phase focuses on ensuring the existing codebase is robust, reliable, and free of bugs.

### 2.1. Unit & Integration Testing
**Goal**: Achieve comprehensive test coverage for all critical components.

- **Testing Framework**: Tests will be implemented using `pytest`.
- **Mocking**: The `pytest-mock` library will be used to create a mock of the `lmstudio` package. This allows for testing the application's logic without needing a live LM Studio instance, ensuring that tests are fast, repeatable, and can run in any environment (like a CI/CD pipeline).

- **Test Implementation Plan**:

  - **`tests/core/test_context_tester.py`**: 
    - Create a `test_binary_search_logic` function that uses a mocked `lmstudio` client.
    - The mock will simulate different scenarios: a model that loads and passes inference at certain context sizes but fails at others.
    - Assert that the `find_max_working_context` method correctly identifies the highest passing context size.
    - Test edge cases: a model that never loads, a model that loads but always fails inference, and a model that works at all tested sizes.

  - **`tests/loaders/test_model_loader.py`**:
    - Mock the `lmstudio.list_downloaded_models` function to return a predefined list of model dictionaries.
    - Test the `scan_and_update_registry` function.
    - Assert that new models are added, existing models are updated (without overwriting test results), and deleted models are removed from the registry.

  - **`tests/utils/test_paths.py`**:
    - Mock the `Path.home()` and `Path.exists()` methods.
    - Test the `get_lmstudio_path` function to ensure it correctly finds the path from the `.lmstudio-home-pointer` file.
    - Test the fallback logic to common directories if the pointer file does not exist.
    - Assert that `get_default_models_file` returns the correct `lmstrix.json` path.

### 2.2. Functional Tests & Usage Examples
**Goal**: Create comprehensive functional tests and practical usage examples that demonstrate all features of both the CLI and Python package.

- **Examples Directory Structure**:
  - `examples/` - Root directory for all examples
    - `cli/` - CLI usage examples
      - `basic_workflow.sh` - Complete workflow: scan, list, test, infer
      - `model_testing.sh` - Focused examples on context testing
      - `inference_examples.sh` - Various inference scenarios
    - `python/` - Python package usage examples
      - `basic_usage.py` - Simple examples using the Python API
      - `advanced_testing.py` - Advanced context testing scenarios
      - `custom_inference.py` - Custom inference workflows
      - `batch_processing.py` - Processing multiple models
    - `prompts/` - Sample prompt files
      - `analysis.toml` - Analysis prompt templates
      - `creative.toml` - Creative writing prompts
      - `coding.toml` - Code generation prompts
      - `qa.toml` - Question-answering prompts
    - `data/` - Sample data files for testing
      - `sample_context.txt` - Large text file for context testing
      - `test_questions.json` - Test questions for QA scenarios

- **Example Content Requirements**:
  - Each example should be self-contained and runnable
  - Include comments explaining what each step does
  - Demonstrate error handling and edge cases
  - Show both successful and failure scenarios
  - Include performance considerations
  - Complement the existing unit tests by showing real-world usage

- **Testing the Examples**:
  - Create `examples/run_all_examples.sh` to execute all examples
  - Verify examples work with mock data when LM Studio is not available
  - Ensure examples demonstrate best practices
  - Include timing and performance metrics where relevant

## 3. Phase 3: Documentation & Release

**Goal**: Prepare the project for a successful v1.0.0 release on PyPI.

### 3.1. Documentation
- **`README.md`**: Update the README to include a clear, concise quick-start guide, installation instructions, and examples for the new CLI commands (`scan`, `test`, `list`).
- **API Documentation**: Add comprehensive docstrings to all public functions and classes, explaining their purpose, arguments, and return values.
- **Examples Documentation**: Add a dedicated `examples/README.md` explaining how to run the examples and what each demonstrates.

### 3.2. Packaging & Release
- **`pyproject.toml`**: Verify that all dependencies, project metadata (version, author, license), and entry points are correct.
- **PyPI Release**: Once testing and documentation are complete, the project will be built and published to PyPI, making it installable via `pip install lmstrix`.

</document_content>
</document>

<document index="7">
<source>README.md</source>
<document_content>
# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

## Installation

```bash
uv pip install --system lmstrix
```

## Quick Start

### Command-Line Interface (CLI)

```bash
# Discover the optimal context window for a model
lmstrix optimize "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# List all available models with their detected context limits
lmstrix models list

# Scan for any new models you've added to LM Studio
lmstrix models scan

# Run inference using a prompt template
lmstrix infer --model "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" --prompt "greeting"
```

### Python API

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    client = LMStrix()

    # Optimize the context window for a specific model
    print("Optimizing context, this may take a moment...")
    optimization = await client.optimize_context("Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf")
    print(f"Optimal context found: {optimization.optimal_size} tokens")
    print("-" * 20)

    # List all models and their properties
    print("Available Models:")
    models = await client.list_models()
    for model in models:
        limit = model.context_limit or "Not yet optimized"
        print(f"- {model.id}: {limit} tokens")
    print("-" * 20)

    # Run inference with a template
    print("Running inference...")
    result = await client.infer(
        model_id="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        prompt_template="Summarize this text: {text}",
        context={"text": "Your long document goes here..."}
    )
    print("Inference Result:")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

## Architecture

LMStrix is designed with a clean, modular architecture:

- **`api/`**: A dedicated client for communicating with the LM Studio local server API.
- **`core/`**: The heart of the application, containing the core business logic for models, inference, and the context optimization algorithm.
- **`loaders/`**: Handles loading and managing data for models, prompts, and context files.
- **`cli/`**: Implements the command-line interface.
- **`utils/`**: Shared utilities and helper functions.

## Development

```bash
# Install in editable mode for development
pip install -e .

# Run the test suite
pytest

# Format and lint the codebase
ruff format .
ruff check .
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are highly welcome! Please feel free to submit pull requests or file issues on our GitHub repository.

## Support

For bugs, feature requests, or general questions, please [file an issue](https://github.com/yourusername/lmstrix/issues) on our GitHub repository.
</document_content>
</document>

<document index="8">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures and configuration
â”œâ”€â”€ run_tests.py             # Simple test runner script
â”œâ”€â”€ test_api/                # API layer tests
â”‚   â”œâ”€â”€ test_client.py       # LMStudioClient tests
â”‚   â””â”€â”€ test_exceptions.py   # Custom exception tests
â”œâ”€â”€ test_core/               # Core module tests
â”‚   â”œâ”€â”€ test_context_tester.py  # Context optimization tests
â”‚   â”œâ”€â”€ test_inference.py    # Inference engine tests
â”‚   â”œâ”€â”€ test_models.py       # Model and registry tests
â”‚   â”œâ”€â”€ test_prompts.py      # Prompt resolution tests
â”‚   â””â”€â”€ test_scanner.py      # Model scanner tests
â”œâ”€â”€ test_loaders/            # Loader tests
â”‚   â”œâ”€â”€ test_context_loader.py   # Context file loading tests
â”‚   â”œâ”€â”€ test_model_loader.py     # Model loader tests
â”‚   â””â”€â”€ test_prompt_loader.py    # Prompt loader tests
â”œâ”€â”€ test_utils/              # Utility tests
â”‚   â””â”€â”€ test_paths.py        # Path utility tests
â”œâ”€â”€ test_integration/        # Integration tests
â”‚   â””â”€â”€ test_cli_integration.py  # CLI integration tests
â””â”€â”€ test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="9">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 2: Testing & Quality Assurance

### Functional Tests & Usage Examples (HIGH PRIORITY)
- [ ] Create `examples` directory structure with subdirectories for CLI, Python, prompts, and data.
- [ ] Write CLI usage examples:
  - [ ] `examples/cli/basic_workflow.sh` - Complete workflow demonstration
  - [ ] `examples/cli/model_testing.sh` - Context testing focused examples
  - [ ] `examples/cli/inference_examples.sh` - Various inference scenarios
- [ ] Write Python package usage examples:
  - [ ] `examples/python/basic_usage.py` - Simple API usage
  - [ ] `examples/python/advanced_testing.py` - Advanced context testing
  - [ ] `examples/python/custom_inference.py` - Custom inference workflows
  - [ ] `examples/python/batch_processing.py` - Processing multiple models
- [ ] Create sample prompt files:
  - [ ] `examples/prompts/analysis.toml` - Analysis templates
  - [ ] `examples/prompts/creative.toml` - Creative writing prompts
  - [ ] `examples/prompts/coding.toml` - Code generation prompts
  - [ ] `examples/prompts/qa.toml` - Question-answering prompts
- [ ] Create sample data files:
  - [ ] `examples/data/sample_context.txt` - Large text for context testing
  - [ ] `examples/data/test_questions.json` - Test questions for QA
- [ ] Create `examples/run_all_examples.sh` to test all examples
- [ ] Write `examples/README.md` documenting all examples

### Unit & Integration Tests
- [ ] Create `tests/core` directory.
- [ ] Create `tests/core/test_context_tester.py`.
- [ ] Implement `test_binary_search_logic` with a mocked `lmstudio` client.
- [ ] Test edge cases (model never loads, inference always fails, etc.).
- [ ] Create `tests/loaders` directory.
- [ ] Create `tests/loaders/test_model_loader.py`.
- [ ] Implement tests for `scan_and_update_registry` (add, update, remove models).
- [ ] Create `tests/utils` directory.
- [ ] Create `tests/utils/test_paths.py`.
- [ ] Implement tests for `get_lmstudio_path` and `get_default_models_file`.

### Code Quality
- [ ] Run `mypy` and fix any reported type errors.
- [ ] Run `ruff` and fix any reported linting issues.

## Phase 3: Documentation

### User Documentation
- [ ] Update `README.md` with the new CLI commands and examples.
- [ ] Add a section to the `README.md` explaining the context testing methodology.

### API Documentation
- [ ] Add comprehensive docstrings to all public functions and classes.

## Phase 4: Package & Release

### Package Preparation
- [ ] Verify all metadata in `pyproject.toml` is accurate.
- [ ] Perform a local test build and installation (`pip install .`).

### Release
- [ ] Tag the release as `v1.0.0` in git.
- [ ] Build the distribution packages (`python -m build`).
- [ ] Publish the package to PyPI.

</document_content>
</document>

<document index="10">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="11">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="12">
<source>examples/README.md</source>
<document_content>
# LMStrix Examples

This directory contains a comprehensive set of examples to demonstrate the usage of both the `lmstrix` command-line interface (CLI) and the Python library. These examples are designed to be run from the root of the project repository.

## How to Run the Examples

A convenience script, `run_all_examples.sh`, is provided to execute all the examples in this directory. To run it, simply execute the following command from the project root:

```bash
bash examples/run_all_examples.sh
```

**Note**: Most of the long-running operations within the example scripts (like model testing and inference) are commented out by default to prevent accidental execution. You can inspect each script and uncomment the relevant lines to run them fully.

## Directory Structure

-   `cli/`: Contains shell scripts that demonstrate the usage of the `lmstrix` CLI.
-   `python/`: Contains Python scripts that show how to use the `lmstrix` library.
-   `prompts/`: Contains sample TOML files with predefined prompts for different tasks.
-   `data/`: Contains sample data files used in the examples, such as a large text file for context testing.

## CLI Examples (`cli/`)

-   `basic_workflow.sh`: A complete, step-by-step workflow showing how to scan for models, list them, test one, and run inference.
-   `model_testing.sh`: Focused examples of the `lmstrix test` command, including how to test a specific model, force a re-test, and customize the test range.
-   `inference_examples.sh`: Demonstrates various ways to run inference, such as using system prompts, reading prompts from files, and streaming responses.

## Python Library Examples (`python/`)

-   `basic_usage.py`: Covers the fundamental operations of the library: scanning, listing, and running a simple inference.
-   `advanced_testing.py`: Shows more advanced context testing scenarios, like running a full scan on all untested models and forcing a re-test.
-   `custom_inference.py`: Illustrates how to customize the inference process with system prompts, streaming, and custom generation parameters.
-   `batch_processing.py`: An example of how to run inference on multiple models in a batch.

## Prompt Files (`prompts/`)

These files contain sample prompts that can be used with the `lmstrix` library. They are organized by task type:

-   `analysis.toml`
-   `creative.toml`
-   `coding.toml`
-   `qa.toml`

## Data Files (`data/`)

-   `sample_context.txt`: A large text file used for context length testing.
-   `test_questions.json`: A set of sample questions for use in question-answering scenarios.

</document_content>
</document>

<document index="13">
<source>examples/cli/basic_workflow.sh</source>
<document_content>
#!/bin/bash
# This script demonstrates a basic, complete workflow for using the LMStrix CLI.
# It covers the main commands in a logical sequence:
# 1. Scan: Discover models downloaded in LM Studio.
# 2. List: View the models found.
# 3. Test: Run a context length test on a specific model.
# 4. Infer: Run a simple inference with the tested model.

# --- Introduction ---
echo "LMStrix Basic Workflow Example"
echo "---------------------------------"
echo "This script requires LM Studio to be running."
echo "Make sure you have at least one model downloaded."

# --- Step 1: Scan for Models ---
echo "\nStep 1: Scanning for models..."
# The `scan` command finds all models in your LM Studio models folder
# and updates the local `lmstrix.json` registry.
lmstrix scan

# --- Step 2: List Models ---
echo "\nStep 2: Listing available models..."
# The `list` command displays all models found in the registry.
# It shows their name, path, and whether they have been tested.
lmstrix list

# --- Step 3: Test a Model's Context Length ---
echo "\nStep 3: Testing a model's context length..."
# The `test` command runs a binary search to find the maximum working context.
# We will try to find the first model from the list to test.
# Note: This can take several minutes depending on the model and your hardware.

# Get the path of the first model from the registry (using jq to parse the JSON)
MODEL_PATH=$(lmstrix list --json | jq -r 'keys[0]')

if [ -z "$MODEL_PATH" ] || [ "$MODEL_PATH" == "null" ]; then
    echo "Error: Could not find a model to test. Please make sure models are downloaded and scanned."
    exit 1
fi

echo "Testing model: $MODEL_PATH"
# The `--model` flag specifies which model to test.
lmstrix test --model "$MODEL_PATH"

# --- Step 4: Run Inference ---
echo "\nStep 4: Running inference with the tested model..."
# The `infer` command runs a prompt through the specified model.
PROMPT="What is the capital of the United Kingdom?"

echo "Using prompt: '$PROMPT'"
lmstrix infer --model "$MODEL_PATH" --prompt "$PROMPT"

# --- Step 5: Review Results ---
echo "\nStep 5: Reviewing updated model list..."
# After testing, the list command will show the updated status and max context length.
lmstrix list

echo "\nWorkflow complete."

</document_content>
</document>

<document index="14">
<source>examples/cli/inference_examples.sh</source>
<document_content>
#!/bin/bash
# This script demonstrates various ways to use the `lmstrix infer` command
# to run inference with models in LM Studio.

# --- Introduction ---
echo "LMStrix Inference Examples"
echo "--------------------------"
echo "This script requires LM Studio to be running and a model to be loaded."

# --- Prerequisite: Scan for models and select one ---
echo "Scanning for models and selecting one for inference..."
lmstrix scan
MODEL_PATH=$(lmstrix list --json | jq -r 'keys[0]')

if [ -z "$MODEL_PATH" ] || [ "$MODEL_PATH" == "null" ]; then
    echo "Error: No models found. Please download a model in LM Studio."
    exit 1
fi

echo "Using model: $MODEL_PATH"

# --- Example 1: Basic Inference with a Prompt ---
echo "\nExample 1: Basic inference with a simple prompt."
# The `--prompt` argument is the most direct way to get a response.
lmstrix infer --model "$MODEL_PATH" --prompt "What are the three primary colors?"

# --- Example 2: Using a System Prompt ---
echo "\nExample 2: Using a system prompt to guide the model's behavior."
# The `--system-prompt` sets the context for the model's persona.
lmstrix infer --model "$MODEL_PATH" \
              --system-prompt "You are a poet. All your answers must be in the form of a haiku." \
              --prompt "Describe a sunrise."

# --- Example 3: Reading a Prompt from a File ---
echo "\nExample 3: Reading a long prompt from a file."
# Create a temporary file with a longer prompt.
PROMPT_FILE="/tmp/prompt.txt"
echo "This is a longer prompt stored in a file. It can contain multiple paragraphs and complex instructions. The `infer` command can read this content directly, which is useful for avoiding complex command-line escaping with long text blocks." > $PROMPT_FILE

echo "Prompt content from file:"
cat $PROMPT_FILE

# The `--prompt-file` argument reads the prompt from the specified file.
lmstrix infer --model "$MODEL_PATH" --prompt-file $PROMPT_FILE

# Clean up the temporary file
rm $PROMPT_FILE

# --- Example 4: Customizing Inference Parameters ---
echo "\nExample 4: Customizing inference parameters for more creative responses."
# You can control the model's output with parameters like temperature and max tokens.
lmstrix infer --model "$MODEL_PATH" \
              --prompt "Tell me a fun fact about space." \
              --temperature 0.8 \
              --max-tokens 50

# --- Example 5: Streaming the Response ---
echo "\nExample 5: Streaming the response in real-time."
# The `--stream` flag will print the response token by token as it is generated.
lmstrix infer --model "$MODEL_PATH" \
              --prompt "Write a short story about a friendly robot." \
              --stream

echo "\n\nInference examples complete."

</document_content>
</document>

<document index="15">
<source>examples/cli/model_testing.sh</source>
<document_content>
#!/bin/bash
# This script demonstrates various ways to use the `lmstrix test` command
# for verifying the context length of models in LM Studio.

# --- Introduction ---
echo "LMStrix Model Testing Examples"
echo "------------------------------"
echo "This script requires LM Studio to be running."

# --- Prerequisite: Scan for models ---
echo "First, ensuring the model registry is up-to-date..."
lmstrix scan

# --- Example 1: Test a Specific Model by Path ---
echo "\nExample 1: Test a specific model by its full path."
# Find the path of the first model in the registry to use as an example.
MODEL_PATH=$(lmstrix list --json | jq -r 'keys[0]')

if [ -z "$MODEL_PATH" ] || [ "$MODEL_PATH" == "null" ]; then
    echo "Error: No models found. Please download a model in LM Studio and run `lmstrix scan`."
    exit 1
fi

echo "Testing model: $MODEL_PATH"
# The `--model` flag is used to specify the model to test.
# lmstrix test --model "$MODEL_PATH"

echo "(Example 1 test is commented out to prevent long run times)."

# --- Example 2: Force a Retest of a Model ---
echo "\nExample 2: Force a retest on a model that may already have results."
# The `--force` flag tells the tester to ignore any previous test results and run again.

echo "Retesting model: $MODEL_PATH"
# lmstrix test --model "$MODEL_PATH" --force

echo "(Example 2 test is commented out)."

# --- Example 3: Run a Full Test on All Untested Models ---
echo "\nExample 3: Test all models that don't have existing results."
# Running `lmstrix test` without a specific model will start a full scan.
# It will iterate through all models in the registry and test each one
# that does not already have a `test_result`.

echo "Starting a full test run on all untested models..."
# lmstrix test

echo "(Example 3 test is commented out)."

# --- Example 4: Customizing the Test with Different Context Sizes ---
echo "\nExample 4: Customize the test range."
# You can specify the minimum and maximum context sizes to test.
# This is useful if you want to narrow the search range.

echo "Testing model with a custom range (1024-4096 tokens): $MODEL_PATH"
# lmstrix test --model "$MODEL_PATH" --min-context 1024 --max-context 4096

echo "(Example 4 test is commented out)."


echo "\nModel testing examples complete."
echo "Uncomment the 'lmstrix test ...' lines to run the commands."

</document_content>
</document>

<document index="16">
<source>examples/data/sample_context.txt</source>
<document_content>
This is a sample text file designed to test the context length of large language models. It contains a significant amount of text to ensure that the model's context window is sufficiently challenged.

The following is an excerpt from "Moby-Dick; or, The Whale" by Herman Melville, chosen for its public domain status and its rich, descriptive language.

Call me Ishmael. Some years agoâ€”never mind how long preciselyâ€”having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking peopleâ€™s hats offâ€”then, I account it high time to get to sea as soon as I can. This is my substitute for pistol and ball. With a philosophical flourish Cato throws himself upon his sword; I quietly take to the ship. There is nothing surprising in this. If they but knew it, almost all men in their degree, some time or other, cherish very nearly the same feelings towards the ocean with me.

There now is your insular city of the Manhattoes, belted round by wharves as Indian isles by coral reefsâ€”commerce surrounds it with her surf. Right and left, the streets take you waterward. Its extreme downtown is the battery, where that noble mole is washed by waves, and cooled by breezes, which a few hours previous were out of sight of land. Look at the crowds of water-gazers there.

Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears Hook to Coenties Slip, and from thence, by Whitehall, northward. What do you see?â€”Posted like silent sentinels all around the town, stand thousands upon thousands of mortal men fixed in ocean reveries. Some leaning against the spiles; some seated upon the pier-heads; some looking over the bulwarks of ships from China; some high aloft in the rigging, as if striving to get a still better seaward peep. But these are all landsmen; of week days pent up in lath and plasterâ€”tied to counters, nailed to benches, clinched to desks. How then is this? Are the green fields gone? What do they here?

But look! here come more crowds, pacing straight for the water, and seemingly bound for a dive. Strange! Nothing will content them but the extremest limit of the land; loitering under the shady lee of yonder warehouses will not suffice. No. They must get just as nigh the water as they possibly can without falling in. And there they standâ€”miles of themâ€”leagues. Inlanders all, they come from lanes and alleys, streets and avenuesâ€”north, east, south, and west. Yet here they all unite. Tell me, does the magnetic virtue of the needles of the compasses of all those ships attract them thither?

But here is an artist. He desires to paint you the dreamiest, shadiest, quietest, most enchanting bit of romantic landscape in all the valley of the Saco. What is the chief element he employs? There stand his trees, each with a hollow trunk, as if a hermit and a crucifix were within; and here sleeps his meadow, and there sleep his cattle; and up from yonder cottage goes a sleepy smoke. Deep into distant woodlands winds a mazy way, reaching to overlapping spurs of mountains bathed in their hill-side blue. But though the picture lies thus tranced, and though this pine-tree shakes down its sighs like leaves upon this shepherdâ€™s head, yet all were vain, unless the shepherdâ€™s eye were fixed upon the magic stream before him. Go visit the Prairies in June, when for scores on scores of miles you wade knee-deep among Tiger-liliesâ€”what is the one charm wanting?â€”Waterâ€”there is not a drop of water there! Were Niagara but a cataract of sand, would you travel your thousand miles to see it? Why did the poor poet of Tennessee, upon suddenly receiving two handfuls of silver, deliberate whether to buy him a coat, which he sadly needed, or invest his money in a pedestrian trip to Rockaway Beach? Why is almost every robust healthy boy with a robust healthy soul in him, at some time or other crazy to go to sea? Why upon your first voyage as a passenger, did you yourself feel such a mystical vibration, when first told that you and your ship were now out of sight of land? Why did the old Persians hold the sea holy? Why did the Greeks give it a separate deity, and own brother of Jove? Surely all this is not without meaning. And still deeper the meaning of that story of Narcissus, who because he could not grasp the tormenting, mild image he saw in the fountain, plunged into it and was drowned. But that same image, we ourselves see in all rivers and oceans. It is the image of the ungraspable phantom of life; and this is the key to it all.

Now, when I say that I am in the habit of going to sea whenever I begin to grow hazy about the eyes, and begin to be over conscious of my lungs, I do not mean to have it inferred that I ever go to sea as a passenger. For to go as a passenger you must needs have a purse, and a purse is but a rag unless you have something in it. Besides, passengers get sea-sickâ€”grow quarrelsomeâ€”donâ€™t sleep of nightsâ€”do not enjoy themselves much, as a general thing;â€”no, I never go as a passenger; nor, though I am something of a salt, do I ever go to sea as a Commodore, or a Captain, or a Cook. I abandon the glory and distinction of such offices to those who like them. For my part, I abominate all honorable respectable toils, trials, and tribulations of every kind whatsoever. It is quite as much as I can do to take care of myself, without taking care of ships, barques, brigs, schooners, and what not. And as for going as a cook,â€”though I confess there is considerable glory in that, a cook being a sort of officer on ship-boardâ€”yet, somehow, I never fancied broiling fowls;â€”though once broiled, judiciously buttered, and judgmatically salted and peppered, there is no one who will speak more respectfully, not to say reverentially, of a broiled fowl than I will. It is out of the idolatrous dotings of the old Egyptians upon broiled ibis and roasted river horse, that you see the mummies of those creatures in their huge bake-houses the pyramids.

No, when I go to sea, I go as a simple sailor, right before the mast, plumb down into the forecastle, aloft there to the royal mast-head. True, they rather order me about some, and make me jump from spar to spar, like a grasshopper in a May meadow. And at first, this sort of thing is unpleasant enough. It touches oneâ€™s sense of honor, particularly if you come of an old established family in the land, the Van Rensselaers, or Randolphs, or Hardicanutes. And more than all, if just previous to putting your hand into the tar-pot, you have been lording it as a country schoolmaster, making the tallest boys stand in awe of you. The transition is a keen one, I assure you, from a schoolmaster to a sailor, and requires a strong decoction of Seneca and the Stoics to enable you to grin and bear it. But even this wears off in time.

What of it, if some old hunks of a sea-captain orders me to get a broom and sweep down the decks? What does that indignity amount to, weighed, I mean, in the scales of the New Testament? Do you think the archangel Gabriel thinks anything the less of me, because I promptly and respectfully obey that old hunks in that particular instance? Who ainâ€™t a slave? Tell me that. Well, then, however the old sea-captains may order me aboutâ€”however they may thump and punch me about, I have the satisfaction of knowing that it is all right; that everybody else is one way or other served in much the same wayâ€”either in a physical or metaphysical point of view, that is; and so the universal thump is passed round, and all hands should rub each otherâ€™s shoulder-blades, and be content.

Again, I always go to sea as a sailor, because they make a point of paying me for my trouble, whereas they never pay passengers a single penny that I ever heard of. On the contrary, passengers themselves must pay. And there is all the difference in the world between paying and being paid. The act of paying is perhaps the most uncomfortable infliction that the two orchard thieves entailed upon us. But being paid,â€”what will compare with it? The urbane activity with which a man receives money is really marvellous, considering that we so earnestly believe money to be the root of all earthly ills, and that on no account can a monied man enter heaven. Ah! how cheerfully we consign ourselves to perdition!

Finally, I always go to sea as a sailor, because of the wholesome exercise and pure air of the forecastle deck. For as in this world, head winds are far more prevalent than winds from astern (that is, if you never violate the Pythagorean maxim), so for the most part the Commodore on the quarter-deck gets his atmosphere at second hand from the sailors on the forecastle. He thinks he breathes it first; but not so. In much the same way do the commonalty lead their leaders in many other things, at the same time that the leaders little suspect it. But wherefore it was that after having repeatedly smelt the sea as a merchant sailor, I should now take it into my head to go on a whaling voyage; this the invisible police officer of the Fates, who has the constant surveillance of me, and secretly dogs me, and influences me in some unaccountable wayâ€”he can answer. And, doubtless, my going on this whaling voyage, formed part of the grand programme of Providence that was drawn up a long time ago. It came in as a sort of brief interlude and solo between more extensive performances. I take it that this part of the bill must have run something like this:

â€œGrand Contested Election for the Presidency of the United States.

â€œWHALING VOYAGE BY ONE ISHMAEL.

â€œBLOODY BATTLE IN AFFGHANISTAN.â€

Though I cannot tell why it was exactly that those stage managers, the Fates, put me down for this shabby part of a whaling voyage, when others were set down for magnificent parts in high tragedies, and short and easy parts in genteel comedies, and jolly parts in farcesâ€”though I cannot tell why this was exactly; yet, now that I recall all the circumstances, I think I can see a little into the springs and motives which being cunningly presented to me under various disguises, induced me to set about performing the part I did, besides cajoling me into the delusion that it was a choice resulting from my own unbiased freewill and discriminating judgment.

Chief among these motives was the overwhelming idea of the great whale himself. Such a portentous and mysterious monster roused all my curiosity. Then the wild and distant seas where he rolled his island bulk; the undeliverable, nameless perils of the whale; these, with all the attending marvels of a thousand Patagonian sights and sounds, helped to sway me to my purpose. With other men, perhaps, such things would not have been inducements; but as for me, I am tormented with an everlasting itch for things remote. I love to sail forbidden seas, and land on barbarous coasts. Not ignoring what is good, I am quick to perceive a horror, and could still be social with itâ€”would they let meâ€”since it is but well to be on friendly terms with all the inmates of the place one lodges in.

By reason of these things, then, the whaling voyage was welcome; the great flood-gates of the wonder-world swung open, and in the wild conceits that swayed me to my purpose, two and two there floated into my inmost soul, endless processions of the whale, and, mid most of them all, one grand hooded phantom, like a snow hill in the air.
</document_content>
</document>

<document index="17">
<source>examples/data/test_questions.json</source>
<document_content>
{
    "questions": [
        {
            "id": "qa001",
            "question": "What is the primary function of the mitochondria in a cell?",
... (file content truncated to first 5 lines)
</document_content>
</document>

<document index="18">
<source>examples/prompts/analysis.toml</source>
<document_content>
# Prompt templates for analysis tasks

[sentiment_analysis]
name = "Sentiment Analysis"
description = "Analyze the sentiment of a given text."
prompt = """
Analyze the sentiment of the following text and classify it as positive, negative, or neutral. Provide a brief explanation for your classification.

Text: {text}

Sentiment:
"""

[summarization]
name = "Text Summarization"
description = "Summarize a long piece of text."
prompt = """
Summarize the following text into a few key points.

Text: {text}

Summary:
"""

[keyword_extraction]
name = "Keyword Extraction"
description = "Extract the main keywords from a text."
prompt = """
Extract the most important keywords from the following text.

Text: {text}

Keywords:
"""

</document_content>
</document>

<document index="19">
<source>examples/prompts/coding.toml</source>
<document_content>
# Prompt templates for code generation tasks

[python_function]
name = "Python Function Generator"
description = "Generate a Python function based on a description."
prompt = """
Write a Python function that does the following: {description}
"""

[sql_query]
name = "SQL Query Generator"
description = "Generate a SQL query from a natural language description."
prompt = """
Generate a SQL query that {description}.
"""

[regex_generator]
name = "Regex Generator"
description = "Generate a regular expression for a given pattern."
prompt = """
Create a regular expression that matches the following pattern: {pattern_description}
"""

</document_content>
</document>

<document index="20">
<source>examples/prompts/creative.toml</source>
<document_content>
# Prompt templates for creative writing tasks

[story_starter]
name = "Story Starter"
description = "Generate a starting paragraph for a story based on a genre and a character."
prompt = """
Write the opening paragraph of a {genre} story featuring a character named {character}.
"""

[poem_generator]
name = "Poem Generator"
description = "Generate a short poem about a given topic."
prompt = """
Write a four-line poem about {topic}.
"""

[dialogue_writer]
name = "Dialogue Writer"
description = "Write a short piece of dialogue between two characters."
prompt = """
Write a brief dialogue between a {character1} and a {character2} who are discussing {topic}.
"""

</document_content>
</document>

<document index="21">
<source>examples/prompts/qa.toml</source>
<document_content>
# Prompt templates for question-answering tasks

[open_domain_qa]
name = "Open Domain Question Answering"
description = "Answer a question based on general knowledge."
prompt = """
Question: {question}
Answer:
"""

[closed_domain_qa]
name = "Closed Domain Question Answering"
description = "Answer a question based on a provided context."
prompt = """
Read the following context and answer the question.

Context: {context}

Question: {question}

Answer:
"""

</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py
# Language: python

import asyncio
from lmstrix.core.models import ModelRegistry
from lmstrix.core.context_tester import ContextTester
from lmstrix.loaders.model_loader import scan_and_update_registry

def advanced_testing_example(()):
    """ A function to demonstrate advanced context testing scenarios...."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py
# Language: python

import asyncio
from lmstrix.core.models import ModelRegistry
from lmstrix.core.inference import InferenceManager
from lmstrix.loaders.model_loader import scan_and_update_registry

def basic_usage_example(()):
    """ A function to demonstrate the basic usage of the LMStrix library...."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py
# Language: python

import asyncio
from lmstrix.core.models import LmsModel, ModelRegistry
from lmstrix.core.inference import InferenceManager
from lmstrix.loaders.model_loader import scan_and_update_registry
from lmstrix.utils.paths import get_default_models_file

def batch_inference_example(()):
    """ An example function that demonstrates batch processing of models...."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py
# Language: python



<document index="22">
<source>examples/run_all_examples.sh</source>
<document_content>
#!/bin/bash
# This script executes all the example scripts in the `cli` and `python` directories.
# It is designed to be run from the root of the project.

set -e # Exit immediately if a command exits with a non-zero status.

echo "Running all examples..."

# --- Run CLI Examples ---
echo "\n--- Running CLI Examples ---"

# Note: The CLI examples themselves have their main commands commented out
# to avoid long run times. This script will execute them, but they will
# mostly print information rather than perform the actual tests or inferences.

chmod +x examples/cli/*.sh

for script in examples/cli/*.sh; do
    echo "\nExecuting $script..."
    bash "$script"
    echo "Finished $script."
done

# --- Run Python Examples ---
echo "\n--- Running Python Examples ---"

# Similarly, the Python examples have their core logic commented out.
# This will run the scripts, but they will not perform the long-running tasks.

for script in examples/python/*.py; do
    echo "\nExecuting $script..."
    # Use python -m to run the module from the project root
    module_name=$(basename -s .py "$script")
    python -m "examples.python.$module_name"
    echo "Finished $script."
done

echo "\nAll examples have been executed."

</document_content>
</document>

<document index="23">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="24">
<source>issues/102.txt</source>
<document_content>
```
$ python -m lmstrix | cat
NAME
__main__.py - A CLI for testing and managing LM Studio models.

SYNOPSIS
__main__.py COMMAND

DESCRIPTION
A CLI for testing and managing LM Studio models.

COMMANDS
COMMAND is one of the following:

infer
Run inference on a specified model.

list
List all models from the registry with their test status.

scan
Scan for LM Studio models and update the local registry.

test
Test the context limits for models.
```

<objective>
Now into @examples write functional tests / usage examples for the various functionalities (of both CLI and the Python package), and run them.
Make sure @examples complement @tests well. Include sensible prompt files and other sensible files in @examples
</objective>

<task>
1. Into @PLAN.md describe in detail the objective
2. Into @TODO.md add the objective as high priority, making sure that it fits with the other tasks
3. Start implementing tasks!
</task>

</document_content>
</document>

<document index="25">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ examples
â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ basic_workflow.sh
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference_examples.sh
â”‚   â”‚   â””â”€â”€ ğŸ“„ model_testing.sh
â”‚   â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ ğŸ“ prompts
â”‚   â””â”€â”€ ğŸ“ python
â”‚       â”œâ”€â”€ ğŸ“„ advanced_testing.py
â”‚       â”œâ”€â”€ ğŸ“„ basic_usage.py
â”‚       â””â”€â”€ ğŸ“„ custom_inference.py
â”œâ”€â”€ ğŸ“ issues
â”‚   â”œâ”€â”€ ğŸ“„ 101.txt
â”‚   â””â”€â”€ ğŸ“„ 102.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __main__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ py.typed
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“ test_api
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_client.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_exceptions.py
â”‚   â”œâ”€â”€ ğŸ“ test_core
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_tester.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_inference.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_models.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_prompts.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_scanner.py
â”‚   â”œâ”€â”€ ğŸ“ test_e2e
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ test_integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_cli_integration.py
â”‚   â”œâ”€â”€ ğŸ“ test_loaders
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_model_loader.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_prompt_loader.py
â”‚   â”œâ”€â”€ ğŸ“ test_utils
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_paths.py
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py
â”‚   â””â”€â”€ ğŸ“„ run_tests.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ build.sh
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ MANIFEST.in
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ pytest.ini
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TESTING.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="2">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="3">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="4">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="5">
<source>MANIFEST.in</source>
<document_content>
include LICENSE
include README.md
include CHANGELOG.md
include pyproject.toml
include src/lmstrix/py.typed
recursive-include tests *.py
recursive-include tests *.toml
recursive-include tests *.json
recursive-exclude * __pycache__
recursive-exclude * *.py[co]
</document_content>
</document>

<document index="6">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Next Steps

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The core functionality is complete. The project has been successfully refactored to use the native `lmstudio` package for all model interactions, and all data is stored in the correct, centralized location. The CLI and Python API are functional.

The immediate next steps are to build a comprehensive test suite to ensure reliability and to prepare the package for its initial public release.

## 2. Phase 2: Testing & Quality Assurance

This phase focuses on ensuring the existing codebase is robust, reliable, and free of bugs.

### 2.1. Unit & Integration Testing
**Goal**: Achieve comprehensive test coverage for all critical components.

- **Testing Framework**: Tests will be implemented using `pytest`.
- **Mocking**: The `pytest-mock` library will be used to create a mock of the `lmstudio` package. This allows for testing the application's logic without needing a live LM Studio instance, ensuring that tests are fast, repeatable, and can run in any environment (like a CI/CD pipeline).

- **Test Implementation Plan**:

  - **`tests/core/test_context_tester.py`**: 
    - Create a `test_binary_search_logic` function that uses a mocked `lmstudio` client.
    - The mock will simulate different scenarios: a model that loads and passes inference at certain context sizes but fails at others.
    - Assert that the `find_max_working_context` method correctly identifies the highest passing context size.
    - Test edge cases: a model that never loads, a model that loads but always fails inference, and a model that works at all tested sizes.

  - **`tests/loaders/test_model_loader.py`**:
    - Mock the `lmstudio.list_downloaded_models` function to return a predefined list of model dictionaries.
    - Test the `scan_and_update_registry` function.
    - Assert that new models are added, existing models are updated (without overwriting test results), and deleted models are removed from the registry.

  - **`tests/utils/test_paths.py`**:
    - Mock the `Path.home()` and `Path.exists()` methods.
    - Test the `get_lmstudio_path` function to ensure it correctly finds the path from the `.lmstudio-home-pointer` file.
    - Test the fallback logic to common directories if the pointer file does not exist.
    - Assert that `get_default_models_file` returns the correct `lmstrix.json` path.

### 2.2. Functional Tests & Usage Examples
**Goal**: Create comprehensive functional tests and practical usage examples that demonstrate all features of both the CLI and Python package.

- **Examples Directory Structure**:
  - `examples/` - Root directory for all examples
    - `cli/` - CLI usage examples
      - `basic_workflow.sh` - Complete workflow: scan, list, test, infer
      - `model_testing.sh` - Focused examples on context testing
      - `inference_examples.sh` - Various inference scenarios
    - `python/` - Python package usage examples
      - `basic_usage.py` - Simple examples using the Python API
      - `advanced_testing.py` - Advanced context testing scenarios
      - `custom_inference.py` - Custom inference workflows
      - `batch_processing.py` - Processing multiple models
    - `prompts/` - Sample prompt files
      - `analysis.toml` - Analysis prompt templates
      - `creative.toml` - Creative writing prompts
      - `coding.toml` - Code generation prompts
      - `qa.toml` - Question-answering prompts
    - `data/` - Sample data files for testing
      - `sample_context.txt` - Large text file for context testing
      - `test_questions.json` - Test questions for QA scenarios

- **Example Content Requirements**:
  - Each example should be self-contained and runnable
  - Include comments explaining what each step does
  - Demonstrate error handling and edge cases
  - Show both successful and failure scenarios
  - Include performance considerations
  - Complement the existing unit tests by showing real-world usage

- **Testing the Examples**:
  - Create `examples/run_all_examples.sh` to execute all examples
  - Verify examples work with mock data when LM Studio is not available
  - Ensure examples demonstrate best practices
  - Include timing and performance metrics where relevant

## 3. Phase 3: Documentation & Release

**Goal**: Prepare the project for a successful v1.0.0 release on PyPI.

### 3.1. Documentation
- **`README.md`**: Update the README to include a clear, concise quick-start guide, installation instructions, and examples for the new CLI commands (`scan`, `test`, `list`).
- **API Documentation**: Add comprehensive docstrings to all public functions and classes, explaining their purpose, arguments, and return values.
- **Examples Documentation**: Add a dedicated `examples/README.md` explaining how to run the examples and what each demonstrates.

### 3.2. Packaging & Release
- **`pyproject.toml`**: Verify that all dependencies, project metadata (version, author, license), and entry points are correct.
- **PyPI Release**: Once testing and documentation are complete, the project will be built and published to PyPI, making it installable via `pip install lmstrix`.

</document_content>
</document>

<document index="7">
<source>README.md</source>
<document_content>
# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

## Installation

```bash
uv pip install --system lmstrix
```

## Quick Start

### Command-Line Interface (CLI)

```bash
# Discover the optimal context window for a model
lmstrix optimize "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# List all available models with their detected context limits
lmstrix models list

# Scan for any new models you've added to LM Studio
lmstrix models scan

# Run inference using a prompt template
lmstrix infer --model "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" --prompt "greeting"
```

### Python API

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    client = LMStrix()

    # Optimize the context window for a specific model
    print("Optimizing context, this may take a moment...")
    optimization = await client.optimize_context("Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf")
    print(f"Optimal context found: {optimization.optimal_size} tokens")
    print("-" * 20)

    # List all models and their properties
    print("Available Models:")
    models = await client.list_models()
    for model in models:
        limit = model.context_limit or "Not yet optimized"
        print(f"- {model.id}: {limit} tokens")
    print("-" * 20)

    # Run inference with a template
    print("Running inference...")
    result = await client.infer(
        model_id="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        prompt_template="Summarize this text: {text}",
        context={"text": "Your long document goes here..."}
    )
    print("Inference Result:")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

## Architecture

LMStrix is designed with a clean, modular architecture:

- **`api/`**: A dedicated client for communicating with the LM Studio local server API.
- **`core/`**: The heart of the application, containing the core business logic for models, inference, and the context optimization algorithm.
- **`loaders/`**: Handles loading and managing data for models, prompts, and context files.
- **`cli/`**: Implements the command-line interface.
- **`utils/`**: Shared utilities and helper functions.

## Development

```bash
# Install in editable mode for development
pip install -e .

# Run the test suite
pytest

# Format and lint the codebase
ruff format .
ruff check .
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are highly welcome! Please feel free to submit pull requests or file issues on our GitHub repository.

## Support

For bugs, feature requests, or general questions, please [file an issue](https://github.com/yourusername/lmstrix/issues) on our GitHub repository.
</document_content>
</document>

<document index="8">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures and configuration
â”œâ”€â”€ run_tests.py             # Simple test runner script
â”œâ”€â”€ test_api/                # API layer tests
â”‚   â”œâ”€â”€ test_client.py       # LMStudioClient tests
â”‚   â””â”€â”€ test_exceptions.py   # Custom exception tests
â”œâ”€â”€ test_core/               # Core module tests
â”‚   â”œâ”€â”€ test_context_tester.py  # Context optimization tests
â”‚   â”œâ”€â”€ test_inference.py    # Inference engine tests
â”‚   â”œâ”€â”€ test_models.py       # Model and registry tests
â”‚   â”œâ”€â”€ test_prompts.py      # Prompt resolution tests
â”‚   â””â”€â”€ test_scanner.py      # Model scanner tests
â”œâ”€â”€ test_loaders/            # Loader tests
â”‚   â”œâ”€â”€ test_context_loader.py   # Context file loading tests
â”‚   â”œâ”€â”€ test_model_loader.py     # Model loader tests
â”‚   â””â”€â”€ test_prompt_loader.py    # Prompt loader tests
â”œâ”€â”€ test_utils/              # Utility tests
â”‚   â””â”€â”€ test_paths.py        # Path utility tests
â”œâ”€â”€ test_integration/        # Integration tests
â”‚   â””â”€â”€ test_cli_integration.py  # CLI integration tests
â””â”€â”€ test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="9">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 2: Testing & Quality Assurance

### Functional Tests & Usage Examples (HIGH PRIORITY)
- [ ] Create `examples` directory structure with subdirectories for CLI, Python, prompts, and data.
- [ ] Write CLI usage examples:
  - [ ] `examples/cli/basic_workflow.sh` - Complete workflow demonstration
  - [ ] `examples/cli/model_testing.sh` - Context testing focused examples
  - [ ] `examples/cli/inference_examples.sh` - Various inference scenarios
- [ ] Write Python package usage examples:
  - [ ] `examples/python/basic_usage.py` - Simple API usage
  - [ ] `examples/python/advanced_testing.py` - Advanced context testing
  - [ ] `examples/python/custom_inference.py` - Custom inference workflows
  - [ ] `examples/python/batch_processing.py` - Processing multiple models
- [ ] Create sample prompt files:
  - [ ] `examples/prompts/analysis.toml` - Analysis templates
  - [ ] `examples/prompts/creative.toml` - Creative writing prompts
  - [ ] `examples/prompts/coding.toml` - Code generation prompts
  - [ ] `examples/prompts/qa.toml` - Question-answering prompts
- [ ] Create sample data files:
  - [ ] `examples/data/sample_context.txt` - Large text for context testing
  - [ ] `examples/data/test_questions.json` - Test questions for QA
- [ ] Create `examples/run_all_examples.sh` to test all examples
- [ ] Write `examples/README.md` documenting all examples

### Unit & Integration Tests
- [ ] Create `tests/core` directory.
- [ ] Create `tests/core/test_context_tester.py`.
- [ ] Implement `test_binary_search_logic` with a mocked `lmstudio` client.
- [ ] Test edge cases (model never loads, inference always fails, etc.).
- [ ] Create `tests/loaders` directory.
- [ ] Create `tests/loaders/test_model_loader.py`.
- [ ] Implement tests for `scan_and_update_registry` (add, update, remove models).
- [ ] Create `tests/utils` directory.
- [ ] Create `tests/utils/test_paths.py`.
- [ ] Implement tests for `get_lmstudio_path` and `get_default_models_file`.

### Code Quality
- [ ] Run `mypy` and fix any reported type errors.
- [ ] Run `ruff` and fix any reported linting issues.

## Phase 3: Documentation

### User Documentation
- [ ] Update `README.md` with the new CLI commands and examples.
- [ ] Add a section to the `README.md` explaining the context testing methodology.

### API Documentation
- [ ] Add comprehensive docstrings to all public functions and classes.

## Phase 4: Package & Release

### Package Preparation
- [ ] Verify all metadata in `pyproject.toml` is accurate.
- [ ] Perform a local test build and installation (`pip install .`).

### Release
- [ ] Tag the release as `v1.0.0` in git.
- [ ] Build the distribution packages (`python -m build`).
- [ ] Publish the package to PyPI.

</document_content>
</document>

<document index="10">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="11">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="12">
<source>examples/cli/basic_workflow.sh</source>
<document_content>
#!/bin/bash
# basic_workflow.sh - Complete workflow demonstration for lmstrix CLI
# This example shows the typical workflow: scan models, list them, test context limits, and run inference

echo "=== LMStrix Basic Workflow Example ==="
echo "This example demonstrates the complete workflow for using lmstrix CLI"
echo

# Step 1: Scan for available LM Studio models
echo "Step 1: Scanning for LM Studio models..."
echo "Command: python -m lmstrix scan"
python -m lmstrix scan
echo

# Step 2: List all discovered models
echo "Step 2: Listing all discovered models..."
echo "Command: python -m lmstrix list"
python -m lmstrix list
echo

# Step 3: Test context limits for a specific model
# Note: Replace 'model-id' with an actual model ID from the list
echo "Step 3: Testing context limits for a model..."
echo "Command: python -m lmstrix test --model_id='your-model-id'"
echo "(In a real scenario, replace 'your-model-id' with an actual model ID)"
# python -m lmstrix test --model_id="llama-3.2-1b-instruct"
echo

# Step 4: Test all untested models
echo "Step 4: Testing all untested models..."
echo "Command: python -m lmstrix test --all"
# python -m lmstrix test --all
echo

# Step 5: Run inference with a simple prompt
echo "Step 5: Running inference with a model..."
echo "Command: python -m lmstrix infer 'Hello, how are you?' --model_id='your-model-id'"
echo "(In a real scenario, replace 'your-model-id' with an actual model ID)"
# python -m lmstrix infer "Hello, how are you?" --model_id="llama-3.2-1b-instruct"
echo

# Step 6: Run inference with custom parameters
echo "Step 6: Running inference with custom parameters..."
echo "Command: python -m lmstrix infer 'Explain quantum computing' --model_id='your-model-id' --temperature=0.3 --max_tokens=150"
# python -m lmstrix infer "Explain quantum computing in simple terms" --model_id="llama-3.2-1b-instruct" --temperature=0.3 --max_tokens=150
echo

echo "=== Workflow Complete ==="
echo "This example showed how to:"
echo "1. Discover models with 'scan'"
echo "2. View models with 'list'"
echo "3. Test context limits with 'test'"
echo "4. Run inference with 'infer'"
echo
echo "For more detailed examples, see:"
echo "- model_testing.sh for context testing examples"
echo "- inference_examples.sh for various inference scenarios"
</document_content>
</document>

<document index="13">
<source>examples/cli/inference_examples.sh</source>
<document_content>
#!/bin/bash
# inference_examples.sh - Various inference scenarios for lmstrix CLI
# This example demonstrates different ways to run inference with models

echo "=== LMStrix Inference Examples ==="
echo "This example shows various inference scenarios"
echo

# Example 1: Basic inference
echo "Example 1: Basic inference with default parameters"
echo "Command: python -m lmstrix infer 'What is the capital of France?' --model_id='your-model-id'"
# python -m lmstrix infer "What is the capital of France?" --model_id="llama-3.2-1b-instruct"
echo

# Example 2: Inference with temperature control
echo "Example 2: Creative writing with high temperature"
echo "Command: python -m lmstrix infer 'Write a short story about a robot' --model_id='your-model-id' --temperature=0.9"
# python -m lmstrix infer "Write a short story about a robot learning to paint" --model_id="llama-3.2-1b-instruct" --temperature=0.9
echo

echo "Example 3: Factual responses with low temperature"
echo "Command: python -m lmstrix infer 'List the planets in our solar system' --model_id='your-model-id' --temperature=0.1"
# python -m lmstrix infer "List the planets in our solar system in order from the sun" --model_id="llama-3.2-1b-instruct" --temperature=0.1
echo

# Example 3: Controlling output length
echo "Example 4: Controlling output length with max_tokens"
echo "Command: python -m lmstrix infer 'Explain machine learning' --model_id='your-model-id' --max_tokens=50"
# python -m lmstrix infer "Explain machine learning" --model_id="llama-3.2-1b-instruct" --max_tokens=50
echo

echo "Example 5: Unlimited output length"
echo "Command: python -m lmstrix infer 'Write a detailed guide' --model_id='your-model-id' --max_tokens=-1"
# python -m lmstrix infer "Write a detailed guide on making coffee" --model_id="llama-3.2-1b-instruct" --max_tokens=-1
echo

# Example 4: Code generation
echo "Example 6: Code generation example"
echo "Command: python -m lmstrix infer 'Write a Python function to calculate factorial' --model_id='your-model-id' --temperature=0.2"
# python -m lmstrix infer "Write a Python function to calculate the factorial of a number" --model_id="llama-3.2-1b-instruct" --temperature=0.2
echo

# Example 5: Multi-line prompts
echo "Example 7: Using multi-line prompts"
echo "Command: python -m lmstrix infer \$'Line 1\\nLine 2\\nLine 3' --model_id='your-model-id'"
# python -m lmstrix infer $'Context: You are a helpful assistant.\nTask: Summarize the benefits of exercise.\nFormat: Bullet points' --model_id="llama-3.2-1b-instruct"
echo

# Example 6: Verbose mode for debugging
echo "Example 8: Running inference in verbose mode"
echo "Command: python -m lmstrix infer 'Hello' --model_id='your-model-id' --verbose"
echo "Verbose mode shows:"
echo "- Model loading progress"
echo "- Token usage"
echo "- Inference time"
echo "- Any errors or warnings"
# python -m lmstrix infer "Hello" --model_id="llama-3.2-1b-instruct" --verbose
echo

# Example 7: Testing context limits with inference
echo "Example 9: Testing large context with inference"
echo "First, check the model's tested context limit:"
echo "Command: python -m lmstrix list"
echo
echo "Then create a prompt near that limit:"
echo "Command: python -m lmstrix infer 'Please analyze this text: [very long text]' --model_id='your-model-id'"
echo

# Example 8: Comparing models
echo "Example 10: Comparing responses from different models"
echo "Run the same prompt with different models to compare:"
echo "python -m lmstrix infer 'Explain quantum computing' --model_id='model-1'"
echo "python -m lmstrix infer 'Explain quantum computing' --model_id='model-2'"
echo

echo "=== Inference Tips ==="
echo "- Use low temperature (0.1-0.3) for factual, consistent responses"
echo "- Use high temperature (0.7-0.9) for creative, varied responses"
echo "- Set max_tokens=-1 for unlimited length (until model stops)"
echo "- Check tested context limits before using large prompts"
echo "- Use verbose mode to debug issues or see performance metrics"
</document_content>
</document>

<document index="14">
<source>examples/cli/model_testing.sh</source>
<document_content>
#!/bin/bash
# model_testing.sh - Context testing focused examples for lmstrix CLI
# This example demonstrates various ways to test model context limits

echo "=== LMStrix Model Context Testing Examples ==="
echo "This example shows different ways to test model context limits"
echo

# Example 1: Test a specific model by ID
echo "Example 1: Testing a specific model"
echo "Command: python -m lmstrix test --model_id='llama-3.2-1b-instruct'"
echo "This tests the context limits of a single model"
# python -m lmstrix test --model_id="llama-3.2-1b-instruct"
echo

# Example 2: Test all untested models
echo "Example 2: Testing all untested models"
echo "Command: python -m lmstrix test --all"
echo "This tests all models that haven't been tested yet"
# python -m lmstrix test --all
echo

# Example 3: Test with verbose output
echo "Example 3: Testing with verbose output for debugging"
echo "Command: python -m lmstrix test --model_id='your-model-id' --verbose"
echo "Verbose mode shows detailed progress and debug information"
# python -m lmstrix test --model_id="llama-3.2-1b-instruct" --verbose
echo

# Example 4: Re-test a previously failed model
echo "Example 4: Re-testing a model that previously failed"
echo "First, list models to see which ones failed:"
echo "Command: python -m lmstrix list"
# python -m lmstrix list
echo
echo "Then re-test the failed model:"
echo "Command: python -m lmstrix test --model_id='failed-model-id'"
# python -m lmstrix test --model_id="failed-model-id"
echo

# Example 5: Understanding test results
echo "Example 5: Understanding test results"
echo "After testing, use 'list' to see the results:"
echo "Command: python -m lmstrix list"
echo
echo "The output shows:"
echo "- Declared Ctx: What the model claims to support"
echo "- Tested Ctx: The actual working context limit found"
echo "- Status: untested, testing, completed, or failed"
echo

# Example 6: Testing workflow for new models
echo "Example 6: Complete testing workflow for new models"
echo "1. Scan for new models: python -m lmstrix scan"
echo "2. List to see new models: python -m lmstrix list"
echo "3. Test new models: python -m lmstrix test --all"
echo "4. Verify results: python -m lmstrix list"
echo

echo "=== Context Testing Tips ==="
echo "- Testing can take several minutes per model"
echo "- The tool uses binary search to efficiently find the maximum context"
echo "- Results are saved automatically to the registry"
echo "- Failed tests can be retried - the tool will start fresh"
echo "- Use --verbose flag to see detailed progress during testing"
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py
# Language: python

import asyncio
import time
from typing import List, Dict, Any
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.models import Model, TestStatus
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry

def test_single_model_with_monitoring((model: Model)) -> Model:
    """Test a single model with progress monitoring."""

def batch_test_models((models: List[Model], concurrent: bool = False)) -> Dict[str, Model]:
    """Test multiple models either sequentially or concurrently."""

def test_with_custom_parameters(()):
    """Example of testing with custom parameters."""

def analyze_test_results(()):
    """Analyze and report on all test results."""

def retry_failed_tests(()):
    """Retry testing for models that previously failed."""

def smart_testing_strategy(()):
    """Implement a smart testing strategy based on model characteristics."""

def main(()):
    """Run all advanced testing examples."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py
# Language: python

import asyncio
from pathlib import Path
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import Model, ModelRegistry

def example_load_and_list_models(()):
    """Example 1: Load model registry and list all models."""

def example_scan_for_models(()):
    """Example 2: Scan for new models and update registry."""

def example_simple_inference(()):
    """Example 3: Run simple inference with a model."""

def example_custom_inference(()):
    """Example 4: Inference with custom parameters."""

def example_filter_models(()):
    """Example 5: Filter models by various criteria."""

def example_model_details(()):
    """Example 6: Get detailed information about a specific model."""

def main(()):
    """Run all examples."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py
# Language: python

import asyncio
import time
from typing import List, Dict, Optional
import json
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.core.models import Model

class CustomInferenceWorkflow:
    """Custom inference workflow with advanced features."""
    def __init__((self, verbose: bool = False)):
    def inference_with_template((
        self, 
        model_id: str, 
        template: str, 
        variables: Dict[str, str],
        **kwargs
    )) -> InferenceResult:
        """Run inference with a prompt template."""
    def context_aware_inference((
        self,
        model_id: str,
        prompt: str,
        context_percentage: float = 0.8,
        **kwargs
    )) -> InferenceResult:
        """Run inference with context limit awareness."""
    def inference_with_retry((
        self,
        model_id: str,
        prompt: str,
        max_retries: int = 3,
        **kwargs
    )) -> Optional[InferenceResult]:
        """Run inference with automatic retry on failure."""
    def compare_model_responses((
        self,
        prompt: str,
        model_ids: List[str],
        **kwargs
    )) -> Dict[str, InferenceResult]:
        """Compare responses from multiple models."""
    def benchmark_inference((
        self,
        model_id: str,
        prompts: List[str],
        **kwargs
    )) -> Dict[str, any]:
        """Benchmark inference performance across multiple prompts."""

def __init__((self, verbose: bool = False)):

def inference_with_template((
        self, 
        model_id: str, 
        template: str, 
        variables: Dict[str, str],
        **kwargs
    )) -> InferenceResult:
    """Run inference with a prompt template."""

def context_aware_inference((
        self,
        model_id: str,
        prompt: str,
        context_percentage: float = 0.8,
        **kwargs
    )) -> InferenceResult:
    """Run inference with context limit awareness."""

def inference_with_retry((
        self,
        model_id: str,
        prompt: str,
        max_retries: int = 3,
        **kwargs
    )) -> Optional[InferenceResult]:
    """Run inference with automatic retry on failure."""

def compare_model_responses((
        self,
        prompt: str,
        model_ids: List[str],
        **kwargs
    )) -> Dict[str, InferenceResult]:
    """Compare responses from multiple models."""

def benchmark_inference((
        self,
        model_id: str,
        prompts: List[str],
        **kwargs
    )) -> Dict[str, any]:
    """Benchmark inference performance across multiple prompts."""

def example_template_inference(()):
    """Example: Using prompt templates."""

def example_context_aware(()):
    """Example: Context-aware inference."""

def example_retry_logic(()):
    """Example: Inference with retry logic."""

def example_model_comparison(()):
    """Example: Compare multiple models."""

def example_performance_benchmark(()):
    """Example: Benchmark model performance."""

def main(()):
    """Run all custom inference examples."""


<document index="15">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="16">
<source>issues/102.txt</source>
<document_content>
```
$ python -m lmstrix | cat
NAME
__main__.py - A CLI for testing and managing LM Studio models.

SYNOPSIS
__main__.py COMMAND

DESCRIPTION
A CLI for testing and managing LM Studio models.

COMMANDS
COMMAND is one of the following:

infer
Run inference on a specified model.

list
List all models from the registry with their test status.

scan
Scan for LM Studio models and update the local registry.

test
Test the context limits for models.
```

<objective>
Now into @examples write functional tests / usage examples for the various functionalities (of both CLI and the Python package), and run them.
Make sure @examples complement @tests well. Include sensible prompt files and other sensible files in @examples
</objective>

<task>
1. Into @PLAN.md describe in detail the objective
2. Into @TODO.md add the objective as high priority, making sure that it fits with the other tasks
3. Start implementing tasks!
</task>

</document_content>
</document>

<document index="17">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 101.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __main__.py
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“ test_api
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_client.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_exceptions.py
â”‚   â”œâ”€â”€ ğŸ“ test_core
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_tester.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_inference.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_models.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_prompts.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_scanner.py
â”‚   â”œâ”€â”€ ğŸ“ test_e2e
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ test_integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ 
â”‚   â”‚       test_cli_integration.py
â”‚   â”œâ”€â”€ ğŸ“ test_loaders
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_model_loader.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_prompt_loader.py
â”‚   â”œâ”€â”€ ğŸ“ test_utils
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_paths.py
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py
â”‚   â””â”€â”€ ğŸ“„ run_tests.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ build.sh
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ pytest.ini
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TESTING.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="2">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="3">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="4">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="5">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Next Steps

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The core functionality is complete. The project has been successfully refactored to use the native `lmstudio` package for all model interactions, and all data is stored in the correct, centralized location. The CLI and Python API are functional.

The immediate next steps are to build a comprehensive test suite to ensure reliability and to prepare the package for its initial public release.

## 2. Phase 2: Testing & Quality Assurance

This phase focuses on ensuring the existing codebase is robust, reliable, and free of bugs.

### 2.1. Unit & Integration Testing
**Goal**: Achieve comprehensive test coverage for all critical components.

- **Testing Framework**: Tests will be implemented using `pytest`.
- **Mocking**: The `pytest-mock` library will be used to create a mock of the `lmstudio` package. This allows for testing the application's logic without needing a live LM Studio instance, ensuring that tests are fast, repeatable, and can run in any environment (like a CI/CD pipeline).

- **Test Implementation Plan**:

  - **`tests/core/test_context_tester.py`**: 
    - Create a `test_binary_search_logic` function that uses a mocked `lmstudio` client.
    - The mock will simulate different scenarios: a model that loads and passes inference at certain context sizes but fails at others.
    - Assert that the `find_max_working_context` method correctly identifies the highest passing context size.
    - Test edge cases: a model that never loads, a model that loads but always fails inference, and a model that works at all tested sizes.

  - **`tests/loaders/test_model_loader.py`**:
    - Mock the `lmstudio.list_downloaded_models` function to return a predefined list of model dictionaries.
    - Test the `scan_and_update_registry` function.
    - Assert that new models are added, existing models are updated (without overwriting test results), and deleted models are removed from the registry.

  - **`tests/utils/test_paths.py`**:
    - Mock the `Path.home()` and `Path.exists()` methods.
    - Test the `get_lmstudio_path` function to ensure it correctly finds the path from the `.lmstudio-home-pointer` file.
    - Test the fallback logic to common directories if the pointer file does not exist.
    - Assert that `get_default_models_file` returns the correct `lmstrix.json` path.

## 3. Phase 3: Documentation & Release

**Goal**: Prepare the project for a successful v1.0.0 release on PyPI.

### 3.1. Documentation
- **`README.md`**: Update the README to include a clear, concise quick-start guide, installation instructions, and examples for the new CLI commands (`scan`, `test`, `list`).
- **API Documentation**: Add comprehensive docstrings to all public functions and classes, explaining their purpose, arguments, and return values.

### 3.2. Packaging & Release
- **`pyproject.toml`**: Verify that all dependencies, project metadata (version, author, license), and entry points are correct.
- **PyPI Release**: Once testing and documentation are complete, the project will be built and published to PyPI, making it installable via `pip install lmstrix`.

</document_content>
</document>

<document index="6">
<source>README.md</source>
<document_content>
# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command-Line Interface (CLI)

```bash
# Discover the optimal context window for a model
lmstrix optimize "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# List all available models with their detected context limits
lmstrix models list

# Scan for any new models you've added to LM Studio
lmstrix models scan

# Run inference using a prompt template
lmstrix infer --model "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" --prompt "greeting"
```

### Python API

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    client = LMStrix()

    # Optimize the context window for a specific model
    print("Optimizing context, this may take a moment...")
    optimization = await client.optimize_context("Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf")
    print(f"Optimal context found: {optimization.optimal_size} tokens")
    print("-" * 20)

    # List all models and their properties
    print("Available Models:")
    models = await client.list_models()
    for model in models:
        limit = model.context_limit or "Not yet optimized"
        print(f"- {model.id}: {limit} tokens")
    print("-" * 20)

    # Run inference with a template
    print("Running inference...")
    result = await client.infer(
        model_id="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        prompt_template="Summarize this text: {text}",
        context={"text": "Your long document goes here..."}
    )
    print("Inference Result:")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

## Architecture

LMStrix is designed with a clean, modular architecture:

- **`api/`**: A dedicated client for communicating with the LM Studio local server API.
- **`core/`**: The heart of the application, containing the core business logic for models, inference, and the context optimization algorithm.
- **`loaders/`**: Handles loading and managing data for models, prompts, and context files.
- **`cli/`**: Implements the command-line interface.
- **`utils/`**: Shared utilities and helper functions.

## Development

```bash
# Install in editable mode for development
pip install -e .

# Run the test suite
pytest

# Format and lint the codebase
ruff format .
ruff check .
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are highly welcome! Please feel free to submit pull requests or file issues on our GitHub repository.

## Support

For bugs, feature requests, or general questions, please [file an issue](https://github.com/yourusername/lmstrix/issues) on our GitHub repository.
</document_content>
</document>

<document index="7">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures and configuration
â”œâ”€â”€ run_tests.py             # Simple test runner script
â”œâ”€â”€ test_api/                # API layer tests
â”‚   â”œâ”€â”€ test_client.py       # LMStudioClient tests
â”‚   â””â”€â”€ test_exceptions.py   # Custom exception tests
â”œâ”€â”€ test_core/               # Core module tests
â”‚   â”œâ”€â”€ test_context_tester.py  # Context optimization tests
â”‚   â”œâ”€â”€ test_inference.py    # Inference engine tests
â”‚   â”œâ”€â”€ test_models.py       # Model and registry tests
â”‚   â”œâ”€â”€ test_prompts.py      # Prompt resolution tests
â”‚   â””â”€â”€ test_scanner.py      # Model scanner tests
â”œâ”€â”€ test_loaders/            # Loader tests
â”‚   â”œâ”€â”€ test_context_loader.py   # Context file loading tests
â”‚   â”œâ”€â”€ test_model_loader.py     # Model loader tests
â”‚   â””â”€â”€ test_prompt_loader.py    # Prompt loader tests
â”œâ”€â”€ test_utils/              # Utility tests
â”‚   â””â”€â”€ test_paths.py        # Path utility tests
â”œâ”€â”€ test_integration/        # Integration tests
â”‚   â””â”€â”€ test_cli_integration.py  # CLI integration tests
â””â”€â”€ test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="8">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 2: Testing & Quality Assurance

### Unit & Integration Tests
- [ ] Create `tests/core` directory.
- [ ] Create `tests/core/test_context_tester.py`.
- [ ] Implement `test_binary_search_logic` with a mocked `lmstudio` client.
- [ ] Test edge cases (model never loads, inference always fails, etc.).
- [ ] Create `tests/loaders` directory.
- [ ] Create `tests/loaders/test_model_loader.py`.
- [ ] Implement tests for `scan_and_update_registry` (add, update, remove models).
- [ ] Create `tests/utils` directory.
- [ ] Create `tests/utils/test_paths.py`.
- [ ] Implement tests for `get_lmstudio_path` and `get_default_models_file`.

### Code Quality
- [ ] Run `mypy` and fix any reported type errors.
- [ ] Run `ruff` and fix any reported linting issues.

## Phase 3: Documentation

### User Documentation
- [ ] Update `README.md` with the new CLI commands and examples.
- [ ] Add a section to the `README.md` explaining the context testing methodology.

### API Documentation
- [ ] Add comprehensive docstrings to all public functions and classes.

## Phase 4: Package & Release

### Package Preparation
- [ ] Verify all metadata in `pyproject.toml` is accurate.
- [ ] Perform a local test build and installation (`pip install .`).

### Release
- [ ] Tag the release as `v1.0.0` in git.
- [ ] Build the distribution packages (`python -m build`).
- [ ] Publish the package to PyPI.

</document_content>
</document>

<document index="9">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="10">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="11">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="12">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 101.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __main__.py
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“ test_api
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_client.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_exceptions.py
â”‚   â”œâ”€â”€ ğŸ“ test_core
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_tester.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_inference.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_models.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_prompts.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_scanner.py
â”‚   â”œâ”€â”€ ğŸ“ test_e2e
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ test_integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_cli_integration.py
â”‚   â”œâ”€â”€ ğŸ“ test_loaders
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_model_loader.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_prompt_loader.py
â”‚   â”œâ”€â”€ ğŸ“ test_utils
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_paths.py
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py
â”‚   â””â”€â”€ ğŸ“„ run_tests.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ build.sh
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ pytest.ini
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TESTING.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Next Steps

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The core functionality is complete. The project has been successfully refactored to use the native `lmstudio` package for all model interactions, and all data is stored in the correct, centralized location. The CLI and Python API are functional.

The immediate next steps are to build a comprehensive test suite to ensure reliability and to prepare the package for its initial public release.

## 2. Phase 2: Testing & Quality Assurance

This phase focuses on ensuring the existing codebase is robust, reliable, and free of bugs.

### 2.1. Unit & Integration Testing
**Goal**: Achieve comprehensive test coverage for all critical components.

- **Testing Framework**: Tests will be implemented using `pytest`.
- **Mocking**: The `pytest-mock` library will be used to create a mock of the `lmstudio` package. This allows for testing the application's logic without needing a live LM Studio instance, ensuring that tests are fast, repeatable, and can run in any environment (like a CI/CD pipeline).

- **Test Implementation Plan**:

  - **`tests/core/test_context_tester.py`**: 
    - Create a `test_binary_search_logic` function that uses a mocked `lmstudio` client.
    - The mock will simulate different scenarios: a model that loads and passes inference at certain context sizes but fails at others.
    - Assert that the `find_max_working_context` method correctly identifies the highest passing context size.
    - Test edge cases: a model that never loads, a model that loads but always fails inference, and a model that works at all tested sizes.

  - **`tests/loaders/test_model_loader.py`**:
    - Mock the `lmstudio.list_downloaded_models` function to return a predefined list of model dictionaries.
    - Test the `scan_and_update_registry` function.
    - Assert that new models are added, existing models are updated (without overwriting test results), and deleted models are removed from the registry.

  - **`tests/utils/test_paths.py`**:
    - Mock the `Path.home()` and `Path.exists()` methods.
    - Test the `get_lmstudio_path` function to ensure it correctly finds the path from the `.lmstudio-home-pointer` file.
    - Test the fallback logic to common directories if the pointer file does not exist.
    - Assert that `get_default_models_file` returns the correct `lmstrix.json` path.

## 3. Phase 3: Documentation & Release

**Goal**: Prepare the project for a successful v1.0.0 release on PyPI.

### 3.1. Documentation
- **`README.md`**: Update the README to include a clear, concise quick-start guide, installation instructions, and examples for the new CLI commands (`scan`, `test`, `list`).
- **API Documentation**: Add comprehensive docstrings to all public functions and classes, explaining their purpose, arguments, and return values.

### 3.2. Packaging & Release
- **`pyproject.toml`**: Verify that all dependencies, project metadata (version, author, license), and entry points are correct.
- **PyPI Release**: Once testing and documentation are complete, the project will be built and published to PyPI, making it installable via `pip install lmstrix`.

</document_content>
</document>

<document index="5">
<source>README.md</source>
<document_content>
# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command-Line Interface (CLI)

```bash
# Discover the optimal context window for a model
lmstrix optimize "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# List all available models with their detected context limits
lmstrix models list

# Scan for any new models you've added to LM Studio
lmstrix models scan

# Run inference using a prompt template
lmstrix infer --model "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" --prompt "greeting"
```

### Python API

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    client = LMStrix()

    # Optimize the context window for a specific model
    print("Optimizing context, this may take a moment...")
    optimization = await client.optimize_context("Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf")
    print(f"Optimal context found: {optimization.optimal_size} tokens")
    print("-" * 20)

    # List all models and their properties
    print("Available Models:")
    models = await client.list_models()
    for model in models:
        limit = model.context_limit or "Not yet optimized"
        print(f"- {model.id}: {limit} tokens")
    print("-" * 20)

    # Run inference with a template
    print("Running inference...")
    result = await client.infer(
        model_id="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        prompt_template="Summarize this text: {text}",
        context={"text": "Your long document goes here..."}
    )
    print("Inference Result:")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

## Architecture

LMStrix is designed with a clean, modular architecture:

- **`api/`**: A dedicated client for communicating with the LM Studio local server API.
- **`core/`**: The heart of the application, containing the core business logic for models, inference, and the context optimization algorithm.
- **`loaders/`**: Handles loading and managing data for models, prompts, and context files.
- **`cli/`**: Implements the command-line interface.
- **`utils/`**: Shared utilities and helper functions.

## Development

```bash
# Install in editable mode for development
pip install -e .

# Run the test suite
pytest

# Format and lint the codebase
ruff format .
ruff check .
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are highly welcome! Please feel free to submit pull requests or file issues on our GitHub repository.

## Support

For bugs, feature requests, or general questions, please [file an issue](https://github.com/yourusername/lmstrix/issues) on our GitHub repository.
</document_content>
</document>

<document index="6">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures and configuration
â”œâ”€â”€ run_tests.py             # Simple test runner script
â”œâ”€â”€ test_api/                # API layer tests
â”‚   â”œâ”€â”€ test_client.py       # LMStudioClient tests
â”‚   â””â”€â”€ test_exceptions.py   # Custom exception tests
â”œâ”€â”€ test_core/               # Core module tests
â”‚   â”œâ”€â”€ test_context_tester.py  # Context optimization tests
â”‚   â”œâ”€â”€ test_inference.py    # Inference engine tests
â”‚   â”œâ”€â”€ test_models.py       # Model and registry tests
â”‚   â”œâ”€â”€ test_prompts.py      # Prompt resolution tests
â”‚   â””â”€â”€ test_scanner.py      # Model scanner tests
â”œâ”€â”€ test_loaders/            # Loader tests
â”‚   â”œâ”€â”€ test_context_loader.py   # Context file loading tests
â”‚   â”œâ”€â”€ test_model_loader.py     # Model loader tests
â”‚   â””â”€â”€ test_prompt_loader.py    # Prompt loader tests
â”œâ”€â”€ test_utils/              # Utility tests
â”‚   â””â”€â”€ test_paths.py        # Path utility tests
â”œâ”€â”€ test_integration/        # Integration tests
â”‚   â””â”€â”€ test_cli_integration.py  # CLI integration tests
â””â”€â”€ test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="7">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 2: Testing & Quality Assurance

### Unit & Integration Tests
- [ ] Create `tests/core` directory.
- [ ] Create `tests/core/test_context_tester.py`.
- [ ] Implement `test_binary_search_logic` with a mocked `lmstudio` client.
- [ ] Test edge cases (model never loads, inference always fails, etc.).
- [ ] Create `tests/loaders` directory.
- [ ] Create `tests/loaders/test_model_loader.py`.
- [ ] Implement tests for `scan_and_update_registry` (add, update, remove models).
- [ ] Create `tests/utils` directory.
- [ ] Create `tests/utils/test_paths.py`.
- [ ] Implement tests for `get_lmstudio_path` and `get_default_models_file`.

### Code Quality
- [ ] Run `mypy` and fix any reported type errors.
- [ ] Run `ruff` and fix any reported linting issues.

## Phase 3: Documentation

### User Documentation
- [ ] Update `README.md` with the new CLI commands and examples.
- [ ] Add a section to the `README.md` explaining the context testing methodology.

### API Documentation
- [ ] Add comprehensive docstrings to all public functions and classes.

## Phase 4: Package & Release

### Package Preparation
- [ ] Verify all metadata in `pyproject.toml` is accurate.
- [ ] Perform a local test build and installation (`pip install .`).

### Release
- [ ] Tag the release as `v1.0.0` in git.
- [ ] Build the distribution packages (`python -m build`).
- [ ] Publish the package to PyPI.

</document_content>
</document>

<document index="8">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="9">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="10">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="11">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 101.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __main__.py
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ ğŸ“ test_api
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_client.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_exceptions.py
â”‚   â”œâ”€â”€ ğŸ“ test_core
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_tester.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_inference.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_models.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_prompts.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_scanner.py
â”‚   â”œâ”€â”€ ğŸ“ test_e2e
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“ test_integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_cli_integration.py
â”‚   â”œâ”€â”€ ğŸ“ test_loaders
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_context_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_model_loader.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_prompt_loader.py
â”‚   â”œâ”€â”€ ğŸ“ test_utils
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ test_paths.py
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ conftest.py
â”‚   â””â”€â”€ ğŸ“„ run_tests.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ build.sh
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ pytest.ini
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Next Steps

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The core functionality is complete. The project has been successfully refactored to use the native `lmstudio` package for all model interactions, and all data is stored in the correct, centralized location. The CLI and Python API are functional.

The immediate next steps are to build a comprehensive test suite to ensure reliability and to prepare the package for its initial public release.

## 2. Phase 2: Testing & Quality Assurance

This phase focuses on ensuring the existing codebase is robust, reliable, and free of bugs.

### 2.1. Unit & Integration Testing
**Goal**: Achieve comprehensive test coverage for all critical components.

- **Testing Framework**: Tests will be implemented using `pytest`.
- **Mocking**: The `pytest-mock` library will be used to create a mock of the `lmstudio` package. This allows for testing the application's logic without needing a live LM Studio instance, ensuring that tests are fast, repeatable, and can run in any environment (like a CI/CD pipeline).

- **Test Implementation Plan**:

  - **`tests/core/test_context_tester.py`**: 
    - Create a `test_binary_search_logic` function that uses a mocked `lmstudio` client.
    - The mock will simulate different scenarios: a model that loads and passes inference at certain context sizes but fails at others.
    - Assert that the `find_max_working_context` method correctly identifies the highest passing context size.
    - Test edge cases: a model that never loads, a model that loads but always fails inference, and a model that works at all tested sizes.

  - **`tests/loaders/test_model_loader.py`**:
    - Mock the `lmstudio.list_downloaded_models` function to return a predefined list of model dictionaries.
    - Test the `scan_and_update_registry` function.
    - Assert that new models are added, existing models are updated (without overwriting test results), and deleted models are removed from the registry.

  - **`tests/utils/test_paths.py`**:
    - Mock the `Path.home()` and `Path.exists()` methods.
    - Test the `get_lmstudio_path` function to ensure it correctly finds the path from the `.lmstudio-home-pointer` file.
    - Test the fallback logic to common directories if the pointer file does not exist.
    - Assert that `get_default_models_file` returns the correct `lmstrix.json` path.

## 3. Phase 3: Documentation & Release

**Goal**: Prepare the project for a successful v1.0.0 release on PyPI.

### 3.1. Documentation
- **`README.md`**: Update the README to include a clear, concise quick-start guide, installation instructions, and examples for the new CLI commands (`scan`, `test`, `list`).
- **API Documentation**: Add comprehensive docstrings to all public functions and classes, explaining their purpose, arguments, and return values.

### 3.2. Packaging & Release
- **`pyproject.toml`**: Verify that all dependencies, project metadata (version, author, license), and entry points are correct.
- **PyPI Release**: Once testing and documentation are complete, the project will be built and published to PyPI, making it installable via `pip install lmstrix`.

</document_content>
</document>

<document index="5">
<source>README.md</source>
<document_content>
# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command-Line Interface (CLI)

```bash
# Discover the optimal context window for a model
lmstrix optimize "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# List all available models with their detected context limits
lmstrix models list

# Scan for any new models you've added to LM Studio
lmstrix models scan

# Run inference using a prompt template
lmstrix infer --model "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" --prompt "greeting"
```

### Python API

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    client = LMStrix()

    # Optimize the context window for a specific model
    print("Optimizing context, this may take a moment...")
    optimization = await client.optimize_context("Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf")
    print(f"Optimal context found: {optimization.optimal_size} tokens")
    print("-" * 20)

    # List all models and their properties
    print("Available Models:")
    models = await client.list_models()
    for model in models:
        limit = model.context_limit or "Not yet optimized"
        print(f"- {model.id}: {limit} tokens")
    print("-" * 20)

    # Run inference with a template
    print("Running inference...")
    result = await client.infer(
        model_id="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        prompt_template="Summarize this text: {text}",
        context={"text": "Your long document goes here..."}
    )
    print("Inference Result:")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

## Architecture

LMStrix is designed with a clean, modular architecture:

- **`api/`**: A dedicated client for communicating with the LM Studio local server API.
- **`core/`**: The heart of the application, containing the core business logic for models, inference, and the context optimization algorithm.
- **`loaders/`**: Handles loading and managing data for models, prompts, and context files.
- **`cli/`**: Implements the command-line interface.
- **`utils/`**: Shared utilities and helper functions.

## Development

```bash
# Install in editable mode for development
pip install -e .

# Run the test suite
pytest

# Format and lint the codebase
ruff format .
ruff check .
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are highly welcome! Please feel free to submit pull requests or file issues on our GitHub repository.

## Support

For bugs, feature requests, or general questions, please [file an issue](https://github.com/yourusername/lmstrix/issues) on our GitHub repository.
</document_content>
</document>

<document index="6">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 2: Testing & Quality Assurance

### Unit & Integration Tests
- [ ] Create `tests/core` directory.
- [ ] Create `tests/core/test_context_tester.py`.
- [ ] Implement `test_binary_search_logic` with a mocked `lmstudio` client.
- [ ] Test edge cases (model never loads, inference always fails, etc.).
- [ ] Create `tests/loaders` directory.
- [ ] Create `tests/loaders/test_model_loader.py`.
- [ ] Implement tests for `scan_and_update_registry` (add, update, remove models).
- [ ] Create `tests/utils` directory.
- [ ] Create `tests/utils/test_paths.py`.
- [ ] Implement tests for `get_lmstudio_path` and `get_default_models_file`.

### Code Quality
- [ ] Run `mypy` and fix any reported type errors.
- [ ] Run `ruff` and fix any reported linting issues.

## Phase 3: Documentation

### User Documentation
- [ ] Update `README.md` with the new CLI commands and examples.
- [ ] Add a section to the `README.md` explaining the context testing methodology.

### API Documentation
- [ ] Add comprehensive docstrings to all public functions and classes.

## Phase 4: Package & Release

### Package Preparation
- [ ] Verify all metadata in `pyproject.toml` is accurate.
- [ ] Perform a local test build and installation (`pip install .`).

### Release
- [ ] Tag the release as `v1.0.0` in git.
- [ ] Build the distribution packages (`python -m build`).
- [ ] Publish the package to PyPI.

</document_content>
</document>

<document index="7">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="8">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="9">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="10">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 
â”‚       101.t
â”‚       xt
â”œâ”€â”€ ğŸ“ 
â”‚   obsolete
â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚   â”‚   lmsm.
â”‚   â”‚   json
â”‚   â””â”€â”€ ğŸ“„ 
â”‚       lmsm.
â”‚       py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ 
â”‚   results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ 
â”‚   â”‚   lmstr
â”‚   â”‚   ix
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   a
â”‚   â”‚   â”‚   p
â”‚   â”‚   â”‚   i
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   c
â”‚   â”‚   â”‚   l
â”‚   â”‚   â”‚   i
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   c
â”‚   â”‚   â”‚   o
â”‚   â”‚   â”‚   r
â”‚   â”‚   â”‚   e
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   l
â”‚   â”‚   â”‚   o
â”‚   â”‚   â”‚   a
â”‚   â”‚   â”‚   d
â”‚   â”‚   â”‚   e
â”‚   â”‚   â”‚   r
â”‚   â”‚   â”‚   s
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   u
â”‚   â”‚   â”‚   t
â”‚   â”‚   â”‚   i
â”‚   â”‚   â”‚   l
â”‚   â”‚   â”‚   s
â”‚   â”‚   â””â”€â”€  
â”‚   â”‚        
â”‚   â”‚       _
â”‚   â”‚       _
â”‚   â”‚       i
â”‚   â”‚       n
â”‚   â”‚       i
â”‚   â”‚       t
â”‚   â”‚       _
â”‚   â”‚       _
â”‚   â”‚       .
â”‚   â”‚       p
â”‚   â”‚       y
â”‚   â””â”€â”€ ğŸ“ 
â”‚       lmstr
â”‚       ix.eg
â”‚       g-inf
â”‚       o
â”œâ”€â”€ ğŸ“„ 
â”‚   .gitignor
â”‚   e
â”œâ”€â”€ ğŸ“„ 
â”‚   build.sh
â”œâ”€â”€ ğŸ“„ 
â”‚   CHANGELOG
â”‚   .md
â”œâ”€â”€ ğŸ“„ 
â”‚   LICENSE
â”œâ”€â”€ ğŸ“„ 
â”‚   llms.txt
â”œâ”€â”€ ğŸ“„ 
â”‚   PLAN.md
â”œâ”€â”€ ğŸ“„ 
â”‚   pyproject
â”‚   .toml
â”œâ”€â”€ ğŸ“„ 
â”‚   README.md
â”œâ”€â”€ ğŸ“„ 
â”‚   TODO.md
â””â”€â”€ ğŸ“„ 
    WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Next Steps

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The core functionality is complete. The project has been successfully refactored to use the native `lmstudio` package for all model interactions, and all data is stored in the correct, centralized location. The CLI and Python API are functional.

The immediate next steps are to build a comprehensive test suite to ensure reliability and to prepare the package for its initial public release.

## 2. Phase 2: Testing & Quality Assurance

This phase focuses on ensuring the existing codebase is robust, reliable, and free of bugs.

### 2.1. Unit & Integration Testing
**Goal**: Achieve comprehensive test coverage for all critical components.

- **Testing Framework**: Tests will be implemented using `pytest`.
- **Mocking**: The `pytest-mock` library will be used to create a mock of the `lmstudio` package. This allows for testing the application's logic without needing a live LM Studio instance, ensuring that tests are fast, repeatable, and can run in any environment (like a CI/CD pipeline).

- **Test Implementation Plan**:

  - **`tests/core/test_context_tester.py`**: 
    - Create a `test_binary_search_logic` function that uses a mocked `lmstudio` client.
    - The mock will simulate different scenarios: a model that loads and passes inference at certain context sizes but fails at others.
    - Assert that the `find_max_working_context` method correctly identifies the highest passing context size.
    - Test edge cases: a model that never loads, a model that loads but always fails inference, and a model that works at all tested sizes.

  - **`tests/loaders/test_model_loader.py`**:
    - Mock the `lmstudio.list_downloaded_models` function to return a predefined list of model dictionaries.
    - Test the `scan_and_update_registry` function.
    - Assert that new models are added, existing models are updated (without overwriting test results), and deleted models are removed from the registry.

  - **`tests/utils/test_paths.py`**:
    - Mock the `Path.home()` and `Path.exists()` methods.
    - Test the `get_lmstudio_path` function to ensure it correctly finds the path from the `.lmstudio-home-pointer` file.
    - Test the fallback logic to common directories if the pointer file does not exist.
    - Assert that `get_default_models_file` returns the correct `lmstrix.json` path.

## 3. Phase 3: Documentation & Release

**Goal**: Prepare the project for a successful v1.0.0 release on PyPI.

### 3.1. Documentation
- **`README.md`**: Update the README to include a clear, concise quick-start guide, installation instructions, and examples for the new CLI commands (`scan`, `test`, `list`).
- **API Documentation**: Add comprehensive docstrings to all public functions and classes, explaining their purpose, arguments, and return values.

### 3.2. Packaging & Release
- **`pyproject.toml`**: Verify that all dependencies, project metadata (version, author, license), and entry points are correct.
- **PyPI Release**: Once testing and documentation are complete, the project will be built and published to PyPI, making it installable via `pip install lmstrix`.

</document_content>
</document>

<document index="5">
<source>README.md</source>
<document_content>
# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command-Line Interface (CLI)

```bash
# Discover the optimal context window for a model
lmstrix optimize "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"

# List all available models with their detected context limits
lmstrix models list

# Scan for any new models you've added to LM Studio
lmstrix models scan

# Run inference using a prompt template
lmstrix infer --model "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf" --prompt "greeting"
```

### Python API

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    client = LMStrix()

    # Optimize the context window for a specific model
    print("Optimizing context, this may take a moment...")
    optimization = await client.optimize_context("Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf")
    print(f"Optimal context found: {optimization.optimal_size} tokens")
    print("-" * 20)

    # List all models and their properties
    print("Available Models:")
    models = await client.list_models()
    for model in models:
        limit = model.context_limit or "Not yet optimized"
        print(f"- {model.id}: {limit} tokens")
    print("-" * 20)

    # Run inference with a template
    print("Running inference...")
    result = await client.infer(
        model_id="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        prompt_template="Summarize this text: {text}",
        context={"text": "Your long document goes here..."}
    )
    print("Inference Result:")
    print(result.content)

if __name__ == "__main__":
    asyncio.run(main())
```

## Architecture

LMStrix is designed with a clean, modular architecture:

- **`api/`**: A dedicated client for communicating with the LM Studio local server API.
- **`core/`**: The heart of the application, containing the core business logic for models, inference, and the context optimization algorithm.
- **`loaders/`**: Handles loading and managing data for models, prompts, and context files.
- **`cli/`**: Implements the command-line interface.
- **`utils/`**: Shared utilities and helper functions.

## Development

```bash
# Install in editable mode for development
pip install -e .

# Run the test suite
pytest

# Format and lint the codebase
ruff format .
ruff check .
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are highly welcome! Please feel free to submit pull requests or file issues on our GitHub repository.

## Support

For bugs, feature requests, or general questions, please [file an issue](https://github.com/yourusername/lmstrix/issues) on our GitHub repository.
</document_content>
</document>

<document index="6">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 2: Testing & Quality Assurance

### Unit & Integration Tests
- [ ] Create `tests/core` directory.
- [ ] Create `tests/core/test_context_tester.py`.
- [ ] Implement `test_binary_search_logic` with a mocked `lmstudio` client.
- [ ] Test edge cases (model never loads, inference always fails, etc.).
- [ ] Create `tests/loaders` directory.
- [ ] Create `tests/loaders/test_model_loader.py`.
- [ ] Implement tests for `scan_and_update_registry` (add, update, remove models).
- [ ] Create `tests/utils` directory.
- [ ] Create `tests/utils/test_paths.py`.
- [ ] Implement tests for `get_lmstudio_path` and `get_default_models_file`.

### Code Quality
- [ ] Run `mypy` and fix any reported type errors.
- [ ] Run `ruff` and fix any reported linting issues.

## Phase 3: Documentation

### User Documentation
- [ ] Update `README.md` with the new CLI commands and examples.
- [ ] Add a section to the `README.md` explaining the context testing methodology.

### API Documentation
- [ ] Add comprehensive docstrings to all public functions and classes.

## Phase 4: Package & Release

### Package Preparation
- [ ] Verify all metadata in `pyproject.toml` is accurate.
- [ ] Perform a local test build and installation (`pip install .`).

### Release
- [ ] Tag the release as `v1.0.0` in git.
- [ ] Build the distribution packages (`python -m build`).
- [ ] Publish the package to PyPI.

</document_content>
</document>

<document index="7">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="8">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="9">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="10">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 
â”‚       101.t
â”‚       xt
â”œâ”€â”€ ğŸ“ 
â”‚   obsolete
â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚   â”‚   lmsm.
â”‚   â”‚   json
â”‚   â””â”€â”€ ğŸ“„ 
â”‚       lmsm.
â”‚       py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ 
â”‚   results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ 
â”‚   â”‚   lmstr
â”‚   â”‚   ix
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   a
â”‚   â”‚   â”‚   p
â”‚   â”‚   â”‚   i
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   c
â”‚   â”‚   â”‚   l
â”‚   â”‚   â”‚   i
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   c
â”‚   â”‚   â”‚   o
â”‚   â”‚   â”‚   r
â”‚   â”‚   â”‚   e
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   l
â”‚   â”‚   â”‚   o
â”‚   â”‚   â”‚   a
â”‚   â”‚   â”‚   d
â”‚   â”‚   â”‚   e
â”‚   â”‚   â”‚   r
â”‚   â”‚   â”‚   s
â”‚   â”‚   â”œâ”€â”€  
â”‚   â”‚   â”‚    
â”‚   â”‚   â”‚   u
â”‚   â”‚   â”‚   t
â”‚   â”‚   â”‚   i
â”‚   â”‚   â”‚   l
â”‚   â”‚   â”‚   s
â”‚   â”‚   â””â”€â”€  
â”‚   â”‚        
â”‚   â”‚       _
â”‚   â”‚       _
â”‚   â”‚       i
â”‚   â”‚       n
â”‚   â”‚       i
â”‚   â”‚       t
â”‚   â”‚       _
â”‚   â”‚       _
â”‚   â”‚       .
â”‚   â”‚       p
â”‚   â”‚       y
â”‚   â””â”€â”€ ğŸ“ 
â”‚       lmstr
â”‚       ix.eg
â”‚       g-inf
â”‚       o
â”œâ”€â”€ ğŸ“„ 
â”‚   .gitignor
â”‚   e
â”œâ”€â”€ ğŸ“„ 
â”‚   build.sh
â”œâ”€â”€ ğŸ“„ 
â”‚   CHANGELOG
â”‚   .md
â”œâ”€â”€ ğŸ“„ 
â”‚   LICENSE
â”œâ”€â”€ ğŸ“„ 
â”‚   llms.txt
â”œâ”€â”€ ğŸ“„ 
â”‚   PLAN.md
â”œâ”€â”€ ğŸ“„ 
â”‚   pyproject
â”‚   .toml
â”œâ”€â”€ ğŸ“„ 
â”‚   README.md
â”œâ”€â”€ ğŸ“„ 
â”‚   TODO.md
â””â”€â”€ ğŸ“„ 
    WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
uv.lock
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
target/
Thumbs.db
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` packageâ€™s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Next Steps

## 1. Project Vision & Status

**Vision**: Deliver a reliable, installable tool that solves the critical problem of models in LM Studio declaring false context limits. The tool provides automated discovery of true operational context limits.

**Current Status**: The core functionality is complete. The project has been successfully refactored to use the native `lmstudio` package for all model interactions, and all data is stored in the correct, centralized location. The CLI and Python API are functional.

The immediate next steps are to build a comprehensive test suite to ensure reliability and to prepare the package for its initial public release.

## 2. Phase 2: Testing & Quality Assurance

This phase focuses on ensuring the existing codebase is robust, reliable, and free of bugs.

### 2.1. Unit & Integration Testing
**Goal**: Achieve comprehensive test coverage for all critical components.

- **Testing Framework**: Tests will be implemented using `pytest`.
- **Mocking**: The `pytest-mock` library will be used to create a mock of the `lmstudio` package. This allows for testing the application's logic without needing a live LM Studio instance, ensuring that tests are fast, repeatable, and can run in any environment (like a CI/CD pipeline).

- **Test Implementation Plan**:

  - **`tests/core/test_context_tester.py`**: 
    - Create a `test_binary_search_logic` function that uses a mocked `lmstudio` client.
    - The mock will simulate different scenarios: a model that loads and passes inference at certain context sizes but fails at others.
    - Assert that the `find_max_working_context` method correctly identifies the highest passing context size.
    - Test edge cases: a model that never loads, a model that loads but always fails inference, and a model that works at all tested sizes.

  - **`tests/loaders/test_model_loader.py`**:
    - Mock the `lmstudio.list_downloaded_models` function to return a predefined list of model dictionaries.
    - Test the `scan_and_update_registry` function.
    - Assert that new models are added, existing models are updated (without overwriting test results), and deleted models are removed from the registry.

  - **`tests/utils/test_paths.py`**:
    - Mock the `Path.home()` and `Path.exists()` methods.
    - Test the `get_lmstudio_path` function to ensure it correctly finds the path from the `.lmstudio-home-pointer` file.
    - Test the fallback logic to common directories if the pointer file does not exist.
    - Assert that `get_default_models_file` returns the correct `lmstrix.json` path.

## 3. Phase 3: Documentation & Release

**Goal**: Prepare the project for a successful v1.0.0 release on PyPI.

### 3.1. Documentation
- **`README.md`**: Update the README to include a clear, concise quick-start guide, installation instructions, and examples for the new CLI commands (`scan`, `test`, `list`).
- **API Documentation**: Add comprehensive docstrings to all public functions and classes, explaining their purpose, arguments, and return values.

### 3.2. Packaging & Release
- **`pyproject.toml`**: Verify that all dependencies, project metadata (version, author, license), and entry points are correct.
- **PyPI Release**: Once testing and documentation are complete, the project will be built and published to PyPI, making it installable via `pip install lmstrix`.

</document_content>
</document>

<document index="5">
<source>README.md</source>
<document_content>
# LMStrix

A professional, installable PyPI package for managing and utilizing models with LM Studio.

## Overview

LMStrix is a robust toolkit that provides seamless integration with LM Studio's local server, enabling efficient model management and inference. The standout feature is the **Adaptive Context Optimizer**, which automatically determines the maximum operational context length for any given model, eliminating the need for manual tuning.

## Key Features

- **Model Management**: Easily discover, list, and manage LM Studio models
- **Adaptive Context Optimization**: Automatically find the optimal context window for each model using binary search
- **Flexible Inference Engine**: Run inference with customizable prompts and context management
- **Two-Phase Prompt Resolution**: Advanced templating system with placeholder resolution
- **Rich CLI**: Beautiful command-line interface with formatted tables and progress indicators
- **Async-First Design**: Built on modern async/await patterns for optimal performance
- **Robust Error Handling**: Comprehensive exception hierarchy and retry logic

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command Line Interface

```bash
# List available models
lmstrix models list

# Scan for new models
lmstrix models scan

# Run inference
lmstrix infer --model "llama-3.1" --prompt "greeting"

# Optimize a model's context window
lmstrix optimize llama-3.1
```

### Python API

```python
from lmstrix import LMStrix

# Initialize client
client = LMStrix()

# List models
models = await client.list_models()
for model in models:
    print(f"{model.id}: {model.context_limit} tokens")

# Run inference
result = await client.infer(
    model_id="llama-3.1",
    prompt_template="Summarize this text: {text}",
    context={"text": "Your long document here..."}
)
print(result.content)

# Optimize context window
optimization = await client.optimize_context("llama-3.1")
print(f"Optimal context: {optimization.optimal_size} tokens")
```

## Architecture

LMStrix is organized into modular components:

- **`api/`**: LM Studio API client with retry logic
- **`core/`**: Core business logic (models, inference, optimization)
- **`loaders/`**: Data loaders for models, prompts, and contexts
- **`cli/`**: Command-line interface using Fire and Rich
- **`utils/`**: Shared utilities

## Dependencies

- `pydantic`: Data validation and settings management
- `litellm`: Unified LLM API interface
- `rich`: Terminal formatting and progress bars
- `fire`: CLI framework
- `tenacity`: Retry logic
- `loguru`: Advanced logging
- `tiktoken`: Token counting

## Development

```bash
# Install in development mode
pip install -e .

# Run tests
python -m pytest

# Format code
python -m ruff format src/

# Check linting
python -m ruff check src/
```

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests to our repository.

## Support

For issues, feature requests, or questions, please file an issue on our GitHub repository.
</document_content>
</document>

<document index="6">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 Release

## Phase 2: Testing & Quality Assurance

### Unit & Integration Tests
- [ ] Create `tests/core` directory.
- [ ] Create `tests/core/test_context_tester.py`.
- [ ] Implement `test_binary_search_logic` with a mocked `lmstudio` client.
- [ ] Test edge cases (model never loads, inference always fails, etc.).
- [ ] Create `tests/loaders` directory.
- [ ] Create `tests/loaders/test_model_loader.py`.
- [ ] Implement tests for `scan_and_update_registry` (add, update, remove models).
- [ ] Create `tests/utils` directory.
- [ ] Create `tests/utils/test_paths.py`.
- [ ] Implement tests for `get_lmstudio_path` and `get_default_models_file`.

### Code Quality
- [ ] Run `mypy` and fix any reported type errors.
- [ ] Run `ruff` and fix any reported linting issues.

## Phase 3: Documentation

### User Documentation
- [ ] Update `README.md` with the new CLI commands and examples.
- [ ] Add a section to the `README.md` explaining the context testing methodology.

### API Documentation
- [ ] Add comprehensive docstrings to all public functions and classes.

## Phase 4: Package & Release

### Package Preparation
- [ ] Verify all metadata in `pyproject.toml` is accurate.
- [ ] Perform a local test build and installation (`pip install .`).

### Release
- [ ] Tag the release as `v1.0.0` in git.
- [ ] Build the distribution packages (`python -m build`).
- [ ] Publish the package to PyPI.

</document_content>
</document>

<document index="7">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="8">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="9">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="10">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 101.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __version__.py
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.1.0] - 2025-07-24

### Added

#### Project Structure
- Created modern Python package structure with `src/` layout
- Established modular architecture with clear separation of concerns
- Set up proper package metadata in `pyproject.toml`
- Added MIT License
- Created comprehensive `.gitignore` for Python projects

#### Core Components
- **Model Management** (`core/models.py`)
  - Implemented `Model` Pydantic class with validation
  - Created `ModelRegistry` for managing LM Studio models
  - Added backward compatibility with existing `lmsm.json` format
  - Implemented model ID sanitization for safe file operations

- **Inference Engine** (`core/inference.py`)
  - Built async `InferenceEngine` class
  - Integrated with LM Studio API via litellm
  - Added configurable temperature and max_tokens
  - Implemented proper error handling and result formatting

- **Context Optimizer** (`core/context.py`)
  - Developed binary search algorithm for optimal context discovery
  - Created `OptimizationResult` model for tracking results
  - Implemented caching mechanism to avoid redundant optimizations
  - Added configurable min/max bounds and retry logic

- **Prompt Resolution** (`core/prompts.py`)
  - Built two-phase placeholder resolution system
  - Created `PromptTemplate` class with validation
  - Implemented nested placeholder support
  - Added comprehensive error messages for missing placeholders

#### API Client
- **LM Studio Client** (`api/client.py`)
  - Wrapped litellm for LM Studio integration
  - Implemented retry logic with exponential backoff
  - Suppressed litellm verbose output
  - Added proper async/await support
  - Created unified completion interface

- **Exception Hierarchy** (`api/exceptions.py`)
  - Designed comprehensive exception classes
  - Added specific errors for API, validation, and model issues
  - Implemented helpful error messages

#### Data Loaders
- **Model Loader** (`loaders/model_loader.py`)
  - Created functions to load/save model registries
  - Added automatic discovery of model files
  - Implemented verbose logging support

- **Prompt Loader** (`loaders/prompt_loader.py`)
  - Built TOML-based prompt management
  - Added support for categories and descriptions
  - Implemented single and bulk prompt loading

- **Context Loader** (`loaders/context_loader.py`)
  - Created flexible context loading from files
  - Added token estimation using tiktoken
  - Implemented size-limited loading
  - Built support for multiple context files

#### Command Line Interface
- **CLI Framework** (`cli/main.py`)
  - Implemented Fire-based command structure
  - Added Rich formatting for beautiful output
  - Created commands: `models list`, `models scan`, `infer`, `optimize`
  - Integrated progress bars and status indicators

#### Public API
- **High-Level Interface** (`__init__.py`)
  - Created `LMStrix` class for simplified usage
  - Exposed key components in public API
  - Added comprehensive docstrings
  - Implemented convenience methods

### Technical Implementation Details

#### Code Quality
- Full type hints throughout the codebase
- Comprehensive docstrings following Google style
- Structured imports with proper `__all__` exports
- Consistent error handling patterns

#### Architecture Decisions
- Async-first design for better performance
- Dependency injection for testability
- Pydantic models for data validation
- Modular design with clear boundaries

#### Integration Features
- Seamless LM Studio server integration
- Environment variable configuration support
- Flexible file path resolution
- Backward compatibility with existing tools

### Fixed
- Resolved import issues after file reorganization
- Fixed Pydantic model validation errors
- Corrected async/await usage patterns
- Addressed path resolution for cross-platform compatibility

### Security
- No hardcoded credentials or API keys
- Safe file path handling
- Input validation on all user data
- Secure model ID sanitization

## [1.0.0] - 2025-07-24

### Changed

#### Major Refactoring: litellm to lmstudio Native Integration
- **Dependency Pivot**: Completely removed `litellm` dependency and replaced with native `lmstudio` package
- **API Client** (`api/client.py`): 
  - Rewritten to directly use `lmstudio.list_downloaded_models()`
  - Now uses `lmstudio.llm()` for model loading
  - Implements `llm.complete()` for all model interactions
- **Context Tester** (`core/context_tester.py`):
  - Completely rewritten to use native lmstudio client
  - Implements binary search to find maximum effective context window
  - Efficiently loads, tests, and unloads models during testing
- **Inference Engine** (`core/inference.py`):
  - Updated to use native client
  - Ensures models are loaded with tested context length before inference
- **Model Discovery** (`loaders/model_loader.py`):
  - Now uses lmstudio to discover models
  - Ensures local registry is always in sync with LM Studio's downloaded models

### Updated
- **CLI** (`cli/main.py`): All commands (scan, list, test, infer) updated to use refactored core logic
- **Public API** (`__init__.py`): Simplified to provide clean interface to lmstudio-native functionality
- **Development Plan** (`PLAN.md`): Rewritten to reflect new technical direction

### Technical Improvements
- Direct integration with LM Studio for better reliability and performance
- More efficient model loading and unloading
- Better alignment with project's core goal of testing true context limits

## [Unreleased]

### Planned
- Unit tests for all core modules
- Integration tests for API client
- Documentation generation with MkDocs
- GitHub Actions CI/CD pipeline
- PyPI package publishing setup
- Performance benchmarks
- Additional model optimization strategies
</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Native `lmstudio` Integration

## 1. Project Vision

**Vision**: LMStrix v1.0 will be a minimal viable product focused on solving the critical problem with LM Studio: many models falsely declare higher maximum context lengths than they can actually handle.

**Core Problem**: LM Studio models often declare context limits (e.g., 128k tokens) that fail in practice. Models may fail to load, produce gibberish, or only work correctly below a certain "real" max context.

**Solution**: LMStrix provides automated testing and validation of true operational context limits using native `lmstudio` package integration for direct and reliable model interaction.

## 2. Core Technical Approach: Native Integration

All core functionality is built directly on top of the `lmstudio` Python package:

- **Model Discovery**: `lmstudio.list_downloaded_models()`
- **Model Loading/Unloading**: `lmstudio.llm()` and `llm.unload()`
- **Model Information**: `llm.get_info()`
- **Inference**: `llm.complete()`
- **Configuration**: Model loading configured with `config={"context_length": size}`

## 3. Completed Features (v1.0.0)

### Major Refactoring Complete
- âœ… Removed `litellm` dependency completely
- âœ… Implemented native `lmstudio` package integration
- âœ… Refactored all core components for direct LM Studio interaction

### Core Components Implemented
- âœ… **API Client**: Direct wrapper around `lmstudio` package functions
- âœ… **Context Tester**: Binary search algorithm for finding true context limits
- âœ… **Inference Engine**: Native model loading with tested context lengths
- âœ… **Model Discovery**: Sync with LM Studio's downloaded models
- âœ… **CLI Interface**: Full command set (scan, list, test, infer, status)
- âœ… **Python API**: Clean programmatic interface to all functionality

## 4. Phase 2: Testing, Documentation, and Release

### 4.1. Testing
- **Unit Tests**: Cover the core logic for the binary search algorithm, response validation, and registry management
- **Integration Tests**: Write tests that use a mock of the `lmstudio` package to simulate end-to-end workflows without requiring a live LM Studio instance

### 4.2. Documentation
- Update `README.md` to reflect the new `lmstudio`-based approach
- Create a clear guide on the context testing methodology
- Document the CLI commands and Python API

### 4.3. Release
- Ensure `pyproject.toml` is complete and accurate
- Publish v1.0.0 to PyPI

## 5. Future Enhancements (Post v1.0)

- Advanced context optimization algorithms (beyond binary search validation)
- Streaming support for real-time inference
- Multi-model workflow capabilities
- GUI/web interface for easier interaction
- Performance benchmarking tools
- Model comparison features
</document_content>
</document>

<document index="5">
<source>README.md</source>
<document_content>
# LMStrix

A professional, installable PyPI package for managing and utilizing models with LM Studio.

## Overview

LMStrix is a robust toolkit that provides seamless integration with LM Studio's local server, enabling efficient model management and inference. The standout feature is the **Adaptive Context Optimizer**, which automatically determines the maximum operational context length for any given model, eliminating the need for manual tuning.

## Key Features

- **Model Management**: Easily discover, list, and manage LM Studio models
- **Adaptive Context Optimization**: Automatically find the optimal context window for each model using binary search
- **Flexible Inference Engine**: Run inference with customizable prompts and context management
- **Two-Phase Prompt Resolution**: Advanced templating system with placeholder resolution
- **Rich CLI**: Beautiful command-line interface with formatted tables and progress indicators
- **Async-First Design**: Built on modern async/await patterns for optimal performance
- **Robust Error Handling**: Comprehensive exception hierarchy and retry logic

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command Line Interface

```bash
# List available models
lmstrix models list

# Scan for new models
lmstrix models scan

# Run inference
lmstrix infer --model "llama-3.1" --prompt "greeting"

# Optimize a model's context window
lmstrix optimize llama-3.1
```

### Python API

```python
from lmstrix import LMStrix

# Initialize client
client = LMStrix()

# List models
models = await client.list_models()
for model in models:
    print(f"{model.id}: {model.context_limit} tokens")

# Run inference
result = await client.infer(
    model_id="llama-3.1",
    prompt_template="Summarize this text: {text}",
    context={"text": "Your long document here..."}
)
print(result.content)

# Optimize context window
optimization = await client.optimize_context("llama-3.1")
print(f"Optimal context: {optimization.optimal_size} tokens")
```

## Architecture

LMStrix is organized into modular components:

- **`api/`**: LM Studio API client with retry logic
- **`core/`**: Core business logic (models, inference, optimization)
- **`loaders/`**: Data loaders for models, prompts, and contexts
- **`cli/`**: Command-line interface using Fire and Rich
- **`utils/`**: Shared utilities

## Dependencies

- `pydantic`: Data validation and settings management
- `litellm`: Unified LLM API interface
- `rich`: Terminal formatting and progress bars
- `fire`: CLI framework
- `tenacity`: Retry logic
- `loguru`: Advanced logging
- `tiktoken`: Token counting

## Development

```bash
# Install in development mode
pip install -e .

# Run tests
python -m pytest

# Format code
python -m ruff format src/

# Check linting
python -m ruff check src/
```

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests to our repository.

## Support

For issues, feature requests, or questions, please file an issue on our GitHub repository.
</document_content>
</document>

<document index="6">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 MVP

## Phase 1: Core Functionality

### Model Discovery & Registry

- [x] Create system path detection for LM Studio data directory
- [x] Implement model scanner that finds all downloaded models
- [x] Add compatibility layer for existing lmsm.json format
- [x] Create data directory structure in LM Studio path
- [x] Implement model registry save/load with proper paths
- [x] Add model metadata extraction (size, declared context, capabilities)
- [x] Create model registry update mechanism
- [x] Add model removal detection and cleanup
- [x] Update model discovery to use lmstudio.list_downloaded_models()
- [x] Extract real model metadata using model.get_info()

### Context Validation System

- [x] Replace litellm with native lmstudio package
- [x] Update LMStudioClient to use lmstudio.llm() for model loading
- [x] Implement model loading with specific context size using config parameter
- [x] Update inference to use model.complete() instead of litellm
- [x] Add model.unload() after each test to free resources
- [x] Create context testing engine base class
- [x] Add simple prompt testing ("2+2=" -> "4")
- [x] Implement binary search for maximum loadable context
- [x] Create progressive context testing from min to max
- [x] Add response validation logic
- [ ] Implement per-model logging system
- [ ] Create log file format and structure
- [ ] Add context test status tracking
- [ ] Implement test resumption for interrupted tests
- [ ] Add tested_max_context field to model registry
- [ ] Create context test results storage

### CLI Updates

- [x] Update CLI to use new model discovery
- [x] Implement `lmstrix scan` command
- [x] Update `lmstrix list` to show context test status
- [x] Create `lmstrix test <model_id>` command
- [x] Add `lmstrix test --all` for batch testing
- [x] Create `lmstrix status` to show testing progress
- [x] Add progress bars for long operations
- [x] Implement proper error messages
- [x] Add CLI help documentation

### Python API Updates

- [x] Update LMStrix class with context testing methods
- [x] Add test_context_limits method
- [x] Create get_tested_context_limit method
- [x] Add context test status query methods
- [x] Implement async context testing support
- [x] Add batch testing capabilities
- [x] Create context test result models
- [x] Add proper exception handling

## Phase 2: Testing & Quality

### Unit Tests

- [ ] Write tests for model discovery
- [ ] Write tests for path detection logic
- [ ] Write tests for context binary search
- [ ] Write tests for response validation
- [ ] Write tests for log file handling
- [ ] Write tests for registry updates
- [ ] Write tests for CLI commands
- [ ] Write tests for error conditions

### Integration Tests

- [ ] Create mock LM Studio server
- [ ] Write end-to-end context testing tests
- [ ] Test interrupted test resumption
- [ ] Test batch model testing
- [ ] Test error recovery scenarios

### Code Quality

- [ ] Run mypy and fix type errors
- [ ] Run ruff and fix linting issues
- [ ] Add missing type hints
- [ ] Add comprehensive docstrings
- [ ] Review error handling

## Phase 3: Documentation

### User Documentation

- [ ] Update README.md with context testing features
- [ ] Write context testing methodology guide
- [ ] Create troubleshooting section
- [ ] Add common issues and solutions
- [ ] Write quick start guide

### API Documentation

- [ ] Document all CLI commands
- [ ] Document Python API methods
- [ ] Add code examples
- [ ] Create configuration guide
- [ ] Document log file format

### Examples

- [ ] Create example: Test single model
- [ ] Create example: Batch test all models
- [ ] Create example: Query test results
- [ ] Create example: Custom test prompts

## Phase 4: Package & Release

### Package Preparation

- [ ] Update pyproject.toml with all dependencies
- [ ] Add package metadata
- [ ] Include data files in package
- [ ] Test package build
- [ ] Test local installation

### Pre-release Testing

- [ ] Test on fresh Python environment
- [ ] Test on different OS platforms
- [ ] Verify all CLI commands work
- [ ] Test with real LM Studio instance
- [ ] Verify data storage locations

### Release Process

- [ ] Update version to 1.0.0
- [ ] Create git tag v1.0.0
- [ ] Build distribution packages
- [ ] Test on Test PyPI
- [ ] Publish to PyPI
- [ ] Create GitHub release
- [ ] Update documentation

## Critical Path Items

These must be completed for MVP:

1. [x] Model discovery with proper paths
2. [x] Context testing engine
3. [ ] Result logging and storage
4. [x] Basic CLI commands
5. [ ] Minimal documentation
6. [ ] Package configuration
</document_content>
</document>

<document index="7">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="8">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="9">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 101.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ lmstrix
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cli
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_tester.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ scanner.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ loaders
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ paths.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __version__.py
â”‚   â””â”€â”€ ğŸ“ lmstrix.egg-info
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.1.0] - 2025-07-24

### Added

#### Project Structure
- Created modern Python package structure with `src/` layout
- Established modular architecture with clear separation of concerns
- Set up proper package metadata in `pyproject.toml`
- Added MIT License
- Created comprehensive `.gitignore` for Python projects

#### Core Components
- **Model Management** (`core/models.py`)
  - Implemented `Model` Pydantic class with validation
  - Created `ModelRegistry` for managing LM Studio models
  - Added backward compatibility with existing `lmsm.json` format
  - Implemented model ID sanitization for safe file operations

- **Inference Engine** (`core/inference.py`)
  - Built async `InferenceEngine` class
  - Integrated with LM Studio API via litellm
  - Added configurable temperature and max_tokens
  - Implemented proper error handling and result formatting

- **Context Optimizer** (`core/context.py`)
  - Developed binary search algorithm for optimal context discovery
  - Created `OptimizationResult` model for tracking results
  - Implemented caching mechanism to avoid redundant optimizations
  - Added configurable min/max bounds and retry logic

- **Prompt Resolution** (`core/prompts.py`)
  - Built two-phase placeholder resolution system
  - Created `PromptTemplate` class with validation
  - Implemented nested placeholder support
  - Added comprehensive error messages for missing placeholders

#### API Client
- **LM Studio Client** (`api/client.py`)
  - Wrapped litellm for LM Studio integration
  - Implemented retry logic with exponential backoff
  - Suppressed litellm verbose output
  - Added proper async/await support
  - Created unified completion interface

- **Exception Hierarchy** (`api/exceptions.py`)
  - Designed comprehensive exception classes
  - Added specific errors for API, validation, and model issues
  - Implemented helpful error messages

#### Data Loaders
- **Model Loader** (`loaders/model_loader.py`)
  - Created functions to load/save model registries
  - Added automatic discovery of model files
  - Implemented verbose logging support

- **Prompt Loader** (`loaders/prompt_loader.py`)
  - Built TOML-based prompt management
  - Added support for categories and descriptions
  - Implemented single and bulk prompt loading

- **Context Loader** (`loaders/context_loader.py`)
  - Created flexible context loading from files
  - Added token estimation using tiktoken
  - Implemented size-limited loading
  - Built support for multiple context files

#### Command Line Interface
- **CLI Framework** (`cli/main.py`)
  - Implemented Fire-based command structure
  - Added Rich formatting for beautiful output
  - Created commands: `models list`, `models scan`, `infer`, `optimize`
  - Integrated progress bars and status indicators

#### Public API
- **High-Level Interface** (`__init__.py`)
  - Created `LMStrix` class for simplified usage
  - Exposed key components in public API
  - Added comprehensive docstrings
  - Implemented convenience methods

### Technical Implementation Details

#### Code Quality
- Full type hints throughout the codebase
- Comprehensive docstrings following Google style
- Structured imports with proper `__all__` exports
- Consistent error handling patterns

#### Architecture Decisions
- Async-first design for better performance
- Dependency injection for testability
- Pydantic models for data validation
- Modular design with clear boundaries

#### Integration Features
- Seamless LM Studio server integration
- Environment variable configuration support
- Flexible file path resolution
- Backward compatibility with existing tools

### Fixed
- Resolved import issues after file reorganization
- Fixed Pydantic model validation errors
- Corrected async/await usage patterns
- Addressed path resolution for cross-platform compatibility

### Security
- No hardcoded credentials or API keys
- Safe file path handling
- Input validation on all user data
- Secure model ID sanitization

## [1.0.0] - 2025-07-24

### Changed

#### Major Refactoring: litellm to lmstudio Native Integration
- **Dependency Pivot**: Completely removed `litellm` dependency and replaced with native `lmstudio` package
- **API Client** (`api/client.py`): 
  - Rewritten to directly use `lmstudio.list_downloaded_models()`
  - Now uses `lmstudio.llm()` for model loading
  - Implements `llm.complete()` for all model interactions
- **Context Tester** (`core/context_tester.py`):
  - Completely rewritten to use native lmstudio client
  - Implements binary search to find maximum effective context window
  - Efficiently loads, tests, and unloads models during testing
- **Inference Engine** (`core/inference.py`):
  - Updated to use native client
  - Ensures models are loaded with tested context length before inference
- **Model Discovery** (`loaders/model_loader.py`):
  - Now uses lmstudio to discover models
  - Ensures local registry is always in sync with LM Studio's downloaded models

### Updated
- **CLI** (`cli/main.py`): All commands (scan, list, test, infer) updated to use refactored core logic
- **Public API** (`__init__.py`): Simplified to provide clean interface to lmstudio-native functionality
- **Development Plan** (`PLAN.md`): Rewritten to reflect new technical direction

### Technical Improvements
- Direct integration with LM Studio for better reliability and performance
- More efficient model loading and unloading
- Better alignment with project's core goal of testing true context limits

## [Unreleased]

### Planned
- Unit tests for all core modules
- Integration tests for API client
- Documentation generation with MkDocs
- GitHub Actions CI/CD pipeline
- PyPI package publishing setup
- Performance benchmarks
- Additional model optimization strategies
</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 Pivot to Native `lmstudio`

## 1. Project Pivot: From `litellm` to `lmstudio`

**Vision**: LMStrix v1.0 will be a minimal viable product focused on solving the critical problem with LM Studio: many models falsely declare higher maximum context lengths than they can actually handle.

**Core Problem**: LM Studio models often declare context limits (e.g., 128k tokens) that fail in practice. Models may fail to load, produce gibberish, or only work correctly below a certain "real" max context.

**Strategic Change**: The initial v0.1.0 implementation relied on `litellm` for API interaction. Based on performance and reliability issues (see `issues/101.txt`), this plan outlines a pivot to using the native `lmstudio` Python package directly for all model interactions. This provides a more robust and direct integration with LM Studio.

## 2. Core Technical Approach: Native Integration

All core functionality will be built directly on top of the `lmstudio` Python package. This completely removes the `litellm` dependency.

- **Model Discovery**: `lmstudio.list_downloaded_models()`
- **Model Loading/Unloading**: `lmstudio.llm()` and `llm.unload()`
- **Model Information**: `llm.get_info()`
- **Inference**: `llm.complete()`
- **Configuration**: Model loading will be configured with `config={"context_length": size}`.

## 3. Phase 1: Refactoring and MVP Feature Implementation

This phase focuses on replacing the existing `litellm`-based implementation and completing the core features for the MVP.

### 3.1. Foundational Refactoring
**Goal**: Replace all `litellm` functionality with the `lmstudio` package.

- **Dependency Management**: Remove `litellm` from `pyproject.toml`.
- **API Client**: Rewrite `src/lmstrix/api/client.py` to be a thin wrapper around the `lmstudio` package's functions.
- **Inference Engine**: Update `src/lmstrix/core/inference.py` to use the new client and `llm.complete()`.
- **Context Testing**: Update `src/lmstrix/core/context_tester.py` to use `lmstudio.llm()` for loading models with specific context sizes and `llm.unload()` for cleanup.

### 3.2. Model Discovery & Registry
**Goal**: Reliable model discovery and metadata storage.

- **Scanner**: Use `lmstudio.list_downloaded_models()` to discover all models.
- **Metadata**: Use `llm.get_info()` to extract detailed and accurate model metadata.
- **Registry**: Create/update a `models.json` registry file.
- **Storage**: Store all data (`models.json`, logs) in a dedicated `lmstrix` folder within the LM Studio application data directory to avoid cluttering the project.

### 3.3. Context Validation System
**Goal**: Automatically discover the true operational context limit for each model.

- **Test Procedure**:
    1. For a given model, use a binary search algorithm between a minimum (e.g., 2048) and the model's declared maximum context length.
    2. In each step, attempt to load the model using `lmstudio.llm(model_id, config={"context_length": current_size})`.
    3. If loading succeeds, perform a simple "needle in a haystack" test: run inference with a prompt that requires recalling a specific piece of information. A simple "2+2=" -> "4" check is a good start.
    4. If the test passes, this context size is considered valid. The search continues for a higher valid size.
    5. The highest context size that both loads and passes the inference test is recorded as the `tested_max_context`.
    6. The model is unloaded using `llm.unload()` after each test to free up system resources.
- **Logging**: Log the entire test procedure for each model to `{model_id}_context_test.log`, recording context size, load success, and inference success/failure.
- **Results**: Store the final `tested_max_context` and a `context_test_status` (e.g., `untested`, `passed`, `failed`) in the `models.json` registry.

### 3.4. CLI and Python API
**Goal**: Provide simple and effective interfaces for users.

- **CLI Commands**:
    - `lmstrix scan`: Scan for models and update the registry.
    - `lmstrix list`: List all models, showing their declared vs. tested context limits and test status.
    - `lmstrix test <model_id|--all>`: Run the context validation test on a specific model or all untested models.
    - `lmstrix status`: Show a summary of testing progress.
- **Python API**:
    - `lmstrix.list_models()`: Returns a list of `Model` objects.
    - `lmstrix.test_context(model_id)`: Runs the validation test for a model.
    - `lmstrix.get_model(model_id)`: Retrieves a model with its tested metadata.

## 4. Phase 2: Testing, Documentation, and Release

### 4.1. Testing
- **Unit Tests**: Cover the core logic for the binary search algorithm, response validation, and registry management.
- **Integration Tests**: Write tests that use a mock of the `lmstudio` package to simulate end-to-end workflows without requiring a live LM Studio instance.

### 4.2. Documentation
- Update `README.md` to reflect the new `lmstudio`-based approach and remove mentions of `litellm`.
- Create a clear guide on the context testing methodology.
- Document the CLI commands and Python API.

### 4.3. Release
- Ensure `pyproject.toml` is complete and accurate.
- Publish v1.0.0 to PyPI.

## 5. Out of Scope for v1.0

- Advanced context optimization algorithms (beyond the binary search validation).
- Streaming support, multi-model workflows, GUI/web interface.

</document_content>
</document>

<document index="5">
<source>README.md</source>
<document_content>
# LMStrix

A professional, installable PyPI package for managing and utilizing models with LM Studio.

## Overview

LMStrix is a robust toolkit that provides seamless integration with LM Studio's local server, enabling efficient model management and inference. The standout feature is the **Adaptive Context Optimizer**, which automatically determines the maximum operational context length for any given model, eliminating the need for manual tuning.

## Key Features

- **Model Management**: Easily discover, list, and manage LM Studio models
- **Adaptive Context Optimization**: Automatically find the optimal context window for each model using binary search
- **Flexible Inference Engine**: Run inference with customizable prompts and context management
- **Two-Phase Prompt Resolution**: Advanced templating system with placeholder resolution
- **Rich CLI**: Beautiful command-line interface with formatted tables and progress indicators
- **Async-First Design**: Built on modern async/await patterns for optimal performance
- **Robust Error Handling**: Comprehensive exception hierarchy and retry logic

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command Line Interface

```bash
# List available models
lmstrix models list

# Scan for new models
lmstrix models scan

# Run inference
lmstrix infer --model "llama-3.1" --prompt "greeting"

# Optimize a model's context window
lmstrix optimize llama-3.1
```

### Python API

```python
from lmstrix import LMStrix

# Initialize client
client = LMStrix()

# List models
models = await client.list_models()
for model in models:
    print(f"{model.id}: {model.context_limit} tokens")

# Run inference
result = await client.infer(
    model_id="llama-3.1",
    prompt_template="Summarize this text: {text}",
    context={"text": "Your long document here..."}
)
print(result.content)

# Optimize context window
optimization = await client.optimize_context("llama-3.1")
print(f"Optimal context: {optimization.optimal_size} tokens")
```

## Architecture

LMStrix is organized into modular components:

- **`api/`**: LM Studio API client with retry logic
- **`core/`**: Core business logic (models, inference, optimization)
- **`loaders/`**: Data loaders for models, prompts, and contexts
- **`cli/`**: Command-line interface using Fire and Rich
- **`utils/`**: Shared utilities

## Dependencies

- `pydantic`: Data validation and settings management
- `litellm`: Unified LLM API interface
- `rich`: Terminal formatting and progress bars
- `fire`: CLI framework
- `tenacity`: Retry logic
- `loguru`: Advanced logging
- `tiktoken`: Token counting

## Development

```bash
# Install in development mode
pip install -e .

# Run tests
python -m pytest

# Format code
python -m ruff format src/

# Check linting
python -m ruff check src/
```

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests to our repository.

## Support

For issues, feature requests, or questions, please file an issue on our GitHub repository.
</document_content>
</document>

<document index="6">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 MVP

## PRIORITY: Replace litellm with lmstudio package

### Immediate Tasks
- [ ] Remove litellm dependency from pyproject.toml
- [ ] Rewrite LMStudioClient to use native lmstudio package
- [ ] Update ContextTester to use lmstudio.llm() for model loading
- [ ] Replace litellm completion calls with model.complete()
- [ ] Add proper model unloading with model.unload()
- [ ] Update model discovery to use lmstudio.list_downloaded_models()
- [ ] Test with real LM Studio instance

## Phase 1: Core Functionality

### Model Discovery & Registry

- [x] Create system path detection for LM Studio data directory
- [x] Implement model scanner that finds all downloaded models
- [x] Add compatibility layer for existing lmsm.json format
- [x] Create data directory structure in LM Studio path
- [x] Implement model registry save/load with proper paths
- [x] Add model metadata extraction (size, declared context, capabilities)
- [x] Create model registry update mechanism
- [x] Add model removal detection and cleanup
- [ ] Update model discovery to use lmstudio.list_downloaded_models()
- [ ] Extract real model metadata using model.get_info()

### Context Validation System

- [ ] Replace litellm with native lmstudio package
- [ ] Update LMStudioClient to use lmstudio.llm() for model loading
- [ ] Implement model loading with specific context size using config parameter
- [ ] Update inference to use model.complete() instead of litellm
- [ ] Add model.unload() after each test to free resources
- [ ] Create context testing engine base class
- [ ] Add simple prompt testing ("2+2=" -> "4")
- [ ] Implement binary search for maximum loadable context
- [ ] Create progressive context testing from min to max
- [ ] Add response validation logic
- [ ] Implement per-model logging system
- [ ] Create log file format and structure
- [ ] Add context test status tracking
- [ ] Implement test resumption for interrupted tests
- [ ] Add tested_max_context field to model registry
- [ ] Create context test results storage

### CLI Updates

- [x] Update CLI to use new model discovery
- [x] Implement `lmstrix scan` command
- [x] Update `lmstrix list` to show context test status
- [x] Create `lmstrix test <model_id>` command
- [x] Add `lmstrix test --all` for batch testing
- [x] Create `lmstrix status` to show testing progress
- [x] Add progress bars for long operations
- [x] Implement proper error messages
- [x] Add CLI help documentation

### Python API Updates

- [ ] Update LMStrix class with context testing methods
- [ ] Add test_context_limits method
- [ ] Create get_tested_context_limit method
- [ ] Add context test status query methods
- [ ] Implement async context testing support
- [ ] Add batch testing capabilities
- [ ] Create context test result models
- [ ] Add proper exception handling

## Phase 2: Testing & Quality

### Unit Tests

- [ ] Write tests for model discovery
- [ ] Write tests for path detection logic
- [ ] Write tests for context binary search
- [ ] Write tests for response validation
- [ ] Write tests for log file handling
- [ ] Write tests for registry updates
- [ ] Write tests for CLI commands
- [ ] Write tests for error conditions

### Integration Tests

- [ ] Create mock LM Studio server
- [ ] Write end-to-end context testing tests
- [ ] Test interrupted test resumption
- [ ] Test batch model testing
- [ ] Test error recovery scenarios

### Code Quality

- [ ] Run mypy and fix type errors
- [ ] Run ruff and fix linting issues
- [ ] Add missing type hints
- [ ] Add comprehensive docstrings
- [ ] Review error handling

## Phase 3: Documentation

### User Documentation

- [ ] Update README.md with context testing features
- [ ] Write context testing methodology guide
- [ ] Create troubleshooting section
- [ ] Add common issues and solutions
- [ ] Write quick start guide

### API Documentation

- [ ] Document all CLI commands
- [ ] Document Python API methods
- [ ] Add code examples
- [ ] Create configuration guide
- [ ] Document log file format

### Examples

- [ ] Create example: Test single model
- [ ] Create example: Batch test all models
- [ ] Create example: Query test results
- [ ] Create example: Custom test prompts

## Phase 4: Package & Release

### Package Preparation

- [ ] Update pyproject.toml with all dependencies
- [ ] Add package metadata
- [ ] Include data files in package
- [ ] Test package build
- [ ] Test local installation

### Pre-release Testing

- [ ] Test on fresh Python environment
- [ ] Test on different OS platforms
- [ ] Verify all CLI commands work
- [ ] Test with real LM Studio instance
- [ ] Verify data storage locations

### Release Process

- [ ] Update version to 1.0.0
- [ ] Create git tag v1.0.0
- [ ] Build distribution packages
- [ ] Test on Test PyPI
- [ ] Publish to PyPI
- [ ] Create GitHub release
- [ ] Update documentation

## Critical Path Items

These must be completed for MVP:

1. [ ] Model discovery with proper paths
2. [ ] Context testing engine
3. [ ] Result logging and storage
4. [ ] Basic CLI commands
5. [ ] Minimal documentation
6. [ ] Package configuration
</document_content>
</document>

<document index="7">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="8">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="9">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”‚   â””â”€â”€ ğŸ“„ 101.txt
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â””â”€â”€ ğŸ“ lmstrix
â”‚       â”œâ”€â”€ ğŸ“ api
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   __init__.p
â”‚       â”‚   â”‚   y
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   client.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ 
â”‚       â”‚       exceptions
â”‚       â”‚       .py
â”‚       â”œâ”€â”€ ğŸ“ cli
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   __init__.p
â”‚       â”‚   â”‚   y
â”‚       â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚       â”œâ”€â”€ ğŸ“ core
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   __init__.p
â”‚       â”‚   â”‚   y
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   context.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   context_te
â”‚       â”‚   â”‚   ster.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   inference.
â”‚       â”‚   â”‚   py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   models.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   prompts.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ 
â”‚       â”‚       scanner.py
â”‚       â”œâ”€â”€ ğŸ“ loaders
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   __init__.p
â”‚       â”‚   â”‚   y
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   context_lo
â”‚       â”‚   â”‚   ader.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   model_load
â”‚       â”‚   â”‚   er.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ 
â”‚       â”‚       prompt_loa
â”‚       â”‚       der.py
â”‚       â”œâ”€â”€ ğŸ“ utils
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ 
â”‚       â”‚   â”‚   __init__.p
â”‚       â”‚   â”‚   y
â”‚       â”‚   â””â”€â”€ ğŸ“„ 
â”‚       â”‚       paths.py
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â””â”€â”€ ğŸ“„ 
â”‚           __version__.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ llms.txt
â”œâ”€â”€ ğŸ“„ PLAN.md
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ TODO.md
â””â”€â”€ ğŸ“„ WORK.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.1.0] - 2025-07-24

### Added

#### Project Structure
- Created modern Python package structure with `src/` layout
- Established modular architecture with clear separation of concerns
- Set up proper package metadata in `pyproject.toml`
- Added MIT License
- Created comprehensive `.gitignore` for Python projects

#### Core Components
- **Model Management** (`core/models.py`)
  - Implemented `Model` Pydantic class with validation
  - Created `ModelRegistry` for managing LM Studio models
  - Added backward compatibility with existing `lmsm.json` format
  - Implemented model ID sanitization for safe file operations

- **Inference Engine** (`core/inference.py`)
  - Built async `InferenceEngine` class
  - Integrated with LM Studio API via litellm
  - Added configurable temperature and max_tokens
  - Implemented proper error handling and result formatting

- **Context Optimizer** (`core/context.py`)
  - Developed binary search algorithm for optimal context discovery
  - Created `OptimizationResult` model for tracking results
  - Implemented caching mechanism to avoid redundant optimizations
  - Added configurable min/max bounds and retry logic

- **Prompt Resolution** (`core/prompts.py`)
  - Built two-phase placeholder resolution system
  - Created `PromptTemplate` class with validation
  - Implemented nested placeholder support
  - Added comprehensive error messages for missing placeholders

#### API Client
- **LM Studio Client** (`api/client.py`)
  - Wrapped litellm for LM Studio integration
  - Implemented retry logic with exponential backoff
  - Suppressed litellm verbose output
  - Added proper async/await support
  - Created unified completion interface

- **Exception Hierarchy** (`api/exceptions.py`)
  - Designed comprehensive exception classes
  - Added specific errors for API, validation, and model issues
  - Implemented helpful error messages

#### Data Loaders
- **Model Loader** (`loaders/model_loader.py`)
  - Created functions to load/save model registries
  - Added automatic discovery of model files
  - Implemented verbose logging support

- **Prompt Loader** (`loaders/prompt_loader.py`)
  - Built TOML-based prompt management
  - Added support for categories and descriptions
  - Implemented single and bulk prompt loading

- **Context Loader** (`loaders/context_loader.py`)
  - Created flexible context loading from files
  - Added token estimation using tiktoken
  - Implemented size-limited loading
  - Built support for multiple context files

#### Command Line Interface
- **CLI Framework** (`cli/main.py`)
  - Implemented Fire-based command structure
  - Added Rich formatting for beautiful output
  - Created commands: `models list`, `models scan`, `infer`, `optimize`
  - Integrated progress bars and status indicators

#### Public API
- **High-Level Interface** (`__init__.py`)
  - Created `LMStrix` class for simplified usage
  - Exposed key components in public API
  - Added comprehensive docstrings
  - Implemented convenience methods

### Technical Implementation Details

#### Code Quality
- Full type hints throughout the codebase
- Comprehensive docstrings following Google style
- Structured imports with proper `__all__` exports
- Consistent error handling patterns

#### Architecture Decisions
- Async-first design for better performance
- Dependency injection for testability
- Pydantic models for data validation
- Modular design with clear boundaries

#### Integration Features
- Seamless LM Studio server integration
- Environment variable configuration support
- Flexible file path resolution
- Backward compatibility with existing tools

### Fixed
- Resolved import issues after file reorganization
- Fixed Pydantic model validation errors
- Corrected async/await usage patterns
- Addressed path resolution for cross-platform compatibility

### Security
- No hardcoded credentials or API keys
- Safe file path handling
- Input validation on all user data
- Secure model ID sanitization

## [Unreleased]

### Planned
- Unit tests for all core modules
- Integration tests for API client
- Documentation generation with MkDocs
- GitHub Actions CI/CD pipeline
- PyPI package publishing setup
- Performance benchmarks
- Additional model optimization strategies
</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>PLAN.md</source>
<document_content>
# LMStrix Development Plan - v1.0 MVP

## Project Vision

LMStrix v1.0 will be a minimal viable product focused on solving the critical problem with LM Studio: many models falsely declare higher maximum context lengths than they can actually handle. The tool will provide automated discovery of true operational context limits and maintain a reliable model registry.

## Core Problem Statement

LM Studio models often declare context limits (e.g., 128k tokens) that fail in practice:
1. Models may fail to load at declared context length
2. Models may load but produce gibberish output
3. Only below a certain "real" max context do models produce correct output

## Technical Approach

Use the native `lmstudio` Python package for all model operations:
- Model discovery: `lmstudio.list_downloaded_models()`
- Model loading: `lmstudio.llm(model_id, config={"contextLength": size})`
- Model info: `model.get_info()`
- Inference: `model.complete(prompt)`
- Unloading: `model.unload()`

This avoids the limitations and issues with `litellm` and provides direct integration with LM Studio.

## Phase 1: Core Functionality (MVP Focus)

### 1.1 Model Discovery & Registry
**Goal**: Reliable model discovery and metadata storage

**Implementation**:
- Scan LM Studio models directory
- Create/update model registry JSON file
- Store in LM Studio data directory (not project directory)
- Maintain backward compatibility with existing lmsm.json format

**Key Features**:
- Auto-detect LM Studio installation path
- Discover all downloaded models
- Extract basic metadata (size, declared context, capabilities)
- Save to system-appropriate location

### 1.2 Context Validation System
**Goal**: Automatically discover true operational context limits

**Implementation Details**:
1. **Progressive Context Testing**:
   - Use `lmstudio` package for model loading and inference
   - Start with minimal context (32 tokens)
   - Test with simple prompt: "2+2="
   - Verify correct response: "4"
   - Binary search between min and declared max
   
2. **Test Procedure**:
   ```
   For each model:
   a. Try loading model with lmstudio.llm(model_id, config={"contextLength": max_context})
   b. If load fails, binary search down to find loadable context
   c. Once loaded, test inference at various context sizes using lmstudio completion
   d. Record results in model-specific log file
   e. Determine highest context that produces correct output
   f. Unload model after testing with llm.unload()
   ```

3. **Logging System**:
   - Create log file per model: `{model_id}_context_test.log`
   - Record: timestamp, context_size, load_success, prompt, response
   - Store logs in LM Studio data directory

4. **Result Storage**:
   - Add `tested_max_context` field to model registry
   - Add `context_test_status` field (untested/testing/completed/failed)
   - Add `context_test_log` field pointing to log file

### 1.3 CLI Interface
**Goal**: Simple, effective command-line interface

**Commands**:
- `lmstrix scan` - Scan for models and update registry
- `lmstrix list` - List all models with their metadata
- `lmstrix test <model_id>` - Test specific model's context limits
- `lmstrix test --all` - Test all untested models
- `lmstrix status` - Show testing progress and results

### 1.4 Python API
**Goal**: Programmatic access to core functionality

**Key Methods**:
```python
# Initialize
lmstrix = LMStrix()

# List models
models = lmstrix.list_models()

# Test context limits
result = lmstrix.test_context_limits(
    model_id="lucy-128k",
    min_context=32,
    max_context=128*1024,
    test_prompt="2+2="
)

# Get tested limits
limit = lmstrix.get_tested_context_limit("lucy-128k")
```

## Phase 2: Testing & Documentation

### 2.1 Basic Testing
**Goal**: Ensure reliability of core features

**Tests**:
- Unit tests for model discovery
- Unit tests for context testing logic
- Integration tests with mock LM Studio server
- CLI command tests

### 2.2 Essential Documentation
**Goal**: Clear, practical documentation

**Deliverables**:
- README.md with quick start guide
- CLI command reference
- Python API reference
- Context testing methodology explanation
- Troubleshooting guide

## Phase 3: Package & Release

### 3.1 Package Preparation
**Goal**: Installable via pip

**Tasks**:
- Ensure pyproject.toml is complete
- Add all necessary metadata
- Include data files properly
- Test installation process

### 3.2 Initial Release
**Goal**: v1.0.0 on PyPI

**Steps**:
1. Final testing
2. Version tagging
3. Build distribution
4. Upload to PyPI
5. Announce availability

## Out of Scope for v1.0

The following features are explicitly NOT included in v1.0:
- Advanced context optimization algorithms
- Streaming support
- Multi-model workflows
- Plugin system
- Performance monitoring
- Web interface
- Docker/Kubernetes support
- Enterprise features

## Success Criteria for v1.0

1. **Functional**:
   - Successfully discovers all LM Studio models
   - Accurately tests context limits
   - Produces reliable results
   - Saves data in appropriate system location

2. **Usable**:
   - Simple CLI interface
   - Clear Python API
   - Helpful error messages
   - Progress indication during testing

3. **Reliable**:
   - Handles errors gracefully
   - Doesn't corrupt LM Studio installation
   - Saves progress incrementally
   - Can resume interrupted tests

4. **Documented**:
   - Clear installation instructions
   - Usage examples
   - Troubleshooting guide
   - API reference

## Implementation Notes

### Data Storage Locations
- Use LM Studio's data directory for all persistent data
- Model registry: `{lm_studio_path}/lmstrix/models.json`
- Test logs: `{lm_studio_path}/lmstrix/context_tests/`
- Never store data in the package directory

### Context Testing Algorithm
```python
def test_context_limits(model_id, min_ctx=32, max_ctx=None):
    # 1. Try loading model with lmstudio.llm() at max context
    # 2. If fails, binary search for loadable context
    # 3. Test inference at various sizes using model.complete()
    # 4. Find highest context with correct output
    # 5. Log all attempts
    # 6. Unload model with model.unload()
    # 7. Update registry with results
```

### LM Studio Integration
- Use `lmstudio` package directly instead of `litellm`
- Leverage native model loading: `lmstudio.llm(model_id, config)`
- Use native completion API: `model.complete(prompt)`
- Properly unload models after testing to free resources
- Access model info with `model.get_info()`

### Error Handling
- Graceful handling of LM Studio connection issues
- Clear error messages for common problems
- Automatic retry with exponential backoff
- Save partial results on interruption
</document_content>
</document>

<document index="5">
<source>README.md</source>
<document_content>
# LMStrix

A professional, installable PyPI package for managing and utilizing models with LM Studio.

## Overview

LMStrix is a robust toolkit that provides seamless integration with LM Studio's local server, enabling efficient model management and inference. The standout feature is the **Adaptive Context Optimizer**, which automatically determines the maximum operational context length for any given model, eliminating the need for manual tuning.

## Key Features

- **Model Management**: Easily discover, list, and manage LM Studio models
- **Adaptive Context Optimization**: Automatically find the optimal context window for each model using binary search
- **Flexible Inference Engine**: Run inference with customizable prompts and context management
- **Two-Phase Prompt Resolution**: Advanced templating system with placeholder resolution
- **Rich CLI**: Beautiful command-line interface with formatted tables and progress indicators
- **Async-First Design**: Built on modern async/await patterns for optimal performance
- **Robust Error Handling**: Comprehensive exception hierarchy and retry logic

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command Line Interface

```bash
# List available models
lmstrix models list

# Scan for new models
lmstrix models scan

# Run inference
lmstrix infer --model "llama-3.1" --prompt "greeting"

# Optimize a model's context window
lmstrix optimize llama-3.1
```

### Python API

```python
from lmstrix import LMStrix

# Initialize client
client = LMStrix()

# List models
models = await client.list_models()
for model in models:
    print(f"{model.id}: {model.context_limit} tokens")

# Run inference
result = await client.infer(
    model_id="llama-3.1",
    prompt_template="Summarize this text: {text}",
    context={"text": "Your long document here..."}
)
print(result.content)

# Optimize context window
optimization = await client.optimize_context("llama-3.1")
print(f"Optimal context: {optimization.optimal_size} tokens")
```

## Architecture

LMStrix is organized into modular components:

- **`api/`**: LM Studio API client with retry logic
- **`core/`**: Core business logic (models, inference, optimization)
- **`loaders/`**: Data loaders for models, prompts, and contexts
- **`cli/`**: Command-line interface using Fire and Rich
- **`utils/`**: Shared utilities

## Dependencies

- `pydantic`: Data validation and settings management
- `litellm`: Unified LLM API interface
- `rich`: Terminal formatting and progress bars
- `fire`: CLI framework
- `tenacity`: Retry logic
- `loguru`: Advanced logging
- `tiktoken`: Token counting

## Development

```bash
# Install in development mode
pip install -e .

# Run tests
python -m pytest

# Format code
python -m ruff format src/

# Check linting
python -m ruff check src/
```

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests to our repository.

## Support

For issues, feature requests, or questions, please file an issue on our GitHub repository.
</document_content>
</document>

<document index="6">
<source>TODO.md</source>
<document_content>
# TODO List for LMStrix v1.0 MVP

## PRIORITY: Replace litellm with lmstudio package

### Immediate Tasks
- [ ] Remove litellm dependency from pyproject.toml
- [ ] Rewrite LMStudioClient to use native lmstudio package
- [ ] Update ContextTester to use lmstudio.llm() for model loading
- [ ] Replace litellm completion calls with model.complete()
- [ ] Add proper model unloading with model.unload()
- [ ] Update model discovery to use lmstudio.list_downloaded_models()
- [ ] Test with real LM Studio instance

## Phase 1: Core Functionality

### Model Discovery & Registry

- [x] Create system path detection for LM Studio data directory
- [x] Implement model scanner that finds all downloaded models
- [x] Add compatibility layer for existing lmsm.json format
- [x] Create data directory structure in LM Studio path
- [x] Implement model registry save/load with proper paths
- [x] Add model metadata extraction (size, declared context, capabilities)
- [x] Create model registry update mechanism
- [x] Add model removal detection and cleanup
- [ ] Update model discovery to use lmstudio.list_downloaded_models()
- [ ] Extract real model metadata using model.get_info()

### Context Validation System

- [ ] Replace litellm with native lmstudio package
- [ ] Update LMStudioClient to use lmstudio.llm() for model loading
- [ ] Implement model loading with specific context size using config parameter
- [ ] Update inference to use model.complete() instead of litellm
- [ ] Add model.unload() after each test to free resources
- [ ] Create context testing engine base class
- [ ] Add simple prompt testing ("2+2=" -> "4")
- [ ] Implement binary search for maximum loadable context
- [ ] Create progressive context testing from min to max
- [ ] Add response validation logic
- [ ] Implement per-model logging system
- [ ] Create log file format and structure
- [ ] Add context test status tracking
- [ ] Implement test resumption for interrupted tests
- [ ] Add tested_max_context field to model registry
- [ ] Create context test results storage

### CLI Updates

- [x] Update CLI to use new model discovery
- [x] Implement `lmstrix scan` command
- [x] Update `lmstrix list` to show context test status
- [x] Create `lmstrix test <model_id>` command
- [x] Add `lmstrix test --all` for batch testing
- [x] Create `lmstrix status` to show testing progress
- [x] Add progress bars for long operations
- [x] Implement proper error messages
- [x] Add CLI help documentation

### Python API Updates

- [ ] Update LMStrix class with context testing methods
- [ ] Add test_context_limits method
- [ ] Create get_tested_context_limit method
- [ ] Add context test status query methods
- [ ] Implement async context testing support
- [ ] Add batch testing capabilities
- [ ] Create context test result models
- [ ] Add proper exception handling

## Phase 2: Testing & Quality

### Unit Tests

- [ ] Write tests for model discovery
- [ ] Write tests for path detection logic
- [ ] Write tests for context binary search
- [ ] Write tests for response validation
- [ ] Write tests for log file handling
- [ ] Write tests for registry updates
- [ ] Write tests for CLI commands
- [ ] Write tests for error conditions

### Integration Tests

- [ ] Create mock LM Studio server
- [ ] Write end-to-end context testing tests
- [ ] Test interrupted test resumption
- [ ] Test batch model testing
- [ ] Test error recovery scenarios

### Code Quality

- [ ] Run mypy and fix type errors
- [ ] Run ruff and fix linting issues
- [ ] Add missing type hints
- [ ] Add comprehensive docstrings
- [ ] Review error handling

## Phase 3: Documentation

### User Documentation

- [ ] Update README.md with context testing features
- [ ] Write context testing methodology guide
- [ ] Create troubleshooting section
- [ ] Add common issues and solutions
- [ ] Write quick start guide

### API Documentation

- [ ] Document all CLI commands
- [ ] Document Python API methods
- [ ] Add code examples
- [ ] Create configuration guide
- [ ] Document log file format

### Examples

- [ ] Create example: Test single model
- [ ] Create example: Batch test all models
- [ ] Create example: Query test results
- [ ] Create example: Custom test prompts

## Phase 4: Package & Release

### Package Preparation

- [ ] Update pyproject.toml with all dependencies
- [ ] Add package metadata
- [ ] Include data files in package
- [ ] Test package build
- [ ] Test local installation

### Pre-release Testing

- [ ] Test on fresh Python environment
- [ ] Test on different OS platforms
- [ ] Verify all CLI commands work
- [ ] Test with real LM Studio instance
- [ ] Verify data storage locations

### Release Process

- [ ] Update version to 1.0.0
- [ ] Create git tag v1.0.0
- [ ] Build distribution packages
- [ ] Test on Test PyPI
- [ ] Publish to PyPI
- [ ] Create GitHub release
- [ ] Update documentation

## Critical Path Items

These must be completed for MVP:

1. [ ] Model discovery with proper paths
2. [ ] Context testing engine
3. [ ] Result logging and storage
4. [ ] Basic CLI commands
5. [ ] Minimal documentation
6. [ ] Package configuration
</document_content>
</document>

<document index="7">
<source>WORK.md</source>
<document_content>
# Work Progress

## Completed: MVP Core Functionality

### Summary of Work Completed (2025-07-24)

Successfully implemented the core functionality for LMStrix v1.0 MVP focused on solving the critical problem with LM Studio models falsely declaring higher context limits than they can handle.

### Completed Items

1. **System Path Detection** âœ“
   - Implemented LM Studio data directory detection
   - Created lmstrix data directory structure  
   - Updated model registry to use proper paths

2. **Context Testing Engine** âœ“
   - Created ContextTester class with binary search algorithm
   - Implemented simulated model loading with context size
   - Added binary search for max loadable context
   - Implemented simple prompt testing ("2+2=")
   - Created per-model logging system

3. **Model Management** âœ“
   - Updated Model class with context testing fields
   - Added ContextTestStatus enum
   - Refactored ModelRegistry with save/load functionality
   - Created ModelScanner for automatic discovery

4. **CLI Interface** âœ“
   - `lmstrix scan` - Scan for models
   - `lmstrix list` - List models with test status
   - `lmstrix test <model>` - Test specific model
   - `lmstrix test --all` - Test all untested models  
   - `lmstrix status` - Show testing progress
   - Added rich formatting and progress indicators

5. **Package Configuration** âœ“
   - Created comprehensive pyproject.toml
   - Fixed all linting issues
   - Formatted code with ruff

### Architecture Implemented

- Data stored in LM Studio directory: `{lmstudio_path}/lmstrix/`
- Model registry: `{lmstudio_path}/lmsm.json` or `{lmstudio_path}/lmstrix/models.json`
- Test logs: `{lmstudio_path}/lmstrix/context_tests/{model_id}_context_test.log`
- Backward compatible with existing lmsm.json format

### Ready for Testing

The MVP is now ready for:
1. Local installation: `pip install -e .`
2. Model scanning: `lmstrix scan`
3. Context testing: `lmstrix test --all`

### Next Priority Tasks

1. Write unit tests for core functionality
2. Test with real LM Studio instance
3. Package and publish to PyPI
4. Create user documentation
</document_content>
</document>

<document index="8">
<source>issues/101.txt</source>
<document_content>
Shouldn't the code use @ref/llmstudio.txt for much more? We don't want to use inferencing by `litellm`, it's shit. We want to use to use the `lmstudio` package for loading models and also for completion/inferencing.

</document_content>
</document>

<document index="9">
<source>llms.txt</source>
<document_content>
Project Structure:
ğŸ“ lmstrix
â”œâ”€â”€ ğŸ“ issues
â”œâ”€â”€ ğŸ“ obsolete
â”‚   â”œâ”€â”€ ğŸ“„ lmsm.json
â”‚   â””â”€â”€ ğŸ“„ lmsm.py
â”œâ”€â”€ ğŸ“ ref
â”œâ”€â”€ ğŸ“ results
â”œâ”€â”€ ğŸ“ src
â”‚   â””â”€â”€ ğŸ“ lmstrix
â”‚       â”œâ”€â”€ ğŸ“ api
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ client.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ exceptions.py
â”‚       â”œâ”€â”€ ğŸ“ cli
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚       â”œâ”€â”€ ğŸ“ core
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ context.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ inference.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ models.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ prompts.py
â”‚       â”œâ”€â”€ ğŸ“ loaders
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ context_loader.py
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ model_loader.py
â”‚       â”‚   â””â”€â”€ ğŸ“„ prompt_loader.py
â”‚       â”œâ”€â”€ ğŸ“ utils
â”‚       â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â””â”€â”€ ğŸ“„ __version__.py
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ CHANGELOG.md
â”œâ”€â”€ ğŸ“„ LICENSE
â””â”€â”€ ğŸ“„ README.md


<documents>
<document index="1">
<source>.gitignore</source>
<document_content>
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/

</document_content>
</document>

<document index="2">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [0.1.0] - 2025-07-24

### Added

#### Project Structure
- Created modern Python package structure with `src/` layout
- Established modular architecture with clear separation of concerns
- Set up proper package metadata in `pyproject.toml`
- Added MIT License
- Created comprehensive `.gitignore` for Python projects

#### Core Components
- **Model Management** (`core/models.py`)
  - Implemented `Model` Pydantic class with validation
  - Created `ModelRegistry` for managing LM Studio models
  - Added backward compatibility with existing `lmsm.json` format
  - Implemented model ID sanitization for safe file operations

- **Inference Engine** (`core/inference.py`)
  - Built async `InferenceEngine` class
  - Integrated with LM Studio API via litellm
  - Added configurable temperature and max_tokens
  - Implemented proper error handling and result formatting

- **Context Optimizer** (`core/context.py`)
  - Developed binary search algorithm for optimal context discovery
  - Created `OptimizationResult` model for tracking results
  - Implemented caching mechanism to avoid redundant optimizations
  - Added configurable min/max bounds and retry logic

- **Prompt Resolution** (`core/prompts.py`)
  - Built two-phase placeholder resolution system
  - Created `PromptTemplate` class with validation
  - Implemented nested placeholder support
  - Added comprehensive error messages for missing placeholders

#### API Client
- **LM Studio Client** (`api/client.py`)
  - Wrapped litellm for LM Studio integration
  - Implemented retry logic with exponential backoff
  - Suppressed litellm verbose output
  - Added proper async/await support
  - Created unified completion interface

- **Exception Hierarchy** (`api/exceptions.py`)
  - Designed comprehensive exception classes
  - Added specific errors for API, validation, and model issues
  - Implemented helpful error messages

#### Data Loaders
- **Model Loader** (`loaders/model_loader.py`)
  - Created functions to load/save model registries
  - Added automatic discovery of model files
  - Implemented verbose logging support

- **Prompt Loader** (`loaders/prompt_loader.py`)
  - Built TOML-based prompt management
  - Added support for categories and descriptions
  - Implemented single and bulk prompt loading

- **Context Loader** (`loaders/context_loader.py`)
  - Created flexible context loading from files
  - Added token estimation using tiktoken
  - Implemented size-limited loading
  - Built support for multiple context files

#### Command Line Interface
- **CLI Framework** (`cli/main.py`)
  - Implemented Fire-based command structure
  - Added Rich formatting for beautiful output
  - Created commands: `models list`, `models scan`, `infer`, `optimize`
  - Integrated progress bars and status indicators

#### Public API
- **High-Level Interface** (`__init__.py`)
  - Created `LMStrix` class for simplified usage
  - Exposed key components in public API
  - Added comprehensive docstrings
  - Implemented convenience methods

### Technical Implementation Details

#### Code Quality
- Full type hints throughout the codebase
- Comprehensive docstrings following Google style
- Structured imports with proper `__all__` exports
- Consistent error handling patterns

#### Architecture Decisions
- Async-first design for better performance
- Dependency injection for testability
- Pydantic models for data validation
- Modular design with clear boundaries

#### Integration Features
- Seamless LM Studio server integration
- Environment variable configuration support
- Flexible file path resolution
- Backward compatibility with existing tools

### Fixed
- Resolved import issues after file reorganization
- Fixed Pydantic model validation errors
- Corrected async/await usage patterns
- Addressed path resolution for cross-platform compatibility

### Security
- No hardcoded credentials or API keys
- Safe file path handling
- Input validation on all user data
- Secure model ID sanitization

## [Unreleased]

### Planned
- Unit tests for all core modules
- Integration tests for API client
- Documentation generation with MkDocs
- GitHub Actions CI/CD pipeline
- PyPI package publishing setup
- Performance benchmarks
- Additional model optimization strategies
</document_content>
</document>

<document index="3">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="4">
<source>README.md</source>
<document_content>
# LMStrix

A professional, installable PyPI package for managing and utilizing models with LM Studio.

## Overview

LMStrix is a robust toolkit that provides seamless integration with LM Studio's local server, enabling efficient model management and inference. The standout feature is the **Adaptive Context Optimizer**, which automatically determines the maximum operational context length for any given model, eliminating the need for manual tuning.

## Key Features

- **Model Management**: Easily discover, list, and manage LM Studio models
- **Adaptive Context Optimization**: Automatically find the optimal context window for each model using binary search
- **Flexible Inference Engine**: Run inference with customizable prompts and context management
- **Two-Phase Prompt Resolution**: Advanced templating system with placeholder resolution
- **Rich CLI**: Beautiful command-line interface with formatted tables and progress indicators
- **Async-First Design**: Built on modern async/await patterns for optimal performance
- **Robust Error Handling**: Comprehensive exception hierarchy and retry logic

## Installation

```bash
pip install lmstrix
```

## Quick Start

### Command Line Interface

```bash
# List available models
lmstrix models list

# Scan for new models
lmstrix models scan

# Run inference
lmstrix infer --model "llama-3.1" --prompt "greeting"

# Optimize a model's context window
lmstrix optimize llama-3.1
```

### Python API

```python
from lmstrix import LMStrix

# Initialize client
client = LMStrix()

# List models
models = await client.list_models()
for model in models:
    print(f"{model.id}: {model.context_limit} tokens")

# Run inference
result = await client.infer(
    model_id="llama-3.1",
    prompt_template="Summarize this text: {text}",
    context={"text": "Your long document here..."}
)
print(result.content)

# Optimize context window
optimization = await client.optimize_context("llama-3.1")
print(f"Optimal context: {optimization.optimal_size} tokens")
```

## Architecture

LMStrix is organized into modular components:

- **`api/`**: LM Studio API client with retry logic
- **`core/`**: Core business logic (models, inference, optimization)
- **`loaders/`**: Data loaders for models, prompts, and contexts
- **`cli/`**: Command-line interface using Fire and Rich
- **`utils/`**: Shared utilities

## Dependencies

- `pydantic`: Data validation and settings management
- `litellm`: Unified LLM API interface
- `rich`: Terminal formatting and progress bars
- `fire`: CLI framework
- `tenacity`: Retry logic
- `loguru`: Advanced logging
- `tiktoken`: Token counting

## Development

```bash
# Install in development mode
pip install -e .

# Run tests
python -m pytest

# Format code
python -m ruff format src/

# Check linting
python -m ruff check src/
```

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests to our repository.

## Support

For issues, feature requests, or questions, please file an issue on our GitHub repository.
</document_content>
</document>

<document index="5">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

from lmstrix.__version__ import __version__
from lmstrix.api import CompletionResponse, LMStudioClient
from lmstrix.core import (
    ContextOptimizer,
    InferenceEngine,
    Model,
    ModelRegistry,
    PromptResolver,
)
from lmstrix.loaders import load_context, load_model_registry, load_prompts

class LMStrix:
    """High-level interface for LMStrix functionality."""
    def __init__((
        self,
        endpoint: str = "http://localhost:1234/v1",
        models_file: str | None = None,
        verbose: bool = False,
    )):
        """Initialize LMStrix client."""
    def list_models((self)) -> list[Model]:
        """List all available models."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a specific model by ID."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs,
    )):
        """Run inference on a model."""
    def optimize_context((self, model_id: str)):
        """Find optimal context size for a model."""

def __init__((
        self,
        endpoint: str = "http://localhost:1234/v1",
        models_file: str | None = None,
        verbose: bool = False,
    )):
    """Initialize LMStrix client."""

def list_models((self)) -> list[Model]:
    """List all available models."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a specific model by ID."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs,
    )):
    """Run inference on a model."""

def optimize_context((self, model_id: str)):
    """Find optimal context size for a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__version__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

import os
from typing import Any, Optional
import litellm
from litellm import acompletion
from loguru import logger
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential
from lmstrix.api.exceptions import APIConnectionError, InferenceError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Async client for interacting with LM Studio API."""
    def __init__((
        self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False
    )):
        """Initialize the LM Studio client."""

def __init__((
        self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False
    )):
    """Initialize the LM Studio client."""

def acompletion((
        self,
        model_id: str,
        messages: list[dict[str, str]],
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> CompletionResponse:
    """Make an async completion request to LM Studio."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.table import Table
from lmstrix import LMStrix

class LMStrixCLI:
    """LMStrix command-line interface."""
    def __init__((
        self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False
    )):
        """Initialize the CLI. ..."""
    def models((self, action: str = "list", model_id: str = None)):
        """Manage models. ..."""
    def _list_models((self)):
        """List all available models."""
    def infer((
        self,
        prompt: str,
        model: str = None,
        max_tokens: int = 2048,
        temperature: float = 0.7,
    )):
        """Run inference on a model."""
    def optimize((self, model: str, all: bool = False)):
        """Run context optimization for models."""

def __init__((
        self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False
    )):
    """Initialize the CLI. ..."""

def models((self, action: str = "list", model_id: str = None)):
    """Manage models. ..."""

def _list_models((self)):
    """List all available models."""

def infer((
        self,
        prompt: str,
        model: str = None,
        max_tokens: int = 2048,
        temperature: float = 0.7,
    )):
    """Run inference on a model."""

def optimize((self, model: str, all: bool = False)):
    """Run context optimization for models."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from typing import Optional, Tuple
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: Optional[LMStudioClient] = None,
        cache_file: Optional[Path] = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: Optional[str] = None,
    )) -> Tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: Optional[int] = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: Optional[LMStudioClient] = None,
        cache_file: Optional[Path] = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: Optional[str] = None,
    )) -> Tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: Optional[int] = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any, Optional
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: Optional[LMStudioClient] = None,
        model_registry: Optional[ModelRegistry] = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: Optional[LMStudioClient] = None,
        model_registry: Optional[ModelRegistry] = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from pathlib import Path
from typing import Any, Optional
from loguru import logger
from pydantic import BaseModel, Field, field_validator

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Optional[Path] = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def get_model((self, model_id: str)) -> Optional[Model]:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def __init__((self, models_file: Optional[Path] = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def get_model((self, model_id: str)) -> Optional[Model]:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any, Optional
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((
        self, text: str, root: dict[str, Any]
    )) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((
        self, text: str, params: Mapping[str, str]
    )) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: Optional[int] = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((
        self, text: str, root: dict[str, Any]
    )) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((
        self, text: str, params: Mapping[str, str]
    )) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: Optional[int] = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
from typing import Union
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: Union[str, Path],
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[Union[str, Path]],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: Union[str, Path],
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: Union[str, Path],
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from typing import Optional
from loguru import logger
from lmstrix.core.models import ModelRegistry

def load_model_registry((
    json_path: Optional[Path] = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load models from a JSON file into a ModelRegistry."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Optional[Path] = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any, Optional
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: Optional[PromptResolver] = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: Optional[PromptResolver] = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python



</documents>
</document_content>
</document>

<document index="10">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="11">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "lmstrix"
version = "1.0.0"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "LMStrix Contributors"},
]
keywords = ["llm", "lmstudio", "context", "optimization", "testing"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "pydantic>=2.0",
    "litellm>=1.0",
    "rich>=13.0",
    "fire>=0.5",
    "toml>=0.10",
    "tenacity>=8.0",
    "loguru>=0.7",
    "httpx>=0.24",
    "tiktoken>=0.5",
    "lmstudio>=0.1",  # For actual LM Studio integration
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-asyncio>=0.21",
    "pytest-cov>=4.0",
    "pytest-mock>=3.0",
    "mypy>=1.0",
    "ruff>=0.1",
    "black>=23.0",
]

docs = [
    "mkdocs>=1.5",
    "mkdocs-material>=9.0",
    "mkdocstrings[python]>=0.22",
]

[project.urls]
"Homepage" = "https://github.com/yourusername/lmstrix"
"Bug Tracker" = "https://github.com/yourusername/lmstrix/issues"
"Documentation" = "https://lmstrix.readthedocs.io"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
lmstrix = ["py.typed"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = "fire.*"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "litellm.*"
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = "lmstudio.*"
ignore_missing_imports = true

[tool.ruff]
target-version = "py310"
line-length = 100
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.isort]
known-third-party = ["pydantic", "litellm", "rich", "fire", "loguru", "tenacity"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_functions = ["test_*"]
python_classes = ["Test*"]
asyncio_mode = "auto"
addopts = [
    "--strict-markers",
    "--tb=short",
    "--cov=lmstrix",
    "--cov-report=term-missing",
    "--cov-report=html",
]

[tool.coverage.run]
source = ["src/lmstrix"]
omit = ["*/tests/*", "*/__init__.py"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if __name__ == .__main__.:",
    "raise AssertionError",
    "raise NotImplementedError",
]
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

from lmstrix.__version__ import __version__
from lmstrix.api import CompletionResponse, LMStudioClient
from lmstrix.core import (
    ContextOptimizer,
    InferenceEngine,
    Model,
    ModelRegistry,
    PromptResolver,
)
from lmstrix.loaders import load_context, load_model_registry, load_prompts

class LMStrix:
    """High-level interface for LMStrix functionality."""
    def __init__((
        self,
        endpoint: str = "http://localhost:1234/v1",
        models_file: str | None = None,
        verbose: bool = False,
    )):
        """Initialize LMStrix client."""
    def list_models((self)) -> list[Model]:
        """List all available models."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a specific model by ID."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs,
    )):
        """Run inference on a model."""
    def optimize_context((self, model_id: str)):
        """Find optimal context size for a model."""

def __init__((
        self,
        endpoint: str = "http://localhost:1234/v1",
        models_file: str | None = None,
        verbose: bool = False,
    )):
    """Initialize LMStrix client."""

def list_models((self)) -> list[Model]:
    """List all available models."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a specific model by ID."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs,
    )):
    """Run inference on a model."""

def optimize_context((self, model_id: str)):
    """Find optimal context size for a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__version__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

import os
from typing import Any
import litellm
from litellm import acompletion
from loguru import logger
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential
from lmstrix.api.exceptions import APIConnectionError, InferenceError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Async client for interacting with LM Studio API."""
    def __init__((self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False)):
        """Initialize the LM Studio client."""

def __init__((self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False)):
    """Initialize the LM Studio client."""

def acompletion((
        self,
        model_id: str,
        messages: list[dict[str, str]],
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> CompletionResponse:
    """Make an async completion request to LM Studio."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix import LMStrix
from lmstrix.core import ContextTester, ModelRegistry
from lmstrix.core.scanner import ModelScanner

class LMStrixCLI:
    """LMStrix command-line interface."""
    def __init__((self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False)):
        """Initialize the CLI. ..."""
    def scan((self)):
        """Scan for models in LM Studio and update registry."""
    def list((self)):
        """List all models with their test status."""
    def test((self, model_id: str = None, all: bool = False)):
        """Test context limits for models."""
    def status((self)):
        """Show testing progress and summary."""
    def models((self, action: str = "list", model_id: str = None)):
        """Legacy models command - redirects to new commands."""
    def infer((
        self,
        prompt: str,
        model: str = None,
        max_tokens: int = 2048,
        temperature: float = 0.7,
    )):
        """Run inference on a model."""
    def optimize((self, model: str, all: bool = False)):
        """Legacy optimize command - redirects to test."""

def __init__((self, endpoint: str = "http://localhost:1234/v1", verbose: bool = False)):
    """Initialize the CLI. ..."""

def scan((self)):
    """Scan for models in LM Studio and update registry."""

def list((self)):
    """List all models with their test status."""

def test((self, model_id: str = None, all: bool = False)):
    """Test context limits for models."""

def status((self)):
    """Show testing progress and summary."""

def models((self, action: str = "list", model_id: str = None)):
    """Legacy models command - redirects to new commands."""

def infer((
        self,
        prompt: str,
        model: str = None,
        max_tokens: int = 2048,
        temperature: float = 0.7,
    )):
    """Run inference on a model."""

def optimize((self, model: str, all: bool = False)):
    """Legacy optimize command - redirects to test."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self,
        model: Model,
        context_size: int,
        log_path: Path,
    )) -> ContextTestResult:
        """Test model at specific context size."""
    def find_max_loadable_context((
        self,
        model: Model,
        min_context: int = 32,
        max_context: int | None = None,
    )) -> tuple[int, Path]:
        """Find maximum context size at which model loads."""
    def find_max_working_context((
        self,
        model: Model,
        min_context: int = 32,
        max_context: int | None = None,
    )) -> tuple[int, int, Path]:
        """Find maximum context size that produces correct output."""
    def test_model((
        self,
        model: Model,
        min_context: int = 32,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self,
        model: Model,
        context_size: int,
        log_path: Path,
    )) -> ContextTestResult:
    """Test model at specific context size."""

def find_max_loadable_context((
        self,
        model: Model,
        min_context: int = 32,
        max_context: int | None = None,
    )) -> tuple[int, Path]:
    """Find maximum context size at which model loads."""

def find_max_working_context((
        self,
        model: Model,
        min_context: int = 32,
        max_context: int | None = None,
    )) -> tuple[int, int, Path]:
    """Find maximum context size that produces correct output."""

def test_model((
        self,
        model: Model,
        min_context: int = 32,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import ModelRegistry

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load models from a JSON file into a ModelRegistry."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_models_registry_path,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_models_registry_path(()) -> Path:
    """Get the path to the models registry JSON file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""


</documents>
</document_content>
</document>

<document index="10">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="11">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "lmstrix"
version = "1.0.0"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "LMStrix Contributors"},
]
keywords = ["llm", "lmstudio", "context", "optimization", "testing"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "pydantic>=2.0",
    
    "rich>=13.0",
    "fire>=0.5",
    "toml>=0.10",
    "tenacity>=8.0",
    "loguru>=0.7",
    "httpx>=0.24",
    "tiktoken>=0.5",
    "lmstudio>=0.1",  # For actual LM Studio integration
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-asyncio>=0.21",
    "pytest-cov>=4.0",
    "pytest-mock>=3.0",
    "mypy>=1.0",
    "ruff>=0.1",
    "black>=23.0",
]

docs = [
    "mkdocs>=1.5",
    "mkdocs-material>=9.0",
    "mkdocstrings[python]>=0.22",
]

[project.urls]
"Homepage" = "https://github.com/yourusername/lmstrix"
"Bug Tracker" = "https://github.com/yourusername/lmstrix/issues"
"Documentation" = "https://lmstrix.readthedocs.io"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
lmstrix = ["py.typed"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = "fire.*"
ignore_missing_imports = true



[[tool.mypy.overrides]]
module = "lmstudio.*"
ignore_missing_imports = true

[tool.ruff]
target-version = "py310"
line-length = 100
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.isort]
known-third-party = ["pydantic", "rich", "fire", "loguru", "tenacity"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_functions = ["test_*"]
python_classes = ["Test*"]
asyncio_mode = "auto"
addopts = [
    "--strict-markers",
    "--tb=short",
    "--cov=lmstrix",
    "--cov-report=term-missing",
    "--cov-report=html",
]

[tool.coverage.run]
source = ["src/lmstrix"]
omit = ["*/tests/*", "*/__init__.py"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if __name__ == .__main__.:",
    "raise AssertionError",
    "raise NotImplementedError",
]
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from typing import List
from lmstrix.__version__ import __version__
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__version__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_models_registry_path,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the models registry JSON file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""


</documents>
</document_content>
</document>

<document index="10">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="11">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "lmstrix"
version = "1.0.0"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "LMStrix Contributors"},
]
keywords = ["llm", "lmstudio", "context", "optimization", "testing"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "pydantic>=2.0",
    
    "rich>=13.0",
    "fire>=0.5",
    "toml>=0.10",
    "tenacity>=8.0",
    "loguru>=0.7",
    "httpx>=0.24",
    "tiktoken>=0.5",
    "lmstudio>=0.1",  # For actual LM Studio integration
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-asyncio>=0.21",
    "pytest-cov>=4.0",
    "pytest-mock>=3.0",
    "mypy>=1.0",
    "ruff>=0.1",
    "black>=23.0",
]

docs = [
    "mkdocs>=1.5",
    "mkdocs-material>=9.0",
    "mkdocstrings[python]>=0.22",
]

[project.urls]
"Homepage" = "https://github.com/yourusername/lmstrix"
"Bug Tracker" = "https://github.com/yourusername/lmstrix/issues"
"Documentation" = "https://lmstrix.readthedocs.io"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
lmstrix = ["py.typed"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[[tool.mypy.overrides]]
module = "fire.*"
ignore_missing_imports = true



[[tool.mypy.overrides]]
module = "lmstudio.*"
ignore_missing_imports = true

[tool.ruff]
target-version = "py310"
line-length = 100
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.isort]
known-third-party = ["pydantic", "rich", "fire", "loguru", "tenacity"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_functions = ["test_*"]
python_classes = ["Test*"]
asyncio_mode = "auto"
addopts = [
    "--strict-markers",
    "--tb=short",
    "--cov=lmstrix",
    "--cov-report=term-missing",
    "--cov-report=html",
]

[tool.coverage.run]
source = ["src/lmstrix"]
omit = ["*/tests/*", "*/__init__.py"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if __name__ == .__main__.:",
    "raise AssertionError",
    "raise NotImplementedError",
]
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from typing import List
from lmstrix.__version__ import __version__
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__version__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_models_registry_path,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""


</documents>
</document_content>
</document>

<document index="11">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="12">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling", "hatch-vcs"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
dynamic = ["version"]
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "LMStrix Contributors"},
]
keywords = ["llm", "lmstudio", "context", "optimization", "testing"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

dependencies = [
    "pydantic>=2.0",
    "rich>=13.0",
    "fire>=0.5",
    "toml>=0.10",
    "tenacity>=8.0",
    "loguru>=0.7",
    "httpx>=0.24",
    "tiktoken>=0.5",
    "lmstudio>=0.1",
]

[project.urls]
"Homepage" = "https://github.com/yourusername/lmstrix"
"Bug Tracker" = "https://github.com/yourusername/lmstrix/issues"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = ["/src", "/tests"]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.envs.default]
dependencies = [
  "pytest",
  "pytest-asyncio",
  "pytest-cov",
  "pytest-mock",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
cov = "pytest --cov-report=term-missing --cov-report=html --cov=src/lmstrix {args:tests}"

[tool.hatch.envs.lint]
detached = true
dependencies = ["ruff"]

[tool.hatch.envs.lint.scripts]
fmt = "ruff format ."
check = "ruff check ."
all = [
  "fmt",
  "check",
]

[tool.ruff]
target-version = "py310"
line-length = 100

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by formatter
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import version, PackageNotFoundError
from typing import List
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_models_registry_path,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""


</documents>
</document_content>
</document>

<document index="11">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="12">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling", "hatch-vcs"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
dynamic = ["version"]
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "Adam Twardoch", email = "adam+github@twardoch.com"},
]
keywords = ["llm", "lmstudio", "context", "optimization", "testing", "ai", "developer-tools", "cli"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Operating System :: OS Independent",
    "Typing :: Typed",
]

dependencies = [
    "pydantic>=2.0",
    "rich>=13.0",
    "fire>=0.5",
    "toml>=0.10",
    "tenacity>=8.0",
    "loguru>=0.7",
    "httpx>=0.24",
    "tiktoken>=0.5",
    "lmstudio-api>=0.1.2",
]

[project.urls]
"Homepage" = "https://github.com/yourusername/lmstrix"
"Bug Tracker" = "https://github.com/yourusername/lmstrix/issues"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = ["/src", "/tests"]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.envs.default]
dependencies = [
  "pytest",
  "pytest-asyncio",
  "pytest-cov",
  "pytest-mock",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
cov = "pytest --cov-report=term-missing --cov-report=html --cov=src/lmstrix {args:tests}"

[tool.hatch.envs.lint]
detached = true
dependencies = ["ruff"]

[tool.hatch.envs.lint.scripts]
fmt = "ruff format ."
check = "ruff check ."
all = [
  "fmt",
  "check",
]

[tool.ruff]
target-version = "py310"
line-length = 100

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501",  # line too long, handled by formatter
    "B008",  # do not perform function calls in argument defaults
    "C901",  # too complex
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import version, PackageNotFoundError
from typing import List
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_models_registry_path,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""


</documents>
</document_content>
</document>

<document index="11">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="12">
<source>pyproject.toml</source>
<document_content>
[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = { text = "MIT" }
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.5",
  "httpx>=0.24",
  "lmstudio>=0.1.0",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.0",
  "tenacity>=8.0",
  "tiktoken>=0.5",
  "toml>=0.10",
]
dynamic = ["version"]

[project.urls]
"Bug Tracker" = "https://github.com/yourusername/lmstrix/issues"
"Homepage" = "https://github.com/yourusername/lmstrix"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[build-system]
requires = ["hatch-vcs", "hatchling"]
build-backend = "hatchling.build"

[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = ["/src", "/tests"]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.envs.default]
dependencies = [
  "pytest",
  "pytest-asyncio",
  "pytest-cov",
  "pytest-mock",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
cov = "pytest --cov-report=term-missing --cov-report=html --cov=src/lmstrix {args:tests}"

[tool.hatch.envs.lint]
detached = true
dependencies = ["ruff"]

[tool.hatch.envs.lint.scripts]
fmt = "ruff format ."
check = "ruff check ."
all = [
  "fmt",
  "check",
]

[tool.ruff]
target-version = "py310"
line-length = 100

[tool.ruff.lint]
select = [
  "E",  # pycodestyle errors
  "W",  # pycodestyle warnings
  "F",  # pyflakes
  "I",  # isort
  "B",  # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
]
ignore = [
  "E501",  # line too long, handled by formatter
  "B008",  # do not perform function calls in argument defaults
  "C901",  # too complex
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

</document_content>
</document>

<document index="13">
<source>pytest.ini</source>
<document_content>
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import version, PackageNotFoundError
from typing import List
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

from lmstrix.cli.main import main


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_default_models_file as get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_lmstudio_path,
    get_lmstrix_data_dir,
    get_default_models_file,
    get_context_tests_dir,
    get_context_test_log_path,
    get_prompts_dir,
    get_contexts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()):
    """Mock LMStudioClient for testing."""

def mock_llm(()):
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()):
    """Sample model data for testing."""

def tmp_models_dir((tmp_path)):
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path)):
    """Create a temporary registry file path."""

def event_loop(()):
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()):
    """Mock completion response from LM Studio."""

def mock_prompt_template(()):
    """Sample prompt template for testing."""

def mock_context_data(()):
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys

def run_tests(()):
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self)):
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self)):
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self)):
        """Test client initialization with different verbose settings."""

def test_completion_response_creation((self)):
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self)):
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self)):
    """Test client initialization with different verbose settings."""

def test_list_models_success((self, mock_lmstudio)):
    """Test successful list_models call."""

def test_list_models_failure((self, mock_lmstudio)):
    """Test list_models with connection error."""

def test_load_model_success((self, mock_lmstudio)):
    """Test successful model loading."""

def test_load_model_failure((self, mock_lmstudio)):
    """Test model loading failure."""

def test_acompletion_success((self, mock_llm, mock_completion_response)):
    """Test successful async completion."""

def test_acompletion_failure((self, mock_llm)):
    """Test async completion failure."""

def test_acompletion_with_defaults((self, mock_llm, mock_completion_response)):
    """Test async completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

import pytest
from lmstrix.api.exceptions import (
    APIConnectionError,
    APIError,
    InferenceError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)):
        """Test base APIError class."""
    def test_api_connection_error((self)):
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)):
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)):
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)):
        """Test that all exceptions inherit from APIError."""

def test_api_error_base((self)):
    """Test base APIError class."""

def test_api_connection_error((self)):
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)):
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)):
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)):
    """Test that all exceptions inherit from APIError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.context_tester import ContextTestResult, ContextTester
from lmstrix.core.models import ContextTestStatus, Model

class TestContextTestResult:
    """Test ContextTestResult class."""
    def test_result_creation_minimal((self)):
        """Test creating a minimal test result."""
    def test_result_creation_full((self)):
        """Test creating a full test result."""
    def test_result_with_error((self)):
        """Test result with error."""
    def test_result_to_dict((self)):
        """Test converting result to dictionary."""
    def test_is_valid_response((self)):
        """Test response validation."""

class TestContextTester:
    """Test ContextTester class."""
    def test_tester_initialization((self, mock_lmstudio_client)):
        """Test context tester initialization."""
    def test_tester_default_client((self)):
        """Test tester creates default client if none provided."""
    def test_generate_test_prompt((self)):
        """Test test prompt generation."""
    def test_estimate_tokens((self)):
        """Test token estimation."""

def test_result_creation_minimal((self)):
    """Test creating a minimal test result."""

def test_result_creation_full((self)):
    """Test creating a full test result."""

def test_result_with_error((self)):
    """Test result with error."""

def test_result_to_dict((self)):
    """Test converting result to dictionary."""

def test_is_valid_response((self)):
    """Test response validation."""

def test_tester_initialization((self, mock_lmstudio_client)):
    """Test context tester initialization."""

def test_tester_default_client((self)):
    """Test tester creates default client if none provided."""

def test_generate_test_prompt((self)):
    """Test test prompt generation."""

def test_estimate_tokens((self)):
    """Test token estimation."""

def test_test_context_load_failure((self, mock_lmstudio_client)):
    """Test context testing when model fails to load."""

def test_test_context_inference_failure((self, mock_lmstudio_client, mock_llm)):
    """Test context testing when inference fails."""

def test_test_context_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful context testing."""

def test_test_context_invalid_response((self, mock_lmstudio_client, mock_llm)):
    """Test context testing with invalid response."""

def test_find_optimal_context_simple((self, mock_lmstudio_client, mock_llm)):
    """Test finding optimal context with simple scenario."""

def mock_completion((llm, prompt, **kwargs)):

def test_save_test_log((self, tmp_path)):
    """Test saving test log to file."""

def test_optimize_model_integration((self, mock_lmstudio_client, mock_llm, tmp_path)):
    """Test full model optimization workflow."""

def mock_completion((llm, prompt, **kwargs)):

def test_optimize_model_failure((self, mock_lmstudio_client)):
    """Test model optimization when all tests fail."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

import time
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelNotFoundError
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model

class TestInferenceResult:
    """Test InferenceResult model."""
    def test_inference_result_success((self)):
        """Test successful inference result."""
    def test_inference_result_failure((self)):
        """Test failed inference result."""
    def test_inference_result_empty_response((self)):
        """Test result with empty response is considered failed."""

class TestInferenceEngine:
    """Test InferenceEngine class."""
    def test_engine_initialization_defaults((self)):
        """Test engine initialization with defaults."""
    def test_engine_initialization_custom((self, mock_lmstudio_client)):
        """Test engine initialization with custom client and registry."""

def test_inference_result_success((self)):
    """Test successful inference result."""

def test_inference_result_failure((self)):
    """Test failed inference result."""

def test_inference_result_empty_response((self)):
    """Test result with empty response is considered failed."""

def test_engine_initialization_defaults((self)):
    """Test engine initialization with defaults."""

def test_engine_initialization_custom((self, mock_lmstudio_client)):
    """Test engine initialization with custom client and registry."""

def test_infer_model_not_found((self)):
    """Test inference with non-existent model."""

def test_infer_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful inference."""

def test_infer_with_untested_model((self, mock_lmstudio_client, mock_llm)):
    """Test inference with model that hasn't been context tested."""

def test_infer_with_max_tokens((self, mock_lmstudio_client, mock_llm)):
    """Test inference with custom max_tokens."""

def test_infer_load_failure((self, mock_lmstudio_client)):
    """Test inference when model fails to load."""

def test_infer_completion_failure((self, mock_lmstudio_client, mock_llm)):
    """Test inference when completion fails."""

def test_run_inference_simple((self, mock_lmstudio_client, mock_llm)):
    """Test simple run_inference method."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
import pytest
from pydantic import ValidationError
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self)):
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self, sample_model_data)):
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self)):
        """Test model creation using field aliases."""
    def test_model_with_context_testing((self, sample_model_data)):
        """Test model with context testing information."""
    def test_model_path_validation((self)):
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self)):
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self, sample_model_data)):
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self)):
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self, tmp_registry_file)):
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
        """Test saving and loading models."""
    def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
        """Test getting a model by ID."""
    def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
        """Test listing all models."""
    def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
        """Test removing a model."""
    def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
        """Test saving/loading models with context test information."""
    def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
        """Test that the saved JSON has the expected format."""

def test_enum_values((self)):
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self, sample_model_data)):
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self)):
    """Test model creation using field aliases."""

def test_model_with_context_testing((self, sample_model_data)):
    """Test model with context testing information."""

def test_model_path_validation((self)):
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self)):
    """Test sanitized_id method."""

def test_model_to_registry_dict((self, sample_model_data)):
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self)):
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self, tmp_registry_file)):
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
    """Test saving and loading models."""

def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
    """Test getting a model by ID."""

def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
    """Test listing all models."""

def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
    """Test removing a model."""

def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
    """Test saving/loading models with context test information."""

def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

import pytest
from pydantic import ValidationError
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)):
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)):
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)):
        """Test resolver initialization."""
    def test_find_placeholders((self)):
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)):
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)):
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)):
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)):
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)):
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)):
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)):
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)):
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)):
        """Test resolution with empty string values."""
    def test_count_tokens((self)):
        """Test token counting."""
    def test_resolve_with_special_characters((self)):
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)):
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)):
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)):
    """Test resolver initialization."""

def test_find_placeholders((self)):
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)):
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)):
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)):
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)):
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)):
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)):
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)):
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)):
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)):
    """Test resolution with empty string values."""

def test_count_tokens((self)):
    """Test token counting."""

def test_resolve_with_special_characters((self)):
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self, tmp_path)):
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self, tmp_path)):
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self, tmp_path)):
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self, tmp_path)):
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self, tmp_path)):
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self, tmp_path)):
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self, tmp_path)):
        """Test that non-model files are skipped."""

def test_scanner_initialization((self, mock_get_path, tmp_path)):
    """Test scanner initialization."""

def test_get_model_size_file((self, tmp_path)):
    """Test getting size of a single model file."""

def test_get_model_size_directory((self, tmp_path)):
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self, tmp_path)):
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self, tmp_path)):
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self, tmp_path)):
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self, tmp_path)):
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self, tmp_path)):
    """Test that non-model files are skipped."""

def test_scan_models((self, mock_get_path, tmp_path)):
    """Test scanning for models."""

def test_sync_with_registry((self, mock_get_path, tmp_path)):
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_e2e/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.cli.main import create_app
from lmstrix.core.models import Model
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
import asyncio
from lmstrix.cli.main import CLI
from lmstrix.core.prompts import ResolvedPrompt
from lmstrix.core.inference import InferenceResult
from lmstrix.cli.main import CLI
from lmstrix.cli.main import main

class TestCLIIntegration:
    """Test CLI integration scenarios."""
    def test_cli_help((self, capsys)):
        """Test CLI help output."""

def mock_lmstudio_setup((self, tmp_path)):
    """Set up mock LM Studio environment."""

def test_list_models_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'models list' command."""

def test_scan_models_command((self, mock_scanner_class, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'models scan' command."""

def test_optimize_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'optimize' command."""

def test_infer_command_missing_prompt((self, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'infer' command with missing prompt."""

def test_infer_with_prompt_file((self, mock_engine_class, mock_load_prompts, mock_get_path, mock_lmstudio_setup, tmp_path)):
    """Test 'infer' command with prompt file."""

def test_cli_help((self, capsys)):
    """Test CLI help output."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import (
    load_context,
    load_context_batch,
    merge_contexts,
)

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path)):
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path)):
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path)):
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path)):
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path)):
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path)):
        """Test loading large context file."""
    def test_load_context_batch_single((self, tmp_path)):
        """Test loading batch with single file."""
    def test_load_context_batch_multiple((self, tmp_path)):
        """Test loading batch with multiple files."""
    def test_load_context_batch_with_errors((self, tmp_path)):
        """Test batch loading continues on error."""
    def test_load_context_batch_empty((self)):
        """Test loading empty batch."""
    def test_merge_contexts_simple((self)):
        """Test merging simple contexts."""
    def test_merge_contexts_with_separator((self)):
        """Test merging with custom separator."""
    def test_merge_contexts_single((self)):
        """Test merging single context."""
    def test_merge_contexts_empty((self)):
        """Test merging empty contexts."""
    def test_merge_contexts_with_headers((self)):
        """Test that merge includes file headers."""

def test_load_context_simple((self, tmp_path)):
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path)):
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path)):
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path)):
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path)):
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path)):
    """Test loading large context file."""

def test_load_context_batch_single((self, tmp_path)):
    """Test loading batch with single file."""

def test_load_context_batch_multiple((self, tmp_path)):
    """Test loading batch with multiple files."""

def test_load_context_batch_with_errors((self, tmp_path)):
    """Test batch loading continues on error."""

def test_load_context_batch_empty((self)):
    """Test loading empty batch."""

def test_merge_contexts_simple((self)):
    """Test merging simple contexts."""

def test_merge_contexts_with_separator((self)):
    """Test merging with custom separator."""

def test_merge_contexts_single((self)):
    """Test merging single context."""

def test_merge_contexts_empty((self)):
    """Test merging empty contexts."""

def test_merge_contexts_with_headers((self)):
    """Test that merge includes file headers."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry, scan_and_update_models

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self, tmp_path)):
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self, tmp_path)):
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self, tmp_path)):
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self, tmp_path)):
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self, tmp_path)):
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self, tmp_path)):
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self, tmp_path)):
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self, tmp_path)):
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self, tmp_path)):
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self, tmp_path)):
    """Test saving registry to custom path."""

def test_scan_and_update_models((self, mock_scanner_class, mock_client_class, tmp_path)):
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((self, mock_scanner_class, mock_client_class)):
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompt_file, load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self, tmp_path)):
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self, tmp_path)):
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self, tmp_path)):
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((self, tmp_path)):
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self, tmp_path)):
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self, tmp_path)):
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self, tmp_path)):
        """Test loading prompts from empty TOML file."""
    def test_load_prompt_file_simple((self, tmp_path)):
        """Test loading a single prompt file."""
    def test_load_prompt_file_missing_template((self, tmp_path)):
        """Test loading prompt file without template field."""
    def test_load_prompt_file_with_defaults((self, tmp_path)):
        """Test loading prompt file with default values."""

def test_load_prompts_simple((self, tmp_path)):
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self, tmp_path)):
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self, tmp_path)):
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((self, tmp_path)):
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self, tmp_path)):
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self, tmp_path)):
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self, tmp_path)):
    """Test loading prompts from empty TOML file."""

def test_load_prompt_file_simple((self, tmp_path)):
    """Test loading a single prompt file."""

def test_load_prompt_file_missing_template((self, tmp_path)):
    """Test loading prompt file without template field."""

def test_load_prompt_file_with_defaults((self, tmp_path)):
    """Test loading prompt file with default values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self, tmp_path)):
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self, tmp_path)):
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self, tmp_path)):
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self, tmp_path)):
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self, tmp_path)):
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self, tmp_path)):
        """Test getting default models file path."""
    def test_get_context_tests_dir((self, tmp_path)):
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self, tmp_path)):
        """Test getting context test log path."""
    def test_get_prompts_dir((self, tmp_path)):
        """Test getting prompts directory."""
    def test_get_contexts_dir((self, tmp_path)):
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((self, tmp_path)):
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self, tmp_path)):
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self, tmp_path)):
    """Test finding LM Studio in shared location."""

def exists_side_effect((self)):

def test_get_lmstudio_path_not_found((self, tmp_path)):
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self, tmp_path)):
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self, tmp_path)):
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self, tmp_path)):
    """Test getting default models file path."""

def test_get_context_tests_dir((self, tmp_path)):
    """Test getting context tests directory."""

def test_get_context_test_log_path((self, tmp_path)):
    """Test getting context test log path."""

def test_get_prompts_dir((self, tmp_path)):
    """Test getting prompts directory."""

def test_get_contexts_dir((self, tmp_path)):
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((self, tmp_path)):
    """Test handling permission errors when creating directories."""


</documents>
</document_content>
</document>

<document index="12">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="13">
<source>pyproject.toml</source>
<document_content>
[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.11"
license = { text = "MIT" }
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.5",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml>=0.10",
]
dynamic = ["version"]

[project.urls]
"Bug Tracker" = "https://github.com/yourusername/lmstrix/issues"
"Homepage" = "https://github.com/yourusername/lmstrix"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[build-system]
requires = ["hatch-vcs", "hatchling"]
build-backend = "hatchling.build"

[tool.hatch.version]
source = "vcs"

[tool.hatch.build.targets.sdist]

[tool.hatch.build.targets.wheel]

[tool.hatch.envs.default]
dependencies = [
  "pytest",
  "pytest-asyncio",
  "pytest-cov",
  "pytest-mock",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
cov = "pytest --cov-report=term-missing --cov-report=html --cov=src/lmstrix {args:tests}"

[tool.hatch.envs.lint]
detached = true
dependencies = ["ruff"]

[tool.hatch.envs.lint.scripts]
fmt = "ruff format ."
check = "ruff check ."
all = [
  "fmt",
  "check",
]

[tool.ruff]
target-version = "py310"
line-length = 100

[tool.ruff.lint]
select = [
  "E",  # pycodestyle errors
  "W",  # pycodestyle warnings
  "F",  # pyflakes
  "I",  # isort
  "B",  # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
]
ignore = [
  "E501",  # line too long, handled by formatter
  "B008",  # do not perform function calls in argument defaults
  "C901",  # too complex
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

</document_content>
</document>

<document index="14">
<source>pytest.ini</source>
<document_content>
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import version, PackageNotFoundError
from typing import List
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

from lmstrix.cli.main import main


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_default_models_file as get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_lmstudio_path,
    get_lmstrix_data_dir,
    get_default_models_file,
    get_context_tests_dir,
    get_context_test_log_path,
    get_prompts_dir,
    get_contexts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()):
    """Mock LMStudioClient for testing."""

def mock_llm(()):
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()):
    """Sample model data for testing."""

def tmp_models_dir((tmp_path)):
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path)):
    """Create a temporary registry file path."""

def event_loop(()):
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()):
    """Mock completion response from LM Studio."""

def mock_prompt_template(()):
    """Sample prompt template for testing."""

def mock_context_data(()):
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys

def run_tests(()):
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self)):
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self)):
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self)):
        """Test client initialization with different verbose settings."""

def test_completion_response_creation((self)):
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self)):
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self)):
    """Test client initialization with different verbose settings."""

def test_list_models_success((self, mock_lmstudio)):
    """Test successful list_models call."""

def test_list_models_failure((self, mock_lmstudio)):
    """Test list_models with connection error."""

def test_load_model_success((self, mock_lmstudio)):
    """Test successful model loading."""

def test_load_model_failure((self, mock_lmstudio)):
    """Test model loading failure."""

def test_acompletion_success((self, mock_llm, mock_completion_response)):
    """Test successful async completion."""

def test_acompletion_failure((self, mock_llm)):
    """Test async completion failure."""

def test_acompletion_with_defaults((self, mock_llm, mock_completion_response)):
    """Test async completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

import pytest
from lmstrix.api.exceptions import (
    APIConnectionError,
    APIError,
    InferenceError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)):
        """Test base APIError class."""
    def test_api_connection_error((self)):
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)):
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)):
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)):
        """Test that all exceptions inherit from APIError."""

def test_api_error_base((self)):
    """Test base APIError class."""

def test_api_connection_error((self)):
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)):
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)):
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)):
    """Test that all exceptions inherit from APIError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.context_tester import ContextTestResult, ContextTester
from lmstrix.core.models import ContextTestStatus, Model

class TestContextTestResult:
    """Test ContextTestResult class."""
    def test_result_creation_minimal((self)):
        """Test creating a minimal test result."""
    def test_result_creation_full((self)):
        """Test creating a full test result."""
    def test_result_with_error((self)):
        """Test result with error."""
    def test_result_to_dict((self)):
        """Test converting result to dictionary."""
    def test_is_valid_response((self)):
        """Test response validation."""

class TestContextTester:
    """Test ContextTester class."""
    def test_tester_initialization((self, mock_lmstudio_client)):
        """Test context tester initialization."""
    def test_tester_default_client((self)):
        """Test tester creates default client if none provided."""
    def test_generate_test_prompt((self)):
        """Test test prompt generation."""
    def test_estimate_tokens((self)):
        """Test token estimation."""

def test_result_creation_minimal((self)):
    """Test creating a minimal test result."""

def test_result_creation_full((self)):
    """Test creating a full test result."""

def test_result_with_error((self)):
    """Test result with error."""

def test_result_to_dict((self)):
    """Test converting result to dictionary."""

def test_is_valid_response((self)):
    """Test response validation."""

def test_tester_initialization((self, mock_lmstudio_client)):
    """Test context tester initialization."""

def test_tester_default_client((self)):
    """Test tester creates default client if none provided."""

def test_generate_test_prompt((self)):
    """Test test prompt generation."""

def test_estimate_tokens((self)):
    """Test token estimation."""

def test_test_context_load_failure((self, mock_lmstudio_client)):
    """Test context testing when model fails to load."""

def test_test_context_inference_failure((self, mock_lmstudio_client, mock_llm)):
    """Test context testing when inference fails."""

def test_test_context_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful context testing."""

def test_test_context_invalid_response((self, mock_lmstudio_client, mock_llm)):
    """Test context testing with invalid response."""

def test_find_optimal_context_simple((self, mock_lmstudio_client, mock_llm)):
    """Test finding optimal context with simple scenario."""

def mock_completion((llm, prompt, **kwargs)):

def test_save_test_log((self, tmp_path)):
    """Test saving test log to file."""

def test_optimize_model_integration((self, mock_lmstudio_client, mock_llm, tmp_path)):
    """Test full model optimization workflow."""

def mock_completion((llm, prompt, **kwargs)):

def test_optimize_model_failure((self, mock_lmstudio_client)):
    """Test model optimization when all tests fail."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

import time
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelNotFoundError
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model

class TestInferenceResult:
    """Test InferenceResult model."""
    def test_inference_result_success((self)):
        """Test successful inference result."""
    def test_inference_result_failure((self)):
        """Test failed inference result."""
    def test_inference_result_empty_response((self)):
        """Test result with empty response is considered failed."""

class TestInferenceEngine:
    """Test InferenceEngine class."""
    def test_engine_initialization_defaults((self)):
        """Test engine initialization with defaults."""
    def test_engine_initialization_custom((self, mock_lmstudio_client)):
        """Test engine initialization with custom client and registry."""

def test_inference_result_success((self)):
    """Test successful inference result."""

def test_inference_result_failure((self)):
    """Test failed inference result."""

def test_inference_result_empty_response((self)):
    """Test result with empty response is considered failed."""

def test_engine_initialization_defaults((self)):
    """Test engine initialization with defaults."""

def test_engine_initialization_custom((self, mock_lmstudio_client)):
    """Test engine initialization with custom client and registry."""

def test_infer_model_not_found((self)):
    """Test inference with non-existent model."""

def test_infer_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful inference."""

def test_infer_with_untested_model((self, mock_lmstudio_client, mock_llm)):
    """Test inference with model that hasn't been context tested."""

def test_infer_with_max_tokens((self, mock_lmstudio_client, mock_llm)):
    """Test inference with custom max_tokens."""

def test_infer_load_failure((self, mock_lmstudio_client)):
    """Test inference when model fails to load."""

def test_infer_completion_failure((self, mock_lmstudio_client, mock_llm)):
    """Test inference when completion fails."""

def test_run_inference_simple((self, mock_lmstudio_client, mock_llm)):
    """Test simple run_inference method."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
import pytest
from pydantic import ValidationError
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self)):
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self, sample_model_data)):
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self)):
        """Test model creation using field aliases."""
    def test_model_with_context_testing((self, sample_model_data)):
        """Test model with context testing information."""
    def test_model_path_validation((self)):
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self)):
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self, sample_model_data)):
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self)):
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self, tmp_registry_file)):
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
        """Test saving and loading models."""
    def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
        """Test getting a model by ID."""
    def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
        """Test listing all models."""
    def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
        """Test removing a model."""
    def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
        """Test saving/loading models with context test information."""
    def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
        """Test that the saved JSON has the expected format."""

def test_enum_values((self)):
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self, sample_model_data)):
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self)):
    """Test model creation using field aliases."""

def test_model_with_context_testing((self, sample_model_data)):
    """Test model with context testing information."""

def test_model_path_validation((self)):
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self)):
    """Test sanitized_id method."""

def test_model_to_registry_dict((self, sample_model_data)):
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self)):
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self, tmp_registry_file)):
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
    """Test saving and loading models."""

def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
    """Test getting a model by ID."""

def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
    """Test listing all models."""

def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
    """Test removing a model."""

def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
    """Test saving/loading models with context test information."""

def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

import pytest
from pydantic import ValidationError
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)):
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)):
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)):
        """Test resolver initialization."""
    def test_find_placeholders((self)):
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)):
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)):
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)):
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)):
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)):
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)):
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)):
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)):
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)):
        """Test resolution with empty string values."""
    def test_count_tokens((self)):
        """Test token counting."""
    def test_resolve_with_special_characters((self)):
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)):
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)):
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)):
    """Test resolver initialization."""

def test_find_placeholders((self)):
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)):
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)):
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)):
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)):
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)):
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)):
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)):
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)):
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)):
    """Test resolution with empty string values."""

def test_count_tokens((self)):
    """Test token counting."""

def test_resolve_with_special_characters((self)):
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self, tmp_path)):
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self, tmp_path)):
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self, tmp_path)):
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self, tmp_path)):
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self, tmp_path)):
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self, tmp_path)):
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self, tmp_path)):
        """Test that non-model files are skipped."""

def test_scanner_initialization((self, mock_get_path, tmp_path)):
    """Test scanner initialization."""

def test_get_model_size_file((self, tmp_path)):
    """Test getting size of a single model file."""

def test_get_model_size_directory((self, tmp_path)):
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self, tmp_path)):
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self, tmp_path)):
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self, tmp_path)):
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self, tmp_path)):
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self, tmp_path)):
    """Test that non-model files are skipped."""

def test_scan_models((self, mock_get_path, tmp_path)):
    """Test scanning for models."""

def test_sync_with_registry((self, mock_get_path, tmp_path)):
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_e2e/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.cli.main import create_app
from lmstrix.core.models import Model
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
import asyncio
from lmstrix.cli.main import CLI
from lmstrix.core.prompts import ResolvedPrompt
from lmstrix.core.inference import InferenceResult
from lmstrix.cli.main import CLI
from lmstrix.cli.main import main

class TestCLIIntegration:
    """Test CLI integration scenarios."""
    def test_cli_help((self, capsys)):
        """Test CLI help output."""

def mock_lmstudio_setup((self, tmp_path)):
    """Set up mock LM Studio environment."""

def test_list_models_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'models list' command."""

def test_scan_models_command((self, mock_scanner_class, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'models scan' command."""

def test_optimize_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'optimize' command."""

def test_infer_command_missing_prompt((self, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'infer' command with missing prompt."""

def test_infer_with_prompt_file((self, mock_engine_class, mock_load_prompts, mock_get_path, mock_lmstudio_setup, tmp_path)):
    """Test 'infer' command with prompt file."""

def test_cli_help((self, capsys)):
    """Test CLI help output."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import (
    load_context,
    load_context_batch,
    merge_contexts,
)

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path)):
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path)):
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path)):
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path)):
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path)):
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path)):
        """Test loading large context file."""
    def test_load_context_batch_single((self, tmp_path)):
        """Test loading batch with single file."""
    def test_load_context_batch_multiple((self, tmp_path)):
        """Test loading batch with multiple files."""
    def test_load_context_batch_with_errors((self, tmp_path)):
        """Test batch loading continues on error."""
    def test_load_context_batch_empty((self)):
        """Test loading empty batch."""
    def test_merge_contexts_simple((self)):
        """Test merging simple contexts."""
    def test_merge_contexts_with_separator((self)):
        """Test merging with custom separator."""
    def test_merge_contexts_single((self)):
        """Test merging single context."""
    def test_merge_contexts_empty((self)):
        """Test merging empty contexts."""
    def test_merge_contexts_with_headers((self)):
        """Test that merge includes file headers."""

def test_load_context_simple((self, tmp_path)):
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path)):
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path)):
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path)):
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path)):
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path)):
    """Test loading large context file."""

def test_load_context_batch_single((self, tmp_path)):
    """Test loading batch with single file."""

def test_load_context_batch_multiple((self, tmp_path)):
    """Test loading batch with multiple files."""

def test_load_context_batch_with_errors((self, tmp_path)):
    """Test batch loading continues on error."""

def test_load_context_batch_empty((self)):
    """Test loading empty batch."""

def test_merge_contexts_simple((self)):
    """Test merging simple contexts."""

def test_merge_contexts_with_separator((self)):
    """Test merging with custom separator."""

def test_merge_contexts_single((self)):
    """Test merging single context."""

def test_merge_contexts_empty((self)):
    """Test merging empty contexts."""

def test_merge_contexts_with_headers((self)):
    """Test that merge includes file headers."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry, scan_and_update_models

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self, tmp_path)):
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self, tmp_path)):
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self, tmp_path)):
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self, tmp_path)):
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self, tmp_path)):
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self, tmp_path)):
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self, tmp_path)):
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self, tmp_path)):
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self, tmp_path)):
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self, tmp_path)):
    """Test saving registry to custom path."""

def test_scan_and_update_models((self, mock_scanner_class, mock_client_class, tmp_path)):
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((self, mock_scanner_class, mock_client_class)):
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompt_file, load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self, tmp_path)):
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self, tmp_path)):
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self, tmp_path)):
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((self, tmp_path)):
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self, tmp_path)):
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self, tmp_path)):
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self, tmp_path)):
        """Test loading prompts from empty TOML file."""
    def test_load_prompt_file_simple((self, tmp_path)):
        """Test loading a single prompt file."""
    def test_load_prompt_file_missing_template((self, tmp_path)):
        """Test loading prompt file without template field."""
    def test_load_prompt_file_with_defaults((self, tmp_path)):
        """Test loading prompt file with default values."""

def test_load_prompts_simple((self, tmp_path)):
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self, tmp_path)):
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self, tmp_path)):
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((self, tmp_path)):
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self, tmp_path)):
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self, tmp_path)):
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self, tmp_path)):
    """Test loading prompts from empty TOML file."""

def test_load_prompt_file_simple((self, tmp_path)):
    """Test loading a single prompt file."""

def test_load_prompt_file_missing_template((self, tmp_path)):
    """Test loading prompt file without template field."""

def test_load_prompt_file_with_defaults((self, tmp_path)):
    """Test loading prompt file with default values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self, tmp_path)):
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self, tmp_path)):
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self, tmp_path)):
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self, tmp_path)):
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self, tmp_path)):
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self, tmp_path)):
        """Test getting default models file path."""
    def test_get_context_tests_dir((self, tmp_path)):
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self, tmp_path)):
        """Test getting context test log path."""
    def test_get_prompts_dir((self, tmp_path)):
        """Test getting prompts directory."""
    def test_get_contexts_dir((self, tmp_path)):
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((self, tmp_path)):
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self, tmp_path)):
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self, tmp_path)):
    """Test finding LM Studio in shared location."""

def exists_side_effect((self)):

def test_get_lmstudio_path_not_found((self, tmp_path)):
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self, tmp_path)):
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self, tmp_path)):
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self, tmp_path)):
    """Test getting default models file path."""

def test_get_context_tests_dir((self, tmp_path)):
    """Test getting context tests directory."""

def test_get_context_test_log_path((self, tmp_path)):
    """Test getting context test log path."""

def test_get_prompts_dir((self, tmp_path)):
    """Test getting prompts directory."""

def test_get_contexts_dir((self, tmp_path)):
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((self, tmp_path)):
    """Test handling permission errors when creating directories."""


</documents>
</document_content>
</document>

<document index="13">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="14">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.5",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml>=0.10",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN101",  # missing type annotation for self
  "ANN102",  # missing type annotation for cls
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]

[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

<document index="15">
<source>pytest.ini</source>
<document_content>
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import version, PackageNotFoundError
from typing import List
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

from lmstrix.cli.main import main


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_default_models_file as get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_lmstudio_path,
    get_lmstrix_data_dir,
    get_default_models_file,
    get_context_tests_dir,
    get_context_test_log_path,
    get_prompts_dir,
    get_contexts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()):
    """Mock LMStudioClient for testing."""

def mock_llm(()):
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()):
    """Sample model data for testing."""

def tmp_models_dir((tmp_path)):
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path)):
    """Create a temporary registry file path."""

def event_loop(()):
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()):
    """Mock completion response from LM Studio."""

def mock_prompt_template(()):
    """Sample prompt template for testing."""

def mock_context_data(()):
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys

def run_tests(()):
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self)):
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self)):
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self)):
        """Test client initialization with different verbose settings."""

def test_completion_response_creation((self)):
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self)):
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self)):
    """Test client initialization with different verbose settings."""

def test_list_models_success((self, mock_lmstudio)):
    """Test successful list_models call."""

def test_list_models_failure((self, mock_lmstudio)):
    """Test list_models with connection error."""

def test_load_model_success((self, mock_lmstudio)):
    """Test successful model loading."""

def test_load_model_failure((self, mock_lmstudio)):
    """Test model loading failure."""

def test_acompletion_success((self, mock_llm, mock_completion_response)):
    """Test successful async completion."""

def test_acompletion_failure((self, mock_llm)):
    """Test async completion failure."""

def test_acompletion_with_defaults((self, mock_llm, mock_completion_response)):
    """Test async completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

import pytest
from lmstrix.api.exceptions import (
    APIConnectionError,
    APIError,
    InferenceError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)):
        """Test base APIError class."""
    def test_api_connection_error((self)):
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)):
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)):
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)):
        """Test that all exceptions inherit from APIError."""

def test_api_error_base((self)):
    """Test base APIError class."""

def test_api_connection_error((self)):
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)):
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)):
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)):
    """Test that all exceptions inherit from APIError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.context_tester import ContextTestResult, ContextTester
from lmstrix.core.models import ContextTestStatus, Model

class TestContextTestResult:
    """Test ContextTestResult class."""
    def test_result_creation_minimal((self)):
        """Test creating a minimal test result."""
    def test_result_creation_full((self)):
        """Test creating a full test result."""
    def test_result_with_error((self)):
        """Test result with error."""
    def test_result_to_dict((self)):
        """Test converting result to dictionary."""
    def test_is_valid_response((self)):
        """Test response validation."""

class TestContextTester:
    """Test ContextTester class."""
    def test_tester_initialization((self, mock_lmstudio_client)):
        """Test context tester initialization."""
    def test_tester_default_client((self)):
        """Test tester creates default client if none provided."""
    def test_generate_test_prompt((self)):
        """Test test prompt generation."""
    def test_estimate_tokens((self)):
        """Test token estimation."""

def test_result_creation_minimal((self)):
    """Test creating a minimal test result."""

def test_result_creation_full((self)):
    """Test creating a full test result."""

def test_result_with_error((self)):
    """Test result with error."""

def test_result_to_dict((self)):
    """Test converting result to dictionary."""

def test_is_valid_response((self)):
    """Test response validation."""

def test_tester_initialization((self, mock_lmstudio_client)):
    """Test context tester initialization."""

def test_tester_default_client((self)):
    """Test tester creates default client if none provided."""

def test_generate_test_prompt((self)):
    """Test test prompt generation."""

def test_estimate_tokens((self)):
    """Test token estimation."""

def test_test_context_load_failure((self, mock_lmstudio_client)):
    """Test context testing when model fails to load."""

def test_test_context_inference_failure((self, mock_lmstudio_client, mock_llm)):
    """Test context testing when inference fails."""

def test_test_context_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful context testing."""

def test_test_context_invalid_response((self, mock_lmstudio_client, mock_llm)):
    """Test context testing with invalid response."""

def test_find_optimal_context_simple((self, mock_lmstudio_client, mock_llm)):
    """Test finding optimal context with simple scenario."""

def mock_completion((llm, prompt, **kwargs)):

def test_save_test_log((self, tmp_path)):
    """Test saving test log to file."""

def test_optimize_model_integration((self, mock_lmstudio_client, mock_llm, tmp_path)):
    """Test full model optimization workflow."""

def mock_completion((llm, prompt, **kwargs)):

def test_optimize_model_failure((self, mock_lmstudio_client)):
    """Test model optimization when all tests fail."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

import time
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelNotFoundError
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model

class TestInferenceResult:
    """Test InferenceResult model."""
    def test_inference_result_success((self)):
        """Test successful inference result."""
    def test_inference_result_failure((self)):
        """Test failed inference result."""
    def test_inference_result_empty_response((self)):
        """Test result with empty response is considered failed."""

class TestInferenceEngine:
    """Test InferenceEngine class."""
    def test_engine_initialization_defaults((self)):
        """Test engine initialization with defaults."""
    def test_engine_initialization_custom((self, mock_lmstudio_client)):
        """Test engine initialization with custom client and registry."""

def test_inference_result_success((self)):
    """Test successful inference result."""

def test_inference_result_failure((self)):
    """Test failed inference result."""

def test_inference_result_empty_response((self)):
    """Test result with empty response is considered failed."""

def test_engine_initialization_defaults((self)):
    """Test engine initialization with defaults."""

def test_engine_initialization_custom((self, mock_lmstudio_client)):
    """Test engine initialization with custom client and registry."""

def test_infer_model_not_found((self)):
    """Test inference with non-existent model."""

def test_infer_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful inference."""

def test_infer_with_untested_model((self, mock_lmstudio_client, mock_llm)):
    """Test inference with model that hasn't been context tested."""

def test_infer_with_max_tokens((self, mock_lmstudio_client, mock_llm)):
    """Test inference with custom max_tokens."""

def test_infer_load_failure((self, mock_lmstudio_client)):
    """Test inference when model fails to load."""

def test_infer_completion_failure((self, mock_lmstudio_client, mock_llm)):
    """Test inference when completion fails."""

def test_run_inference_simple((self, mock_lmstudio_client, mock_llm)):
    """Test simple run_inference method."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
import pytest
from pydantic import ValidationError
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self)):
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self, sample_model_data)):
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self)):
        """Test model creation using field aliases."""
    def test_model_with_context_testing((self, sample_model_data)):
        """Test model with context testing information."""
    def test_model_path_validation((self)):
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self)):
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self, sample_model_data)):
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self)):
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self, tmp_registry_file)):
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
        """Test saving and loading models."""
    def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
        """Test getting a model by ID."""
    def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
        """Test listing all models."""
    def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
        """Test removing a model."""
    def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
        """Test saving/loading models with context test information."""
    def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
        """Test that the saved JSON has the expected format."""

def test_enum_values((self)):
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self, sample_model_data)):
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self)):
    """Test model creation using field aliases."""

def test_model_with_context_testing((self, sample_model_data)):
    """Test model with context testing information."""

def test_model_path_validation((self)):
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self)):
    """Test sanitized_id method."""

def test_model_to_registry_dict((self, sample_model_data)):
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self)):
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self, tmp_registry_file)):
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
    """Test saving and loading models."""

def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
    """Test getting a model by ID."""

def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
    """Test listing all models."""

def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
    """Test removing a model."""

def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
    """Test saving/loading models with context test information."""

def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

import pytest
from pydantic import ValidationError
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)):
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)):
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)):
        """Test resolver initialization."""
    def test_find_placeholders((self)):
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)):
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)):
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)):
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)):
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)):
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)):
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)):
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)):
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)):
        """Test resolution with empty string values."""
    def test_count_tokens((self)):
        """Test token counting."""
    def test_resolve_with_special_characters((self)):
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)):
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)):
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)):
    """Test resolver initialization."""

def test_find_placeholders((self)):
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)):
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)):
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)):
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)):
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)):
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)):
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)):
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)):
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)):
    """Test resolution with empty string values."""

def test_count_tokens((self)):
    """Test token counting."""

def test_resolve_with_special_characters((self)):
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self, tmp_path)):
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self, tmp_path)):
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self, tmp_path)):
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self, tmp_path)):
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self, tmp_path)):
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self, tmp_path)):
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self, tmp_path)):
        """Test that non-model files are skipped."""

def test_scanner_initialization((self, mock_get_path, tmp_path)):
    """Test scanner initialization."""

def test_get_model_size_file((self, tmp_path)):
    """Test getting size of a single model file."""

def test_get_model_size_directory((self, tmp_path)):
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self, tmp_path)):
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self, tmp_path)):
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self, tmp_path)):
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self, tmp_path)):
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self, tmp_path)):
    """Test that non-model files are skipped."""

def test_scan_models((self, mock_get_path, tmp_path)):
    """Test scanning for models."""

def test_sync_with_registry((self, mock_get_path, tmp_path)):
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_e2e/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.cli.main import create_app
from lmstrix.core.models import Model
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
import asyncio
from lmstrix.cli.main import CLI
from lmstrix.core.prompts import ResolvedPrompt
from lmstrix.core.inference import InferenceResult
from lmstrix.cli.main import CLI
from lmstrix.cli.main import main

class TestCLIIntegration:
    """Test CLI integration scenarios."""
    def test_cli_help((self, capsys)):
        """Test CLI help output."""

def mock_lmstudio_setup((self, tmp_path)):
    """Set up mock LM Studio environment."""

def test_list_models_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'models list' command."""

def test_scan_models_command((self, mock_scanner_class, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'models scan' command."""

def test_optimize_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'optimize' command."""

def test_infer_command_missing_prompt((self, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'infer' command with missing prompt."""

def test_infer_with_prompt_file((self, mock_engine_class, mock_load_prompts, mock_get_path, mock_lmstudio_setup, tmp_path)):
    """Test 'infer' command with prompt file."""

def test_cli_help((self, capsys)):
    """Test CLI help output."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import (
    load_context,
    load_context_batch,
    merge_contexts,
)

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path)):
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path)):
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path)):
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path)):
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path)):
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path)):
        """Test loading large context file."""
    def test_load_context_batch_single((self, tmp_path)):
        """Test loading batch with single file."""
    def test_load_context_batch_multiple((self, tmp_path)):
        """Test loading batch with multiple files."""
    def test_load_context_batch_with_errors((self, tmp_path)):
        """Test batch loading continues on error."""
    def test_load_context_batch_empty((self)):
        """Test loading empty batch."""
    def test_merge_contexts_simple((self)):
        """Test merging simple contexts."""
    def test_merge_contexts_with_separator((self)):
        """Test merging with custom separator."""
    def test_merge_contexts_single((self)):
        """Test merging single context."""
    def test_merge_contexts_empty((self)):
        """Test merging empty contexts."""
    def test_merge_contexts_with_headers((self)):
        """Test that merge includes file headers."""

def test_load_context_simple((self, tmp_path)):
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path)):
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path)):
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path)):
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path)):
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path)):
    """Test loading large context file."""

def test_load_context_batch_single((self, tmp_path)):
    """Test loading batch with single file."""

def test_load_context_batch_multiple((self, tmp_path)):
    """Test loading batch with multiple files."""

def test_load_context_batch_with_errors((self, tmp_path)):
    """Test batch loading continues on error."""

def test_load_context_batch_empty((self)):
    """Test loading empty batch."""

def test_merge_contexts_simple((self)):
    """Test merging simple contexts."""

def test_merge_contexts_with_separator((self)):
    """Test merging with custom separator."""

def test_merge_contexts_single((self)):
    """Test merging single context."""

def test_merge_contexts_empty((self)):
    """Test merging empty contexts."""

def test_merge_contexts_with_headers((self)):
    """Test that merge includes file headers."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry, scan_and_update_models

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self, tmp_path)):
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self, tmp_path)):
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self, tmp_path)):
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self, tmp_path)):
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self, tmp_path)):
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self, tmp_path)):
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self, tmp_path)):
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self, tmp_path)):
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self, tmp_path)):
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self, tmp_path)):
    """Test saving registry to custom path."""

def test_scan_and_update_models((self, mock_scanner_class, mock_client_class, tmp_path)):
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((self, mock_scanner_class, mock_client_class)):
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompt_file, load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self, tmp_path)):
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self, tmp_path)):
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self, tmp_path)):
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((self, tmp_path)):
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self, tmp_path)):
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self, tmp_path)):
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self, tmp_path)):
        """Test loading prompts from empty TOML file."""
    def test_load_prompt_file_simple((self, tmp_path)):
        """Test loading a single prompt file."""
    def test_load_prompt_file_missing_template((self, tmp_path)):
        """Test loading prompt file without template field."""
    def test_load_prompt_file_with_defaults((self, tmp_path)):
        """Test loading prompt file with default values."""

def test_load_prompts_simple((self, tmp_path)):
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self, tmp_path)):
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self, tmp_path)):
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((self, tmp_path)):
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self, tmp_path)):
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self, tmp_path)):
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self, tmp_path)):
    """Test loading prompts from empty TOML file."""

def test_load_prompt_file_simple((self, tmp_path)):
    """Test loading a single prompt file."""

def test_load_prompt_file_missing_template((self, tmp_path)):
    """Test loading prompt file without template field."""

def test_load_prompt_file_with_defaults((self, tmp_path)):
    """Test loading prompt file with default values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self, tmp_path)):
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self, tmp_path)):
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self, tmp_path)):
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self, tmp_path)):
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self, tmp_path)):
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self, tmp_path)):
        """Test getting default models file path."""
    def test_get_context_tests_dir((self, tmp_path)):
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self, tmp_path)):
        """Test getting context test log path."""
    def test_get_prompts_dir((self, tmp_path)):
        """Test getting prompts directory."""
    def test_get_contexts_dir((self, tmp_path)):
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((self, tmp_path)):
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self, tmp_path)):
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self, tmp_path)):
    """Test finding LM Studio in shared location."""

def exists_side_effect((self)):

def test_get_lmstudio_path_not_found((self, tmp_path)):
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self, tmp_path)):
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self, tmp_path)):
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self, tmp_path)):
    """Test getting default models file path."""

def test_get_context_tests_dir((self, tmp_path)):
    """Test getting context tests directory."""

def test_get_context_test_log_path((self, tmp_path)):
    """Test getting context test log path."""

def test_get_prompts_dir((self, tmp_path)):
    """Test getting prompts directory."""

def test_get_contexts_dir((self, tmp_path)):
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((self, tmp_path)):
    """Test handling permission errors when creating directories."""


</documents>
</document_content>
</document>

<document index="18">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="19">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.5",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml>=0.10",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN101",  # missing type annotation for self
  "ANN102",  # missing type annotation for cls
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]

[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

<document index="20">
<source>pytest.ini</source>
<document_content>
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import version, PackageNotFoundError
from typing import List
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

from lmstrix.cli.main import main


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_default_models_file as get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


<document index="21">
<source>src/lmstrix/py.typed</source>
<document_content>
# This file indicates that the package supports type hints
# as specified in PEP 561.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_lmstudio_path,
    get_lmstrix_data_dir,
    get_default_models_file,
    get_context_tests_dir,
    get_context_test_log_path,
    get_prompts_dir,
    get_contexts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()):
    """Mock LMStudioClient for testing."""

def mock_llm(()):
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()):
    """Sample model data for testing."""

def tmp_models_dir((tmp_path)):
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path)):
    """Create a temporary registry file path."""

def event_loop(()):
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()):
    """Mock completion response from LM Studio."""

def mock_prompt_template(()):
    """Sample prompt template for testing."""

def mock_context_data(()):
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys

def run_tests(()):
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self)):
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self)):
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self)):
        """Test client initialization with different verbose settings."""

def test_completion_response_creation((self)):
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self)):
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self)):
    """Test client initialization with different verbose settings."""

def test_list_models_success((self, mock_lmstudio)):
    """Test successful list_models call."""

def test_list_models_failure((self, mock_lmstudio)):
    """Test list_models with connection error."""

def test_load_model_success((self, mock_lmstudio)):
    """Test successful model loading."""

def test_load_model_failure((self, mock_lmstudio)):
    """Test model loading failure."""

def test_acompletion_success((self, mock_llm, mock_completion_response)):
    """Test successful async completion."""

def test_acompletion_failure((self, mock_llm)):
    """Test async completion failure."""

def test_acompletion_with_defaults((self, mock_llm, mock_completion_response)):
    """Test async completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

import pytest
from lmstrix.api.exceptions import (
    APIConnectionError,
    APIError,
    InferenceError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)):
        """Test base APIError class."""
    def test_api_connection_error((self)):
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)):
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)):
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)):
        """Test that all exceptions inherit from APIError."""

def test_api_error_base((self)):
    """Test base APIError class."""

def test_api_connection_error((self)):
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)):
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)):
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)):
    """Test that all exceptions inherit from APIError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.context_tester import ContextTestResult, ContextTester
from lmstrix.core.models import ContextTestStatus, Model

class TestContextTestResult:
    """Test ContextTestResult class."""
    def test_result_creation_minimal((self)):
        """Test creating a minimal test result."""
    def test_result_creation_full((self)):
        """Test creating a full test result."""
    def test_result_with_error((self)):
        """Test result with error."""
    def test_result_to_dict((self)):
        """Test converting result to dictionary."""
    def test_is_valid_response((self)):
        """Test response validation."""

class TestContextTester:
    """Test ContextTester class."""
    def test_tester_initialization((self, mock_lmstudio_client)):
        """Test context tester initialization."""
    def test_tester_default_client((self)):
        """Test tester creates default client if none provided."""
    def test_generate_test_prompt((self)):
        """Test test prompt generation."""
    def test_estimate_tokens((self)):
        """Test token estimation."""

def test_result_creation_minimal((self)):
    """Test creating a minimal test result."""

def test_result_creation_full((self)):
    """Test creating a full test result."""

def test_result_with_error((self)):
    """Test result with error."""

def test_result_to_dict((self)):
    """Test converting result to dictionary."""

def test_is_valid_response((self)):
    """Test response validation."""

def test_tester_initialization((self, mock_lmstudio_client)):
    """Test context tester initialization."""

def test_tester_default_client((self)):
    """Test tester creates default client if none provided."""

def test_generate_test_prompt((self)):
    """Test test prompt generation."""

def test_estimate_tokens((self)):
    """Test token estimation."""

def test_test_context_load_failure((self, mock_lmstudio_client)):
    """Test context testing when model fails to load."""

def test_test_context_inference_failure((self, mock_lmstudio_client, mock_llm)):
    """Test context testing when inference fails."""

def test_test_context_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful context testing."""

def test_test_context_invalid_response((self, mock_lmstudio_client, mock_llm)):
    """Test context testing with invalid response."""

def test_find_optimal_context_simple((self, mock_lmstudio_client, mock_llm)):
    """Test finding optimal context with simple scenario."""

def mock_completion((llm, prompt, **kwargs)):

def test_save_test_log((self, tmp_path)):
    """Test saving test log to file."""

def test_optimize_model_integration((self, mock_lmstudio_client, mock_llm, tmp_path)):
    """Test full model optimization workflow."""

def mock_completion((llm, prompt, **kwargs)):

def test_optimize_model_failure((self, mock_lmstudio_client)):
    """Test model optimization when all tests fail."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

import time
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelNotFoundError
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model

class TestInferenceResult:
    """Test InferenceResult model."""
    def test_inference_result_success((self)):
        """Test successful inference result."""
    def test_inference_result_failure((self)):
        """Test failed inference result."""
    def test_inference_result_empty_response((self)):
        """Test result with empty response is considered failed."""

class TestInferenceEngine:
    """Test InferenceEngine class."""
    def test_engine_initialization_defaults((self)):
        """Test engine initialization with defaults."""
    def test_engine_initialization_custom((self, mock_lmstudio_client)):
        """Test engine initialization with custom client and registry."""

def test_inference_result_success((self)):
    """Test successful inference result."""

def test_inference_result_failure((self)):
    """Test failed inference result."""

def test_inference_result_empty_response((self)):
    """Test result with empty response is considered failed."""

def test_engine_initialization_defaults((self)):
    """Test engine initialization with defaults."""

def test_engine_initialization_custom((self, mock_lmstudio_client)):
    """Test engine initialization with custom client and registry."""

def test_infer_model_not_found((self)):
    """Test inference with non-existent model."""

def test_infer_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful inference."""

def test_infer_with_untested_model((self, mock_lmstudio_client, mock_llm)):
    """Test inference with model that hasn't been context tested."""

def test_infer_with_max_tokens((self, mock_lmstudio_client, mock_llm)):
    """Test inference with custom max_tokens."""

def test_infer_load_failure((self, mock_lmstudio_client)):
    """Test inference when model fails to load."""

def test_infer_completion_failure((self, mock_lmstudio_client, mock_llm)):
    """Test inference when completion fails."""

def test_run_inference_simple((self, mock_lmstudio_client, mock_llm)):
    """Test simple run_inference method."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
import pytest
from pydantic import ValidationError
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self)):
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self, sample_model_data)):
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self)):
        """Test model creation using field aliases."""
    def test_model_with_context_testing((self, sample_model_data)):
        """Test model with context testing information."""
    def test_model_path_validation((self)):
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self)):
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self, sample_model_data)):
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self)):
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self, tmp_registry_file)):
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
        """Test saving and loading models."""
    def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
        """Test getting a model by ID."""
    def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
        """Test listing all models."""
    def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
        """Test removing a model."""
    def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
        """Test saving/loading models with context test information."""
    def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
        """Test that the saved JSON has the expected format."""

def test_enum_values((self)):
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self, sample_model_data)):
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self)):
    """Test model creation using field aliases."""

def test_model_with_context_testing((self, sample_model_data)):
    """Test model with context testing information."""

def test_model_path_validation((self)):
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self)):
    """Test sanitized_id method."""

def test_model_to_registry_dict((self, sample_model_data)):
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self)):
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self, tmp_registry_file)):
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
    """Test saving and loading models."""

def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
    """Test getting a model by ID."""

def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
    """Test listing all models."""

def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
    """Test removing a model."""

def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
    """Test saving/loading models with context test information."""

def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

import pytest
from pydantic import ValidationError
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)):
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)):
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)):
        """Test resolver initialization."""
    def test_find_placeholders((self)):
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)):
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)):
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)):
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)):
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)):
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)):
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)):
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)):
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)):
        """Test resolution with empty string values."""
    def test_count_tokens((self)):
        """Test token counting."""
    def test_resolve_with_special_characters((self)):
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)):
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)):
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)):
    """Test resolver initialization."""

def test_find_placeholders((self)):
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)):
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)):
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)):
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)):
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)):
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)):
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)):
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)):
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)):
    """Test resolution with empty string values."""

def test_count_tokens((self)):
    """Test token counting."""

def test_resolve_with_special_characters((self)):
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self, tmp_path)):
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self, tmp_path)):
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self, tmp_path)):
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self, tmp_path)):
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self, tmp_path)):
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self, tmp_path)):
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self, tmp_path)):
        """Test that non-model files are skipped."""

def test_scanner_initialization((self, mock_get_path, tmp_path)):
    """Test scanner initialization."""

def test_get_model_size_file((self, tmp_path)):
    """Test getting size of a single model file."""

def test_get_model_size_directory((self, tmp_path)):
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self, tmp_path)):
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self, tmp_path)):
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self, tmp_path)):
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self, tmp_path)):
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self, tmp_path)):
    """Test that non-model files are skipped."""

def test_scan_models((self, mock_get_path, tmp_path)):
    """Test scanning for models."""

def test_sync_with_registry((self, mock_get_path, tmp_path)):
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_e2e/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.cli.main import create_app
from lmstrix.core.models import Model
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
import asyncio
from lmstrix.cli.main import CLI
from lmstrix.core.prompts import ResolvedPrompt
from lmstrix.core.inference import InferenceResult
from lmstrix.cli.main import CLI
from lmstrix.cli.main import main

class TestCLIIntegration:
    """Test CLI integration scenarios."""
    def test_cli_help((self, capsys)):
        """Test CLI help output."""

def mock_lmstudio_setup((self, tmp_path)):
    """Set up mock LM Studio environment."""

def test_list_models_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'models list' command."""

def test_scan_models_command((self, mock_scanner_class, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'models scan' command."""

def test_optimize_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'optimize' command."""

def test_infer_command_missing_prompt((self, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'infer' command with missing prompt."""

def test_infer_with_prompt_file((self, mock_engine_class, mock_load_prompts, mock_get_path, mock_lmstudio_setup, tmp_path)):
    """Test 'infer' command with prompt file."""

def test_cli_help((self, capsys)):
    """Test CLI help output."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import (
    load_context,
    load_context_batch,
    merge_contexts,
)

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path)):
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path)):
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path)):
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path)):
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path)):
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path)):
        """Test loading large context file."""
    def test_load_context_batch_single((self, tmp_path)):
        """Test loading batch with single file."""
    def test_load_context_batch_multiple((self, tmp_path)):
        """Test loading batch with multiple files."""
    def test_load_context_batch_with_errors((self, tmp_path)):
        """Test batch loading continues on error."""
    def test_load_context_batch_empty((self)):
        """Test loading empty batch."""
    def test_merge_contexts_simple((self)):
        """Test merging simple contexts."""
    def test_merge_contexts_with_separator((self)):
        """Test merging with custom separator."""
    def test_merge_contexts_single((self)):
        """Test merging single context."""
    def test_merge_contexts_empty((self)):
        """Test merging empty contexts."""
    def test_merge_contexts_with_headers((self)):
        """Test that merge includes file headers."""

def test_load_context_simple((self, tmp_path)):
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path)):
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path)):
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path)):
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path)):
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path)):
    """Test loading large context file."""

def test_load_context_batch_single((self, tmp_path)):
    """Test loading batch with single file."""

def test_load_context_batch_multiple((self, tmp_path)):
    """Test loading batch with multiple files."""

def test_load_context_batch_with_errors((self, tmp_path)):
    """Test batch loading continues on error."""

def test_load_context_batch_empty((self)):
    """Test loading empty batch."""

def test_merge_contexts_simple((self)):
    """Test merging simple contexts."""

def test_merge_contexts_with_separator((self)):
    """Test merging with custom separator."""

def test_merge_contexts_single((self)):
    """Test merging single context."""

def test_merge_contexts_empty((self)):
    """Test merging empty contexts."""

def test_merge_contexts_with_headers((self)):
    """Test that merge includes file headers."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry, scan_and_update_models

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self, tmp_path)):
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self, tmp_path)):
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self, tmp_path)):
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self, tmp_path)):
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self, tmp_path)):
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self, tmp_path)):
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self, tmp_path)):
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self, tmp_path)):
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self, tmp_path)):
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self, tmp_path)):
    """Test saving registry to custom path."""

def test_scan_and_update_models((self, mock_scanner_class, mock_client_class, tmp_path)):
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((self, mock_scanner_class, mock_client_class)):
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompt_file, load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self, tmp_path)):
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self, tmp_path)):
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self, tmp_path)):
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((self, tmp_path)):
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self, tmp_path)):
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self, tmp_path)):
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self, tmp_path)):
        """Test loading prompts from empty TOML file."""
    def test_load_prompt_file_simple((self, tmp_path)):
        """Test loading a single prompt file."""
    def test_load_prompt_file_missing_template((self, tmp_path)):
        """Test loading prompt file without template field."""
    def test_load_prompt_file_with_defaults((self, tmp_path)):
        """Test loading prompt file with default values."""

def test_load_prompts_simple((self, tmp_path)):
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self, tmp_path)):
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self, tmp_path)):
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((self, tmp_path)):
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self, tmp_path)):
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self, tmp_path)):
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self, tmp_path)):
    """Test loading prompts from empty TOML file."""

def test_load_prompt_file_simple((self, tmp_path)):
    """Test loading a single prompt file."""

def test_load_prompt_file_missing_template((self, tmp_path)):
    """Test loading prompt file without template field."""

def test_load_prompt_file_with_defaults((self, tmp_path)):
    """Test loading prompt file with default values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self, tmp_path)):
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self, tmp_path)):
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self, tmp_path)):
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self, tmp_path)):
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self, tmp_path)):
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self, tmp_path)):
        """Test getting default models file path."""
    def test_get_context_tests_dir((self, tmp_path)):
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self, tmp_path)):
        """Test getting context test log path."""
    def test_get_prompts_dir((self, tmp_path)):
        """Test getting prompts directory."""
    def test_get_contexts_dir((self, tmp_path)):
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((self, tmp_path)):
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self, tmp_path)):
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self, tmp_path)):
    """Test finding LM Studio in shared location."""

def exists_side_effect((self)):

def test_get_lmstudio_path_not_found((self, tmp_path)):
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self, tmp_path)):
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self, tmp_path)):
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self, tmp_path)):
    """Test getting default models file path."""

def test_get_context_tests_dir((self, tmp_path)):
    """Test getting context tests directory."""

def test_get_context_test_log_path((self, tmp_path)):
    """Test getting context test log path."""

def test_get_prompts_dir((self, tmp_path)):
    """Test getting prompts directory."""

def test_get_contexts_dir((self, tmp_path)):
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((self, tmp_path)):
    """Test handling permission errors when creating directories."""


</documents>
</document_content>
</document>

<document index="26">
<source>obsolete/lmsm.json</source>
<document_content>
{
  "path": "/Users/shared/lmstudio",
  "llms": {
    "amoral-qwen3-14b-i1": {
      "id": "amoral-qwen3-14b-i1",
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/obsolete/lmsm.py
# Language: python

import json
from pathlib import Path
from typing import Any
import fire
import lmstudio as lms
from pydantic import BaseModel, Field
from rich import box
from rich.console import Console
from rich.live import Live
from rich.table import Table

class LmsMInfo(B, a, s, e, M, o, d, e, l):
    def to_table_row((self, base_path: Path)) -> list[str]:
    def to_dict((self)) -> dict[str, Any]:

class LmsM(B, a, s, e, M, o, d, e, l):
    def save((self)) -> None:
    def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
        """Update a model in the cache and save"""
    def remove_model((self, model_key: str)) -> None:
        """Remove a model from the cache and save"""
    def clear_models((self)) -> None:
        """Clear all models from the cache and save"""
    def create_table((self)) -> Table:
    def create_error_table((self, title: str)) -> Table:
    def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

class LmsmCLI:
    """LMStudio Model Manager CLI"""
    def __init__((self)):
    def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
        """List all models in LMStudio"""

def to_table_row((self, base_path: Path)) -> list[str]:

def to_dict((self)) -> dict[str, Any]:

def from_dict((cls, data: dict[str, Any])) -> "LmsMInfo":

def load_or_create((cls, lms_path: Path)) -> "LmsM":

def save((self)) -> None:

def update_model((self, model_key: str, model_info: LmsMInfo)) -> None:
    """Update a model in the cache and save"""

def remove_model((self, model_key: str)) -> None:
    """Remove a model from the cache and save"""

def clear_models((self)) -> None:
    """Clear all models from the cache and save"""

def create_table((self)) -> Table:

def create_error_table((self, title: str)) -> Table:

def update_from_lmstudio((
        self, all_rescan: bool = False, failed_rescan: bool = False
    )) -> None:

def render_table(()) -> Table:

def __init__((self)):

def list((self, all_rescan: bool = False, failed_rescan: bool = False)):
    """List all models in LMStudio"""


<document index="27">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.5",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml>=0.10",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.cli.main:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN101",  # missing type annotation for self
  "ANN102",  # missing type annotation for cls
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]

[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

<document index="28">
<source>pytest.ini</source>
<document_content>
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
asyncio_mode = auto
markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

import asyncio
from importlib.metadata import version, PackageNotFoundError
from typing import List
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix._version import __version__

class LMStrix:
    """Provides a high-level, simplified interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)):
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> List[Model]:
        """Scans for LM Studio models, updates the registry, and returns all models."""
    def list_models((self)) -> List[Model]:
        """Lists all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Runs the context-length test on a specific model and returns the updated model data."""
    def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)):
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> List[Model]:
    """Scans for LM Studio models, updates the registry, and returns all models."""

def list_models((self)) -> List[Model]:
    """Lists all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Runs the context-length test on a specific model and returns the updated model data."""

def infer((
        self, 
        model_id: str, 
        prompt: str, 
        max_tokens: int = -1, 
        temperature: float = 0.7
    )) -> InferenceResult:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

from lmstrix.cli.main import main


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

from typing import Any
import lmstudio
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)):
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length."""
    def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
        """Make an async completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)):
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length."""

def acompletion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = -1,  # -1 for unlimited
    )) -> CompletionResponse:
    """Make an async completion request to a loaded LM Studio model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)):
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)):
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)):
        """Initialize the exception."""

def __init__((self, message: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )):
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)):
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)):
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/cli/main.py
# Language: python

import asyncio
import fire
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference import InferenceEngine
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class LMStrixCLI:
    """A CLI for testing and managing LM Studio models."""
    def scan((self, verbose: bool = False)):
        """Scan for LM Studio models and update the local registry."""
    def list((self, verbose: bool = False)):
        """List all models from the registry with their test status."""
    def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
        """Run inference on a specified model."""

def scan((self, verbose: bool = False)):
    """Scan for LM Studio models and update the local registry."""

def list((self, verbose: bool = False)):
    """List all models from the registry with their test status."""

def test((self, model_id: str = None, all: bool = False, verbose: bool = False)):
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str,
        max_tokens: int = -1,
        temperature: float = 0.7,
        verbose: bool = False,
    )):
    """Run inference on a specified model."""

def main(()):
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )):
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 1024,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from loguru import logger
from lmstrix.api import LMStudioClient
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.models import ContextTestStatus, Model
from lmstrix.utils import get_context_test_log_path

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self, expected: str)) -> bool:
        """Check if response matches expected output."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((self, client: LMStudioClient | None = None)):
        """Initialize context tester."""
    def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
        """Append test result to log file."""
    def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
        """Test model at a specific context size."""
    def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
        """Run full context test on a model using binary search."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )):
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self, expected: str)) -> bool:
    """Check if response matches expected output."""

def __init__((self, client: LMStudioClient | None = None)):
    """Initialize context tester."""

def _log_result((self, log_path: Path, result: ContextTestResult)) -> None:
    """Append test result to log file."""

def _test_at_context((
        self, model_id: str, context_size: int, log_path: Path
    )) -> ContextTestResult:
    """Test model at a specific context size."""

def test_model((
        self,
        model: Model,
        min_context: int = 2048,
        max_context: int | None = None,
    )) -> Model:
    """Run full context test on a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
        """Initialize the inference engine."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
    )):
    """Initialize the inference engine."""

def infer((
        self,
        model_id: str,
        prompt: str,
        max_tokens: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from loguru import logger
from pydantic import BaseModel, Field, field_validator
from lmstrix.utils import get_default_models_file as get_models_registry_path

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model(B, a, s, e, M, o, d, e, l):
    """Represents a single LM Studio model with its metadata."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID suitable for filenames."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Convert to dictionary format for registry storage."""

class Config:
    """Pydantic configuration."""

class ModelRegistry:
    """Manages the collection of available models."""
    def __init__((self, models_file: Path | None = None)):
        """Initialize the model registry."""
    def load((self)) -> None:
        """Load models from the JSON file."""
    def save((self)) -> None:
        """Save models to the JSON file."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def list_models((self)) -> list[Model]:
        """Get all models in the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry and save."""
    def remove_model((self, model_id: str)) -> None:
        """Remove a model from the registry and save."""
    def __len__((self)) -> int:
        """Return the number of models in the registry."""

def validate_path((cls, v: Any)) -> Path:
    """Ensure path is a Path object."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID suitable for filenames."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Convert to dictionary format for registry storage."""

def __init__((self, models_file: Path | None = None)):
    """Initialize the model registry."""

def load((self)) -> None:
    """Load models from the JSON file."""

def save((self)) -> None:
    """Save models to the JSON file."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def list_models((self)) -> list[Model]:
    """Get all models in the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry and save."""

def remove_model((self, model_id: str)) -> None:
    """Remove a model from the registry and save."""

def __len__((self)) -> int:
    """Return the number of models in the registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from loguru import logger
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)):
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)):
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        max_tokens: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)):
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""

def __init__((self)):
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    max_tokens: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from loguru import logger
from lmstrix.api.client import LMStudioClient
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def scan_and_update_registry((verbose: bool = False)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from pathlib import Path
from typing import Any
import toml
from loguru import logger
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


<document index="29">
<source>src/lmstrix/py.typed</source>
<document_content>
# This file indicates that the package supports type hints
# as specified in PEP 561.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.paths import (
    get_lmstudio_path,
    get_lmstrix_data_dir,
    get_default_models_file,
    get_context_tests_dir,
    get_context_test_log_path,
    get_prompts_dir,
    get_contexts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from loguru import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from pathlib import Path
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()):
    """Mock LMStudioClient for testing."""

def mock_llm(()):
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()):
    """Sample model data for testing."""

def tmp_models_dir((tmp_path)):
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path)):
    """Create a temporary registry file path."""

def event_loop(()):
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()):
    """Mock completion response from LM Studio."""

def mock_prompt_template(()):
    """Sample prompt template for testing."""

def mock_context_data(()):
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/core/test_context_tester.py
# Language: python

import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.models import LmsModel, ModelRegistry, TestResult

def mock_model_registry(()):
    """Fixture to create a mock ModelRegistry with a single model."""

def mock_lmstudio_client(()):
    """Fixture to create a mock LM Studio client with async methods."""

def test_binary_search_logic_success((mock_save, mock_load, mock_client_constructor, mock_model_registry, mock_lmstudio_client)):
    """Test the binary search logic for a model that works up to a certain context size."""

def predict_side_effect((prompt, **kwargs)):

def test_model_never_loads((mock_save, mock_load, mock_client_constructor, mock_model_registry, mock_lmstudio_client)):
    """Test the scenario where the model fails to load in LM Studio."""

def test_inference_always_fails((mock_save, mock_load, mock_client_constructor, mock_model_registry, mock_lmstudio_client)):
    """Test the scenario where the model loads but inference always fails."""

def test_model_works_at_max_context((mock_save, mock_load, mock_client_constructor, mock_model_registry, mock_lmstudio_client)):
    """Test a model that works perfectly up to the maximum testable context."""


# File: /Users/Shared/lmstudio/lmstrix/tests/loaders/test_model_loader.py
# Language: python

import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from lmstrix.loaders.model_loader import scan_and_update_registry
from lmstrix.core.models import LmsModel, ModelRegistry, TestResult

def mock_empty_registry(()):
    """Fixture for an empty ModelRegistry."""

def mock_populated_registry(()):
    """Fixture for a ModelRegistry with existing and soon-to-be-deleted models."""

def test_scan_with_new_model((mock_save, mock_load, mock_list_models, mock_get_path, mock_empty_registry)):
    """Test adding a new model to an empty registry."""

def test_scan_updates_and_preserves_data((mock_save, mock_load, mock_list_models, mock_get_path, mock_populated_registry)):
    """Test that scanning adds new models, removes deleted ones, and preserves existing data."""

def test_scan_no_models_found((mock_save, mock_load, mock_list_models, mock_get_path, mock_empty_registry)):
    """Test that the registry remains empty if no models are found."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys

def run_tests(()):
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self)):
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self)):
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self)):
        """Test client initialization with different verbose settings."""

def test_completion_response_creation((self)):
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self)):
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self)):
    """Test client initialization with different verbose settings."""

def test_list_models_success((self, mock_lmstudio)):
    """Test successful list_models call."""

def test_list_models_failure((self, mock_lmstudio)):
    """Test list_models with connection error."""

def test_load_model_success((self, mock_lmstudio)):
    """Test successful model loading."""

def test_load_model_failure((self, mock_lmstudio)):
    """Test model loading failure."""

def test_acompletion_success((self, mock_llm, mock_completion_response)):
    """Test successful async completion."""

def test_acompletion_failure((self, mock_llm)):
    """Test async completion failure."""

def test_acompletion_with_defaults((self, mock_llm, mock_completion_response)):
    """Test async completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

import pytest
from lmstrix.api.exceptions import (
    APIConnectionError,
    APIError,
    InferenceError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)):
        """Test base APIError class."""
    def test_api_connection_error((self)):
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)):
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)):
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)):
        """Test that all exceptions inherit from APIError."""

def test_api_error_base((self)):
    """Test base APIError class."""

def test_api_connection_error((self)):
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)):
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)):
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)):
    """Test that all exceptions inherit from APIError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.context_tester import ContextTestResult, ContextTester
from lmstrix.core.models import ContextTestStatus, Model

class TestContextTestResult:
    """Test ContextTestResult class."""
    def test_result_creation_minimal((self)):
        """Test creating a minimal test result."""
    def test_result_creation_full((self)):
        """Test creating a full test result."""
    def test_result_with_error((self)):
        """Test result with error."""
    def test_result_to_dict((self)):
        """Test converting result to dictionary."""
    def test_is_valid_response((self)):
        """Test response validation."""

class TestContextTester:
    """Test ContextTester class."""
    def test_tester_initialization((self, mock_lmstudio_client)):
        """Test context tester initialization."""
    def test_tester_default_client((self)):
        """Test tester creates default client if none provided."""
    def test_generate_test_prompt((self)):
        """Test test prompt generation."""
    def test_estimate_tokens((self)):
        """Test token estimation."""

def test_result_creation_minimal((self)):
    """Test creating a minimal test result."""

def test_result_creation_full((self)):
    """Test creating a full test result."""

def test_result_with_error((self)):
    """Test result with error."""

def test_result_to_dict((self)):
    """Test converting result to dictionary."""

def test_is_valid_response((self)):
    """Test response validation."""

def test_tester_initialization((self, mock_lmstudio_client)):
    """Test context tester initialization."""

def test_tester_default_client((self)):
    """Test tester creates default client if none provided."""

def test_generate_test_prompt((self)):
    """Test test prompt generation."""

def test_estimate_tokens((self)):
    """Test token estimation."""

def test_test_context_load_failure((self, mock_lmstudio_client)):
    """Test context testing when model fails to load."""

def test_test_context_inference_failure((self, mock_lmstudio_client, mock_llm)):
    """Test context testing when inference fails."""

def test_test_context_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful context testing."""

def test_test_context_invalid_response((self, mock_lmstudio_client, mock_llm)):
    """Test context testing with invalid response."""

def test_find_optimal_context_simple((self, mock_lmstudio_client, mock_llm)):
    """Test finding optimal context with simple scenario."""

def mock_completion((llm, prompt, **kwargs)):

def test_save_test_log((self, tmp_path)):
    """Test saving test log to file."""

def test_optimize_model_integration((self, mock_lmstudio_client, mock_llm, tmp_path)):
    """Test full model optimization workflow."""

def mock_completion((llm, prompt, **kwargs)):

def test_optimize_model_failure((self, mock_lmstudio_client)):
    """Test model optimization when all tests fail."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

import time
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.api.exceptions import InferenceError, ModelNotFoundError
from lmstrix.core.inference import InferenceEngine, InferenceResult
from lmstrix.core.models import Model

class TestInferenceResult:
    """Test InferenceResult model."""
    def test_inference_result_success((self)):
        """Test successful inference result."""
    def test_inference_result_failure((self)):
        """Test failed inference result."""
    def test_inference_result_empty_response((self)):
        """Test result with empty response is considered failed."""

class TestInferenceEngine:
    """Test InferenceEngine class."""
    def test_engine_initialization_defaults((self)):
        """Test engine initialization with defaults."""
    def test_engine_initialization_custom((self, mock_lmstudio_client)):
        """Test engine initialization with custom client and registry."""

def test_inference_result_success((self)):
    """Test successful inference result."""

def test_inference_result_failure((self)):
    """Test failed inference result."""

def test_inference_result_empty_response((self)):
    """Test result with empty response is considered failed."""

def test_engine_initialization_defaults((self)):
    """Test engine initialization with defaults."""

def test_engine_initialization_custom((self, mock_lmstudio_client)):
    """Test engine initialization with custom client and registry."""

def test_infer_model_not_found((self)):
    """Test inference with non-existent model."""

def test_infer_success((self, mock_lmstudio_client, mock_llm)):
    """Test successful inference."""

def test_infer_with_untested_model((self, mock_lmstudio_client, mock_llm)):
    """Test inference with model that hasn't been context tested."""

def test_infer_with_max_tokens((self, mock_lmstudio_client, mock_llm)):
    """Test inference with custom max_tokens."""

def test_infer_load_failure((self, mock_lmstudio_client)):
    """Test inference when model fails to load."""

def test_infer_completion_failure((self, mock_lmstudio_client, mock_llm)):
    """Test inference when completion fails."""

def test_run_inference_simple((self, mock_lmstudio_client, mock_llm)):
    """Test simple run_inference method."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
import pytest
from pydantic import ValidationError
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self)):
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self, sample_model_data)):
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self)):
        """Test model creation using field aliases."""
    def test_model_with_context_testing((self, sample_model_data)):
        """Test model with context testing information."""
    def test_model_path_validation((self)):
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self)):
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self, sample_model_data)):
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self)):
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self, tmp_registry_file)):
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
        """Test saving and loading models."""
    def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
        """Test getting a model by ID."""
    def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
        """Test listing all models."""
    def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
        """Test removing a model."""
    def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
        """Test saving/loading models with context test information."""
    def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
        """Test that the saved JSON has the expected format."""

def test_enum_values((self)):
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self, sample_model_data)):
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self)):
    """Test model creation using field aliases."""

def test_model_with_context_testing((self, sample_model_data)):
    """Test model with context testing information."""

def test_model_path_validation((self)):
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self)):
    """Test sanitized_id method."""

def test_model_to_registry_dict((self, sample_model_data)):
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self)):
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self, tmp_registry_file)):
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((self, tmp_registry_file, sample_model_data)):
    """Test saving and loading models."""

def test_registry_get_model((self, tmp_registry_file, sample_model_data)):
    """Test getting a model by ID."""

def test_registry_list_models((self, tmp_registry_file, sample_model_data)):
    """Test listing all models."""

def test_registry_remove_model((self, tmp_registry_file, sample_model_data)):
    """Test removing a model."""

def test_registry_with_context_test_data((self, tmp_registry_file, sample_model_data)):
    """Test saving/loading models with context test information."""

def test_registry_json_format((self, tmp_registry_file, sample_model_data)):
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

import pytest
from pydantic import ValidationError
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)):
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)):
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)):
        """Test resolver initialization."""
    def test_find_placeholders((self)):
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)):
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)):
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)):
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)):
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)):
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)):
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)):
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)):
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)):
        """Test resolution with empty string values."""
    def test_count_tokens((self)):
        """Test token counting."""
    def test_resolve_with_special_characters((self)):
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)):
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)):
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)):
    """Test resolver initialization."""

def test_find_placeholders((self)):
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)):
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)):
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)):
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)):
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)):
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)):
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)):
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)):
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)):
    """Test resolution with empty string values."""

def test_count_tokens((self)):
    """Test token counting."""

def test_resolve_with_special_characters((self)):
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self, tmp_path)):
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self, tmp_path)):
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self, tmp_path)):
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self, tmp_path)):
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self, tmp_path)):
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self, tmp_path)):
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self, tmp_path)):
        """Test that non-model files are skipped."""

def test_scanner_initialization((self, mock_get_path, tmp_path)):
    """Test scanner initialization."""

def test_get_model_size_file((self, tmp_path)):
    """Test getting size of a single model file."""

def test_get_model_size_directory((self, tmp_path)):
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self, tmp_path)):
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self, tmp_path)):
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self, tmp_path)):
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self, tmp_path)):
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self, tmp_path)):
    """Test that non-model files are skipped."""

def test_scan_models((self, mock_get_path, tmp_path)):
    """Test scanning for models."""

def test_sync_with_registry((self, mock_get_path, tmp_path)):
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_e2e/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
import pytest
from lmstrix.cli.main import create_app
from lmstrix.core.models import Model
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
from lmstrix.cli.main import CLI
import asyncio
from lmstrix.cli.main import CLI
from lmstrix.core.prompts import ResolvedPrompt
from lmstrix.core.inference import InferenceResult
from lmstrix.cli.main import CLI
from lmstrix.cli.main import main

class TestCLIIntegration:
    """Test CLI integration scenarios."""
    def test_cli_help((self, capsys)):
        """Test CLI help output."""

def mock_lmstudio_setup((self, tmp_path)):
    """Set up mock LM Studio environment."""

def test_list_models_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'models list' command."""

def test_scan_models_command((self, mock_scanner_class, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'models scan' command."""

def test_optimize_command((self, mock_client_class, mock_get_path, mock_lmstudio_setup)):
    """Test 'optimize' command."""

def test_infer_command_missing_prompt((self, mock_get_path, mock_lmstudio_setup, capsys)):
    """Test 'infer' command with missing prompt."""

def test_infer_with_prompt_file((self, mock_engine_class, mock_load_prompts, mock_get_path, mock_lmstudio_setup, tmp_path)):
    """Test 'infer' command with prompt file."""

def test_cli_help((self, capsys)):
    """Test CLI help output."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import (
    load_context,
    load_context_batch,
    merge_contexts,
)

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path)):
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path)):
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path)):
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path)):
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path)):
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path)):
        """Test loading large context file."""
    def test_load_context_batch_single((self, tmp_path)):
        """Test loading batch with single file."""
    def test_load_context_batch_multiple((self, tmp_path)):
        """Test loading batch with multiple files."""
    def test_load_context_batch_with_errors((self, tmp_path)):
        """Test batch loading continues on error."""
    def test_load_context_batch_empty((self)):
        """Test loading empty batch."""
    def test_merge_contexts_simple((self)):
        """Test merging simple contexts."""
    def test_merge_contexts_with_separator((self)):
        """Test merging with custom separator."""
    def test_merge_contexts_single((self)):
        """Test merging single context."""
    def test_merge_contexts_empty((self)):
        """Test merging empty contexts."""
    def test_merge_contexts_with_headers((self)):
        """Test that merge includes file headers."""

def test_load_context_simple((self, tmp_path)):
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path)):
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path)):
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path)):
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path)):
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path)):
    """Test loading large context file."""

def test_load_context_batch_single((self, tmp_path)):
    """Test loading batch with single file."""

def test_load_context_batch_multiple((self, tmp_path)):
    """Test loading batch with multiple files."""

def test_load_context_batch_with_errors((self, tmp_path)):
    """Test batch loading continues on error."""

def test_load_context_batch_empty((self)):
    """Test loading empty batch."""

def test_merge_contexts_simple((self)):
    """Test merging simple contexts."""

def test_merge_contexts_with_separator((self)):
    """Test merging with custom separator."""

def test_merge_contexts_single((self)):
    """Test merging single context."""

def test_merge_contexts_empty((self)):
    """Test merging empty contexts."""

def test_merge_contexts_with_headers((self)):
    """Test that merge includes file headers."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry, scan_and_update_models

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self, tmp_path)):
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self, tmp_path)):
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self, tmp_path)):
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self, tmp_path)):
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self, tmp_path)):
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self, tmp_path)):
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self, tmp_path)):
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self, tmp_path)):
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self, tmp_path)):
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self, tmp_path)):
    """Test saving registry to custom path."""

def test_scan_and_update_models((self, mock_scanner_class, mock_client_class, tmp_path)):
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((self, mock_scanner_class, mock_client_class)):
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompt_file, load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self, tmp_path)):
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self, tmp_path)):
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self, tmp_path)):
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((self, tmp_path)):
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self, tmp_path)):
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self, tmp_path)):
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self, tmp_path)):
        """Test loading prompts from empty TOML file."""
    def test_load_prompt_file_simple((self, tmp_path)):
        """Test loading a single prompt file."""
    def test_load_prompt_file_missing_template((self, tmp_path)):
        """Test loading prompt file without template field."""
    def test_load_prompt_file_with_defaults((self, tmp_path)):
        """Test loading prompt file with default values."""

def test_load_prompts_simple((self, tmp_path)):
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self, tmp_path)):
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self, tmp_path)):
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((self, tmp_path)):
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self, tmp_path)):
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self, tmp_path)):
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self, tmp_path)):
    """Test loading prompts from empty TOML file."""

def test_load_prompt_file_simple((self, tmp_path)):
    """Test loading a single prompt file."""

def test_load_prompt_file_missing_template((self, tmp_path)):
    """Test loading prompt file without template field."""

def test_load_prompt_file_with_defaults((self, tmp_path)):
    """Test loading prompt file with default values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self, tmp_path)):
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self, tmp_path)):
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self, tmp_path)):
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self, tmp_path)):
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self, tmp_path)):
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self, tmp_path)):
        """Test getting default models file path."""
    def test_get_context_tests_dir((self, tmp_path)):
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self, tmp_path)):
        """Test getting context test log path."""
    def test_get_prompts_dir((self, tmp_path)):
        """Test getting prompts directory."""
    def test_get_contexts_dir((self, tmp_path)):
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((self, tmp_path)):
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self, tmp_path)):
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((self, tmp_path)):
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self, tmp_path)):
    """Test finding LM Studio in shared location."""

def exists_side_effect((self)):

def test_get_lmstudio_path_not_found((self, tmp_path)):
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self, tmp_path)):
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self, tmp_path)):
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self, tmp_path)):
    """Test getting default models file path."""

def test_get_context_tests_dir((self, tmp_path)):
    """Test getting context tests directory."""

def test_get_context_test_log_path((self, tmp_path)):
    """Test getting context test log path."""

def test_get_prompts_dir((self, tmp_path)):
    """Test getting prompts directory."""

def test_get_contexts_dir((self, tmp_path)):
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((self, tmp_path)):
    """Test handling permission errors when creating directories."""


# File: /Users/Shared/lmstudio/lmstrix/tests/utils/test_paths.py
# Language: python

import pytest
from pathlib import Path
from unittest.mock import patch, mock_open
from lmstrix.utils.paths import get_lmstudio_path, get_default_models_file

def test_get_lmstudio_path_from_pointer_file((mock_home)):
    """Test that the path is correctly read from the .lmstudio-home-pointer file."""

def test_get_lmstudio_path_fallback_to_default((mock_home)):
    """Test the fallback mechanism when the pointer file does not exist."""

def path_exists_side_effect((path)):

def test_get_lmstudio_path_not_found((mock_home)):
    """Test that an exception is raised if no path can be found."""

def test_get_default_models_file_creates_dir((mock_mkdir, mock_appdirs)):
    """Test that the function creates the necessary directory and returns the correct file path."""


</documents>