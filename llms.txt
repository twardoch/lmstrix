Project Structure:
📁 lmstrix
├── 📁 _keep_this
│   └── 📁 adam
│       ├── 📁 analyse
│       │   ├── 📁 0
│       │   ├── 📁 input
│       │   ├── 📁 output
│       │   ├── 📁 output-all
│       │   ├── 📁 review
│       │   ├── 📁 review-cla
│       │   └── 📁 review-gemi
│       └── 📁 out
├── 📁 context_tests
├── 📁 docs
│   ├── 📄 _config.yml
│   ├── 📄 api.md
│   ├── 📄 how-it-works.md
│   ├── 📄 index.md
│   ├── 📄 installation.md
│   └── 📄 usage.md
├── 📁 examples
│   ├── 📁 cli
│   │   ├── 📄 basic_workflow.sh
│   │   ├── 📄 context_control_examples.sh
│   │   ├── 📄 inference_examples.sh
│   │   ├── 📄 model_state_demo.sh
│   │   └── 📄 model_testing.sh
│   ├── 📁 data
│   │   ├── 📄 sample_context.txt
│   │   └── 📄 test_questions.json
│   ├── 📁 prompts
│   │   ├── 📄 analysis.toml
│   │   ├── 📄 coding.toml
│   │   ├── 📄 creative.toml
│   │   └── 📄 qa.toml
│   ├── 📁 python
│   │   ├── 📄 __init__.py
│   │   ├── 📄 advanced_testing.py
│   │   ├── 📄 basic_usage.py
│   │   ├── 📄 batch_processing.py
│   │   ├── 📄 custom_inference.py
│   │   └── 📄 prompt_templates_demo.py
│   ├── 📁 specialized
│   │   └── 📄 elo_liczby.py
│   ├── 📄 prompts.toml
│   ├── 📄 README.md
│   └── 📄 run_all_examples.sh
├── 📁 issues
│   └── 📄 401.txt
├── 📁 src
│   ├── 📁 lmstrix
│   │   ├── 📁 api
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 client.py
│   │   │   ├── 📄 exceptions.py
│   │   │   └── 📄 main.py
│   │   ├── 📁 core
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 concrete_config.py
│   │   │   ├── 📄 context.py
│   │   │   ├── 📄 context_tester.py
│   │   │   ├── 📄 inference.py
│   │   │   ├── 📄 inference_manager.py
│   │   │   ├── 📄 models.py
│   │   │   ├── 📄 prompts.py
│   │   │   └── 📄 scanner.py
│   │   ├── 📁 loaders
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context_loader.py
│   │   │   ├── 📄 model_loader.py
│   │   │   └── 📄 prompt_loader.py
│   │   ├── 📁 utils
│   │   │   ├── 📄 __init__.py
│   │   │   ├── 📄 context_parser.py
│   │   │   ├── 📄 logging.py
│   │   │   ├── 📄 paths.py
│   │   │   └── 📄 state.py
│   │   ├── 📄 __init__.py
│   │   ├── 📄 __main__.py
│   │   └── 📄 py.typed
│   └── 📁 lmstrix.egg-info
├── 📁 tests
│   ├── 📁 test_api
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_client.py
│   │   └── 📄 test_exceptions.py
│   ├── 📁 test_core
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_tester.py
│   │   ├── 📄 test_inference.py
│   │   ├── 📄 test_models.py
│   │   ├── 📄 test_prompts.py
│   │   └── 📄 test_scanner.py
│   ├── 📁 test_integration
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_cli_integration.py
│   ├── 📁 test_loaders
│   │   ├── 📄 __init__.py
│   │   ├── 📄 test_context_loader.py
│   │   ├── 📄 test_model_loader.py
│   │   └── 📄 test_prompt_loader.py
│   ├── 📁 test_utils
│   │   ├── 📄 __init__.py
│   │   └── 📄 test_paths.py
│   ├── 📄 __init__.py
│   ├── 📄 conftest.py
│   └── 📄 run_tests.py
├── 📄 .gitignore
├── 📄 AGENTS.md
├── 📄 build.sh
├── 📄 CHANGELOG.md
├── 📄 CLAUDE.md
├── 📄 cleanup.txt
├── 📄 debug_inference.py
├── 📄 debug_test_logic.py
├── 📄 GEMINI.md
├── 📄 get_out_ctx.py
├── 📄 LICENSE
├── 📄 PLAN.md
├── 📄 publish.sh
├── 📄 pyproject.toml
├── 📄 README.md
├── 📄 test_model_out_ctx.py
├── 📄 test_prompt_example.txt
├── 📄 test_streaming.py
├── 📄 TESTING.md
├── 📄 TODO.md
└── 📄 WORK.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="2">
<source>.gitignore</source>
<document_content>
__marimo__/
__pycache__/
__pypackages__/
_keep_this/adam
._*
.abstra/
.AppleDouble
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.envrc
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.LSOverride
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pixi
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.scrapy
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.pot
*.py.cover
*.py[codz]
*.sage.py
*.so
*.spec
*$py.class
/site
build/
celerybeat-schedule
celerybeat.pid
CLEANUP.txt
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
ehthumbs.db
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
llms.txt
local_settings.py
MANIFEST
marimo/_lsp/
marimo/_static/
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
src/lmstrix/_version.py
src/lmstrix/_version.py
target/
Thumbs.db
uv.lock
var/
venv.bak/
venv/
wheels/
</document_content>
</document>

<document index="3">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-toml
      - id: check-json
      - id: pretty-format-json
        args: [--autofix, --no-sort-keys]
      - id: mixed-line-ending
        args: [--fix=lf]
      - id: detect-private-key

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.15.0
    hooks:
      - id: pyupgrade
        args: [--py310-plus]

  - repo: https://github.com/psf/black
    rev: 23.12.1
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.11
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies:
          - pydantic>=2.0
          - types-toml
        files: ^src/
        args: [--install-types, --non-interactive]

ci:
  autoupdate_schedule: monthly
  skip: [mypy]
</document_content>
</document>

<document index="4">
<source>AGENTS.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="5">
<source>CHANGELOG.md</source>
<document_content>
# Changelog

All notable changes to the LMStrix project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

## [1.0.66] - 2025-08-05

### Fixed
- **Issue #302**: Inference Output Mismatch Investigation - Major Progress
  - Added comprehensive diagnostic logging for all inference parameters
  - Updated default temperature from 0.7 to 0.8 to match LM Studio GUI defaults
  - Added CLI parameters for configuring inference settings (top_k, temperature, etc.)
  - Fixed Rich table display to use full console width with `expand=True`
  - Enhanced response preview in test command from 20 to 60 characters for better readability
  - Created diagnostic tools: `debug_inference.py`, `debug_test_logic.py`, `get_out_ctx.py`

### Enhanced
- **Table Display Improvements**
  - All Rich tables now use full console width (9 tables updated)
  - Removed fixed width parameters for better terminal utilization
  - Improved response preview formatting with better truncation and cleanup

### Added
- **Diagnostic and Debug Tools**
  - `debug_inference.py`: Tool for debugging inference parameter differences
  - `debug_test_logic.py`: Tool for testing context logic
  - `get_out_ctx.py`: Utility for analyzing output context calculations
  - `test_model_out_ctx.py`: Testing script for model context handling

### Testing
- **Test Suite Stability Improvements**
  - Fixed all critical AttributeError issues in test suite
  - Resolved Model.validate_integrity() method errors
  - Fixed PromptResolver and ContextTester method access issues
  - Improved ModelScanner.sync_with_registry() reliability

### Added
- **Compact Output for Test Command** ✅ COMPLETED
  - Added live-updating rich table display for non-verbose test output
  - Shows model ID, context size, and test status in a clean format
  - Progress tracking for batch tests with `--all` flag
  - Maintains detailed logging in verbose mode for debugging
  - Significantly improves readability when testing multiple models
- **Custom Prompt Support for Context Testing** ✅ COMPLETED
  - Added `--prompt` parameter to test command for using custom prompt strings
  - Added `--file_prompt` parameter to test command for loading prompts from files
  - Modified ContextTester and InferenceEngine to accept custom prompts
  - Custom prompts replace the default dual-test prompts (96 digits + 2+3=5)
  - Updated help documentation with examples of custom prompt usage
  - Created test_prompt_example.txt as a demonstration file
- **Issue #307**: Streaming Inference Support ✅ COMPLETED
  - Added `stream_completion()` method to LMStudioClient using lmstudio SDK's `complete_stream()`
  - Implemented token-by-token streaming with callbacks and progress monitoring
  - Added `stream_infer()` methods to InferenceEngine and InferenceManager
  - Added CLI `--stream` flag for real-time token display during inference
  - Added `--stream-timeout` parameter (default 120s) for hang detection
  - Streaming statistics: tokens/second, time to first token
  - Full backward compatibility maintained with existing sync inference
- **Issue #306**: Batch Processing Tool ✅ COMPLETED (`_keep_this/adam/adamall.py`)
  - Processes multiple prompts across all models automatically
  - Smart model management - reuses loaded models when possible
  - Skips existing outputs for resumable batch processing
  - Safe filename generation using pathvalidate
  - Error capture to output files on failure
  - Progress tracking with percentage completion
  - Processes prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`
  - Loads models with 50% context, runs inference with 90% max context
  - Intelligent model sorting by size for optimal processing
- Created `src/lmstrix/api/main.py` with `LMStrixService` class containing all business logic
- Implemented separation of concerns with thin CLI wrapper in `__main__.py`

### Changed
- Default temperature changed from 0.7 to 0.8 to match LM Studio GUI defaults
- Default `out_ctx` now falls back to the model's `ctx_out − 1` when callers leave it as `-1`. This avoids LM Studio SDK hangs triggered by unlimited generations.
- Refactored CLI architecture - moved all business logic from `__main__.py` to `api/main.py`
- Updated `__main__.py` to be a thin wrapper that delegates to `LMStrixService`
- Modified test imports to use new module structure

### Fixed
- **Issue #303**: Fixed loguru output interference with model responses
  - Removed loguru formatting of config dictionaries and raw model responses
  - Changed problematic log messages in `api/client.py` lines 265 and 267 to simple status messages
  - Prevented KeyError and ValueError exceptions when loguru tried to parse model output
  - Ensures clean separation of diagnostic output (stderr) from model output (stdout)
- Fixed import error after removing `src/lmstrix/cli` directory
- Updated `pyproject.toml` entry point from `lmstrix.cli.main:main` to `lmstrix.__main__:main`

### Removed
- Removed `src/lmstrix/cli` directory (merged functionality into `__main__.py`)

## [1.0.59] - 2025-07-31

### Major Improvements & Bug Fixes

#### Issues 201-204 (Completed)
- **Issue 201**: Enhanced model persistence - models now stay loaded between inference calls when no explicit context is specified
- **Issue 202**: Beautiful enhanced logging with emojis, model info, config details, and prompt statistics  
- **Issue 203**: Fixed model lookup to find by both path and ID without changing JSON structure
- **Issue 204**: Added comprehensive verbose stats logging including time to first token, predicted tokens, tokens/second, and total inference time

#### Model Registry Improvements
- Fixed smart model lookup that works with both model paths and IDs
- Preserved original JSON structure keyed by path
- No data duplication - registry size maintained
- Backward compatible with existing path-based lookups

#### Enhanced Logging & Statistics
- Beautiful formatted logging with visual separators and emojis
- Complete inference statistics display including:
  - ⚡ Time to first token
  - ⏱️ Total inference time  
  - 🔢 Predicted tokens
  - 📝 Prompt tokens
  - 🎯 Total tokens
  - 🚀 Tokens/second
  - 🛑 Stop reason
- Eliminated duplicate stats display at end of output

### Added

- **Context Parameter Percentage Support**
  - `--out_ctx` parameter now supports percentage notation (e.g., "80%")
  - Created `utils/context_parser.py` for parsing context parameters
  - Percentage calculated from model's tested or declared maximum context

- **Improved CLI Output**
  - Non-verbose mode for `infer` command now shows only model response
  - Removed extra formatting and status information in quiet mode

- **Prompt File Support with TOML (Issue #104)**
  - Added `--file_prompt` parameter to load prompts from TOML files
  - Added `--dict` parameter for passing key=value pairs for placeholder resolution
  - When using `--file_prompt`, the prompt parameter refers to the prompt name in the TOML file
  - Supports nested placeholders and internal template references
  - Reports unresolved placeholders in verbose mode
  - Includes comprehensive example prompt file with various use cases

- **Enhanced Infer Context Control (Issue #103)**
  - Added `--in_ctx` parameter to control model loading context size
  - Added `--out_ctx` parameter to replace deprecated `--max_tokens`
  - Supports `--in_ctx 0` to load model without context specification
  - When `--in_ctx` is not specified, uses optimal context (tested or declared)
  - Explicit `--in_ctx` always unloads existing models and reloads with specified context
  - Smart unloading: only unloads models that were explicitly loaded with `--in_ctx`

- **Smart Model Loading**
  - Added model state detection to check if models are already loaded
  - Reuses existing loaded models when no explicit context specified
  - Added `--force-reload` flag to force model reload even if already loaded
  - Shows clear status messages about model reuse vs reload

- **Model State Persistence (Issue #201)**
  - Models now stay loaded between `infer` calls when `--in_ctx` is not specified
  - Added StateManager to track last-used model across sessions
  - Model ID parameter (`-m`) is now optional - uses last-used model when omitted
  - Significantly improves performance by avoiding repeated model loading/unloading
  - Created `examples/cli/model_state_demo.sh` demonstrating the feature

- **Context Validation**
  - Validates requested context against model's declared and tested limits
  - Warns when context exceeds safe limits
  - Suggests optimal context values

### Changed

- **Inference Command**
  - Deprecated `--max_tokens` in favor of `--out_ctx` (backward compatible with warnings)
  - Updated help text and documentation for new parameters
  - Improved model loading logic for better memory management
  - Enhanced status messages during inference operations

## [1.0.53] - 2025-07-29

### Fixed

- **Git Configuration**
  - Fixed git pull error by setting upstream branch tracking with `git branch --set-upstream-to=origin/main main`
  - Resolved "exit code(1)" error when running `git pull -v -- origin` without branch specification

### Changed

- **Version Maintenance**
  - Updated to version 1.0.53 with proper git configuration fixes

## [1.0.28 - 1.0.52] - 2025-07-25 to 2025-07-29

### Added

- **Enhanced CLI Features**
  - Added `--sort` option to `lmstrix test --all` command with same sort options as list (id, ctx, dtx, size, etc.)
  - Added `--ctx` option to `lmstrix test --all` for testing all untested models at a specific context size
  - Added `--show` option to `lmstrix list` with three output formats:
    - `id`: Plain newline-delimited list of model IDs
    - `path`: Newline-delimited list of relative paths (same as id currently)
    - `json`: Full JSON array from the registry
  - All `--show` formats respect the `--sort` option for flexible output

- **CLI Improvements**
  - Modified `--ctx` to work with `--all` flag for batch testing at specific context sizes
  - `test --all --ctx` filters models based on context limits and safety checks
  - Added proper status updates and persistence when using `--ctx` for single model tests
  - Fixed model field updates (tested_max_context, last_known_good_context) during --ctx testing

### Changed

- **Removed all asyncio dependencies (Issue #204)**
  - Converted entire codebase from async to synchronous
  - Now uses the native synchronous `lmstudio` package API directly
  - Simplified architecture by removing async/await complexity
  - Implemented signal-based timeout for Unix systems
  - All methods now return results directly without await

### Added

- **Context Size Safety Validation**
  - Added validation to prevent testing at or above `last_known_bad_context`
  - CLI `--ctx` parameter now checks against `last_known_bad_context` and limits to 90% of last bad
  - Automatic testing algorithms now respect `last_known_bad_context` during iterations
  - Added warning messages when context size approaches 80% or 90% of last known bad context
  - Prevents system crashes by avoiding previously failed context sizes

- **Enhanced Context Testing Strategy (Issue #201)**
  - Added `--threshold` parameter to test command (default: 102,400 tokens)
  - Prevents system crashes by limiting initial test size
  - New incremental testing algorithm: test at 1024, then threshold, then increment by 10,240
  - Optimized batch testing for `--all` flag with pass-based approach
  - Models sorted by declared context size to minimize loading/unloading
  - Rich table output showing test results with efficiency percentages

- **Smart Context Testing with Progress Saving**
  - Context tests now start with small context (32) to verify model loads
  - Added fields to track last known good/bad context sizes
  - Tests can resume from previous state if interrupted
  - Progress is saved to JSON after each test iteration
  - Changed test prompt from "2+2=" to "Say hello" for better reliability

### Fixed

- **Terminology Improvements**
  - Changed "Loaded X models" to "Read X models" to avoid confusion with LM Studio's model loading
  - Replaced generic "Check logs for details" with specific error messages

- **Context Testing Stability**
  - Added delays between model load/unload operations to prevent rapid cycling
  - Fixed connection reset issues caused by too-rapid operations
  - Enhanced binary search logging to show progress clearly

### Changed

- **Model Data Structure**
  - Added `last_known_good_context` field for resumable testing
  - Added `last_known_bad_context` field for resumable testing
  - Updated registry serialization to include new fields

## [1.0.28] - 2025-07-25

### Added

- **GitHub Pages Documentation Site**
  - Created comprehensive documentation site structure under `docs/`
  - Added Jekyll configuration with custom theme and navigation
  - Created documentation pages: installation, usage, API reference, how-it-works
  - Set up automatic changelog integration with documentation
  - Added responsive design and syntax highlighting

- **Example Improvements**
  - Added example output logging to `examples.log.txt` and `examples.err.txt`
  - Enhanced error handling in example scripts
  - Added more detailed comments in Python examples

### Fixed

- **Client Compatibility**
  - Fixed attribute access issues in `api/client.py` for embedding models
  - Added proper handling for different model types (LLMs vs Embeddings)
  - Improved error messages for unsupported model types

- **Context Testing Robustness**
  - Enhanced context size detection with better error handling
  - Improved inference validation logic
  - Added fallback mechanisms for edge cases

### Changed

- **CLI Enhancements**
  - Improved output formatting for model listings
  - Better progress indicators during testing
  - More informative error messages

- **Documentation Updates**
  - Updated README with clearer examples
  - Enhanced API documentation with more details
  - Added troubleshooting section

## [1.0.23] - 2025-07-25

### Added

- Implemented comprehensive `test_binary_search_logic` test function with edge cases:
  - Model that works at all sizes
  - Model that never works  
  - Model that loads but never passes inference
  - Model that works up to a specific context size
  - Model that never loads

### Fixed

- Fixed syntax errors in Python example files:
  - `examples/python/batch_processing.py` - Fixed unterminated string literals
  - `examples/python/custom_inference.py` - Fixed similar syntax issues
- Improved code quality by fixing linting issues:
  - Updated file operations to use `Path.open()` instead of `open()` (PTH123)
  - Added proper exception chaining with `from e` (B904)
  - Fixed shadowing of Python builtin `all` by renaming to `all_models` (A002)
- Updated `api/client.py` to handle different attribute names for LLMs vs Embeddings

### Changed

- All mypy type checking now passes without errors
- Package builds successfully as v1.0.23

## [1.0.21] - 2025-07-25

### Fixed

- Code formatting improvements in test files for better readability
- Updated `cleanup.sh` script
- Reduced size of `llms.txt` file significantly for better performance

## [1.0.0] - 2025-07-25

### Changed

- **Major Refactoring: `litellm` to Native `lmstudio` Integration**
  - **Dependency Pivot**: Completely removed the `litellm` dependency and replaced it with the native `lmstudio` package for all model interactions. This provides a more robust, reliable, and direct integration.
  - **API Client (`api/client.py`)**: Rewritten to be a direct, thin wrapper around the `lmstudio` package’s core functions (`list_downloaded_models`, `llm`, `complete`, `unload`).
  - **Context Tester (`core/context_tester.py`)**: The engine for finding the true context limit of a model has been completely rewritten to use the native `lmstudio` client. It now performs a binary search, efficiently loading, testing, and unloading models to determine the maximum operational context size.
  - **Inference Engine (`core/inference.py`)**: Updated to use the native client, ensuring models are loaded with their tested, validated context length before running inference.
  - **Model Discovery (`loaders/model_loader.py`)**: The model scanner now uses `lmstudio` to discover all downloaded models, ensuring the local registry is always perfectly in sync with the LM Studio application.

### Added

- **System Path and Data Storage**
  - Implemented robust detection of the LM Studio data directory by reading the `$HOME/.lmstudio-home-pointer` file.
  - All application data, including the model registry, is now stored in a clean, standardized `lmstrix.json` file directly within the located LM Studio data directory.
  - All test logs are stored in a `context_tests` subdirectory within a new `lmstrix` folder in the LM Studio data directory.

- **CLI and API Enhancements**
  - **CLI (`cli/main.py`)**: All commands (`scan`, `list`, `test`, `infer`) have been updated to use the new, refactored core logic, providing a seamless user experience.
  - **Public API (`__init__.py`)**: The high-level `LMStrix` class has been simplified to provide a clean, modern, and programmatic interface to the library's `lmstudio`-native functionality.

### Fixed

- Resolved all previous import and dependency issues related to `litellm`.
- Standardized the data storage location to prevent fragmentation and improve reliability.

## [0.1.0] - 2025-07-24

### Added

- Initial project structure with `src/` layout.
- First implementation of core components using `litellm`.
- Basic CLI and API interfaces.

</document_content>
</document>

<document index="6">
<source>CLAUDE.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1.  **API Layer** (`src/lmstrix/api/`)
    -   `client.py`: Async client for LM Studio server API with retry logic using `tenacity`.
    -   `exceptions.py`: Custom exception hierarchy for better error handling.

2.  **Core Engine** (`src/lmstrix/core/`)
    -   `context_tester.py`: Binary search algorithm to find optimal context window size, with `rich` progress bar integration.
    -   `inference.py`: Handles the inference process, including prompt building.
    -   `models.py`: Model registry with persistence for tracking tested context limits.
    -   `scanner.py`: Discovers and catalogs available LM Studio models.
    -   `prompts.py`: Prompt resolution and template management.
    -   `context.py`: Manages context, including prompt templates and token counting using `tiktoken`.

3.  **Loaders** (`src/lmstrix/loaders/`)
    -   `model_loader.py`: Manages model registry persistence (JSON).
    -   `prompt_loader.py`: Loads prompt templates from TOML files.
    -   `context_loader.py`: Loads context data from text files.

4.  **CLI** (`src/lmstrix/cli/`)
    -   `main.py`: `fire`-based CLI with commands: `scan`, `list`, `test`, `infer`.
    -   Uses `rich` for beautiful terminal output.

### 2.2. Critical Design Patterns

-   **Async-First**: All API operations use `async/await` for high performance.
-   **Retry Logic**: Uses `tenacity` for automatic retries with exponential backoff.
-   **Model Registry**: Persists discovered models and their tested limits to JSON.
-   **Two-Phase Prompts**: Separates prompt template structure from runtime context.
-   **Binary Search**: Efficiently finds maximum context window through targeted testing.

### 2.3. Dependencies

-   `lmstudio-python`: Official LM Studio Python SDK.
-   `httpx`: Async HTTP client.
-   `pydantic`: Data validation and models.
-   `fire`: CLI framework.
-   `rich`: Terminal formatting.
-   `tenacity`: Retry logic.
-   `tiktoken`: Token counting.
-   `loguru`: Logging.

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="7">
<source>GEMINI.md</source>
<document_content>
# AGENT_INSTRUCTIONS.md

This file provides guidance to AI agents when working with code in this repository.

## 1. Commands

### 1.1. Development

- **Run tests**: `pytest` or `hatch run test`
- **Run tests with coverage**: `pytest --cov=src/lmstrix --cov-report=html` or `hatch run test-cov`
- **Run specific test file**: `pytest tests/test_api/test_exceptions.py`
- **Run only unit tests**: `pytest -m "not integration"`

### 1.2. Linting and Formatting

- **Format code**: `hatch run lint:fmt` or `black . && ruff check --fix .`
- **Check style**: `hatch run lint:style` or `ruff check . && black --check --diff .`
- **Type checking**: `hatch run lint:typing` or `mypy --install-types --non-interactive src/lmstrix tests`
- **All linting**: `hatch run lint:all`

### 1.3. Build and Package

- **Build package**: `hatch build` or use `./build.sh` (runs lint, tests, and builds)
- **Install for development**: `pip install -e .`

## 2. Architecture

LMStrix is a Python toolkit for managing and testing LM Studio models with automatic context limit discovery. The core innovation is the **Adaptive Context Optimizer** that uses binary search to find the true operational context window for any model.

### 2.1. Key Components

1. **API Layer** (`src/lmstrix/api/`)
   - `client.py`: Async client for LM Studio server API with retry logic
   - `exceptions.py`: Custom exception hierarchy for better error handling

2. **Core Engine** (`src/lmstrix/core/`)
   - `context_tester.py`: Binary search algorithm to find optimal context window size
   - `inference.py`: Two-phase prompt templating system (structure + content)
   - `models.py`: Model registry with persistence for tracking tested context limits
   - `scanner.py`: Discovers and catalogs available LM Studio models
   - `prompts.py`: Prompt resolution and template management

3. **Loaders** (`src/lmstrix/loaders/`)
   - `model_loader.py`: Manages model registry persistence (JSON)
   - `prompt_loader.py`: Loads prompt templates from TOML files
   - `context_loader.py`: Loads context data from text files

4. **CLI** (`src/lmstrix/cli/`)
   - `main.py`: Fire-based CLI with commands: `scan`, `list`, `optimize`, `infer`
   - Uses Rich for beautiful terminal output

### 2.2. Critical Design Patterns

- **Async-First**: All API operations use async/await for high performance
- **Retry Logic**: Uses tenacity for automatic retries with exponential backoff
- **Model Registry**: Persists discovered models and their tested limits to JSON
- **Two-Phase Prompts**: Separates prompt template structure from runtime context
- **Binary Search**: Efficiently finds maximum context window through targeted testing

### 2.3. Dependencies

- `lmstudio>=1.4.1`: Official LM Studio Python SDK
- `httpx`: Async HTTP client
- `pydantic`: Data validation and models
- `fire`: CLI framework
- `rich`: Terminal formatting
- `tenacity`: Retry logic
- `tiktoken`: Token counting

# Software Development Rules

## 3. Pre-Work Preparation

### 3.1. Before Starting Any Work

- **ALWAYS** read `WORK.md` in the main project folder for work progress
- Read `README.md` to understand the project
- STEP BACK and THINK HEAVILY STEP BY STEP about the task
- Consider alternatives and carefully choose the best option
- Check for existing solutions in the codebase before starting

### 3.2. Project Documentation to Maintain

- `README.md` - purpose and functionality
- `CHANGELOG.md` - past change release notes (accumulative)
- `PLAN.md` - detailed future goals, clear plan that discusses specifics
- `TODO.md` - flat simplified itemized `- [ ]`-prefixed representation of `PLAN.md`
- `WORK.md` - work progress updates

## 4. General Coding Principles

### 4.1. Core Development Approach

- Iterate gradually, avoiding major changes
- Focus on minimal viable increments and ship early
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Check often the coherence of the code you're writing with the rest of the code
- Analyze code line-by-line

### 4.2. Code Quality Standards

- Use constants over magic numbers
- Write explanatory docstrings/comments that explain what and WHY
- Explain where and how the code is used/referred to elsewhere
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures

## 5. Tool Usage (When Available)

### 5.1. Additional Tools

- If we need a new Python project, run `curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync`
- Use `tree` CLI app if available to verify file locations
- Check existing code with `.venv` folder to scan and consult dependency source code
- Run `DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"` to get a condensed snapshot of the codebase into `llms.txt`

## 6. File Management

### 6.1. File Path Tracking

- **MANDATORY**: In every source file, maintain a `this_file` record showing the path relative to project root
- Place `this_file` record near the top:
- As a comment after shebangs in code files
- In YAML frontmatter for Markdown files
- Update paths when moving files
- Omit leading `./`
- Check `this_file` to confirm you're editing the right file

## 7. Python-Specific Guidelines

### 7.1. PEP Standards

- PEP 8: Use consistent formatting and naming, clear descriptive names
- PEP 20: Keep code simple and explicit, prioritize readability over cleverness
- PEP 257: Write clear, imperative docstrings
- Use type hints in their simplest form (list, dict, | for unions)

### 7.2. Modern Python Practices

- Use f-strings and structural pattern matching where appropriate
- Write modern code with `pathlib`
- ALWAYS add "verbose" mode loguru-based logging & debug-log
- Use `uv add` 
- Use `uv pip install` instead of `pip install`
- Prefix Python CLI tools with `python -m` (e.g., `python -m pytest`)

### 7.3. CLI Scripts Setup

For CLI Python scripts, use `fire` & `rich`, and start with:

```python
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

### 7.4. Post-Edit Python Commands

```bash
fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

## 8. Post-Work Activities

### 8.1. Critical Reflection

- After completing a step, say "Wait, but" and do additional careful critical reasoning
- Go back, think & reflect, revise & improve what you've done
- Don't invent functionality freely
- Stick to the goal of "minimal viable next version"

### 8.2. Documentation Updates

- Update `WORK.md` with what you've done and what needs to be done next
- Document all changes in `CHANGELOG.md`
- Update `TODO.md` and `PLAN.md` accordingly

## 9. Work Methodology

### 9.1. Virtual Team Approach

Be creative, diligent, critical, relentless & funny! Lead two experts:

- **"Ideot"** - for creative, unorthodox ideas
- **"Critin"** - to critique flawed thinking and moderate for balanced discussions

Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

### 9.2. Continuous Work Mode

- Treat all items in `PLAN.md` and `TODO.md` as one huge TASK
- Work on implementing the next item
- Review, reflect, refine, revise your implementation
- Periodically check off completed issues
- Continue to the next item without interruption

## 10. Special Commands

### 10.1. `/plan` Command - Transform Requirements into Detailed Plans

When I say "/plan [requirement]", you must:

1. **DECONSTRUCT** the requirement:

- Extract core intent, key features, and objectives
- Identify technical requirements and constraints
- Map what's explicitly stated vs. what's implied
- Determine success criteria

2. **DIAGNOSE** the project needs:

- Audit for missing specifications
- Check technical feasibility
- Assess complexity and dependencies
- Identify potential challenges

3. **RESEARCH** additional material: 

- Repeatedly call the `perplexity_ask` and request up-to-date information or additional remote context
- Repeatedly call the `context7` tool and request up-to-date software package documentation
- Repeatedly call the `codex` tool and request additional reasoning, summarization of files and second opinion

4. **DEVELOP** the plan structure:

- Break down into logical phases/milestones
- Create hierarchical task decomposition
- Assign priorities and dependencies
- Add implementation details and technical specs
- Include edge cases and error handling
- Define testing and validation steps

5. **DELIVER** to `PLAN.md`:

- Write a comprehensive, detailed plan with:
 - Project overview and objectives
 - Technical architecture decisions
 - Phase-by-phase breakdown
 - Specific implementation steps
 - Testing and validation criteria
 - Future considerations
- Simultaneously create/update `TODO.md` with the flat itemized `- [ ]` representation

**Plan Optimization Techniques:**

- **Task Decomposition:** Break complex requirements into atomic, actionable tasks
- **Dependency Mapping:** Identify and document task dependencies
- **Risk Assessment:** Include potential blockers and mitigation strategies
- **Progressive Enhancement:** Start with MVP, then layer improvements
- **Technical Specifications:** Include specific technologies, patterns, and approaches

### 10.2. `/report` Command

1. Read all `./TODO.md` and `./PLAN.md` files
2. Analyze recent changes
3. Document all changes in `./CHANGELOG.md`
4. Remove completed items from `./TODO.md` and `./PLAN.md`
5. Ensure `./PLAN.md` contains detailed, clear plans with specifics
6. Ensure `./TODO.md` is a flat simplified itemized representation

### 10.3. `/work` Command

1. Read all `./TODO.md` and `./PLAN.md` files and reflect
2. Write down the immediate items in this iteration into `./WORK.md`
3. Work on these items
4. Think, contemplate, research, reflect, refine, revise
5. Be careful, curious, vigilant, energetic
6. Verify your changes and think aloud
7. Consult, research, reflect
8. Periodically remove completed items from `./WORK.md`
9. Tick off completed items from `./TODO.md` and `./PLAN.md`
10. Update `./WORK.md` with improvement tasks
11. Execute `/report`
12. Continue to the next item

## 11. Additional Guidelines

- Ask before extending/refactoring existing code that may add complexity or break things
- Work tirelessly without constant updates when in continuous work mode
- Only notify when you've completed all `PLAN.md` and `TODO.md` items

## 12. Command Summary

- `/plan [requirement]` - Transform vague requirements into detailed `PLAN.md` and `TODO.md`
- `/report` - Update documentation and clean up completed tasks
- `/work` - Enter continuous work mode to implement plans
- You may use these commands autonomously when appropriate

</document_content>
</document>

<document index="8">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</document_content>
</document>

<document index="9">
<source>PLAN.md</source>
<document_content>
# LMStrix Current Development Plan


## ACTIVE: Issue #302 - Inference Output Mismatch (Major Progress Made)

### Current Status
**Major diagnostic improvements completed in v1.0.66:**
- ✅ Added comprehensive diagnostic logging for all inference parameters
- ✅ Updated default temperature from 0.7 to 0.8 to match LM Studio GUI
- ✅ Added CLI parameters for configuring inference settings
- ✅ Created diagnostic tools for parameter comparison
- ✅ Fixed table display and response preview issues

### Remaining Issues to Investigate
1. **Context Length**: GUI uses full 131072, CLI may reduce to 65536
2. **max_predict**: GUI uses -1 (unlimited), CLI calculates specific values
3. **Stop Token Configuration**: May differ between GUI and CLI
4. **Prompt Formatting**: Possible differences in chat template application

## Current Top Priority

### Issue #302 - Inference Output Mismatch (FINAL PHASE)
**Priority: HIGH - Core diagnostic work completed**

#### Remaining Implementation Steps:

##### Step 1: Context Length Investigation ⚠️
- Fix context reduction from full model context to reduced values
- Use full model context by default unless explicitly limited
- Add warning when context is automatically reduced
- Test with full context vs reduced context

##### Step 2: Stop Token Configuration ⚠️
- Compare stop token configuration between GUI and CLI
- Test with stop tokens disabled to rule out early termination
- Verify chat template application doesn't introduce stop tokens

##### Step 3: Final Validation ⚠️
- Run side-by-side comparison with identical parameters
- Verify token counts match between lmstrix and LM Studio
- Create regression test to prevent future issues

### Issue #105 - Adam.toml Simplification (Next Priority)
**Priority: Medium** (after Issue #302 is resolved)
- Simplify adam.toml structure to use flat format instead of nested groups
- Add --text and --text_file parameters to infer command for direct text input
- Update all prompt examples to use simplified approach
- Ensure backward compatibility with existing TOML files


## Future Development Phases

### Phase A: Core Simplification (2-3 weeks)
1. **Configuration Unification**
   - Create utils/config.py for centralized configuration handling
   - Consolidate path handling functions
   - Remove redundant configuration code

2. **Error Handling Standardization**
   - Review and simplify custom exception hierarchy
   - Standardize error messages across codebase
   - Implement consistent logging patterns

### Phase B: CLI Enhancement (1-2 weeks)
1. **Command Improvements**
   - Enhance `scan` command with better progress reporting
   - Improve `list` command with filtering and sorting options
   - Add `reset` command for clearing model test data

2. **User Experience**
   - Better error messages with helpful suggestions
   - Improved help text and documentation
   - Enhanced progress indicators for long-running operations

### Phase C: Testing & Documentation (1 week)
1. **Test Suite Completion**
   - Ensure >90% test coverage maintained
   - Add integration tests for new features
   - Performance benchmarking of improvements

2. **Documentation Updates**
   - Update README.md with latest features
   - Create comprehensive CLI reference
   - Update examples to demonstrate new capabilities

## Success Metrics

- **Functionality**: All existing CLI commands work without regression
- **Performance**: Model loading and inference speed improvements
- **Usability**: Cleaner, more informative user interface
- **Maintainability**: Reduced complexity, better code organization
- **Documentation**: Up-to-date and comprehensive user guides


## Long-term Vision

The goal is to make LMStrix the most user-friendly and efficient tool for managing and testing LM Studio models, with:
- Real-time streaming inference with progress feedback and hang detection
- Intuitive CLI interface with beautiful, informative output
- Smart model management with automatic optimization
- Batch processing capabilities for productivity workflows
- Comprehensive testing capabilities with clear results
- Excellent developer experience with clean, well-documented code
</document_content>
</document>

<document index="10">
<source>README.md</source>
<document_content>
# LMStrix

LMStrix is a professional Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and Python API for managing, testing, and running local language models, with a standout feature: **Adaptive Context Optimization**.

## Key Features

- **🔍 Automatic Context Discovery**: Binary search algorithm to find the true operational context limit of any model
- **📊 Beautiful Verbose Logging**: Enhanced stats display with emojis showing inference metrics, timing, and token usage
- **🚀 Smart Model Management**: Models persist between calls to reduce loading overhead
- **🎯 Flexible Inference Engine**: Run inference with powerful prompt templating and percentage-based output control
- **📋 Comprehensive Model Registry**: Track models, their context limits, and test results with JSON persistence
- **🛡️ Safety Controls**: Configurable thresholds and fail-safes to prevent system crashes
- **💻 Rich CLI Interface**: Beautiful terminal output with progress indicators and formatted tables
- **📈 Compact Test Output**: Live-updating tables show test progress without verbose logging clutter

## Installation

```bash
# Using pip
pip install lmstrix

# Using uv (recommended)
uv pip install lmstrix
```

## Quick Start

### Command-Line Interface

```bash
# Scan for available models in LM Studio
lmstrix scan

# List all models with their context limits and test status
lmstrix list

# Test context limit for a specific model
lmstrix test llama-3.2-3b-instruct

# Test all untested models with safety threshold
lmstrix test --all --threshold 102400

# Run inference with enhanced verbose logging
lmstrix infer "What is the capital of Poland?" -m llama-3.2-3b-instruct --verbose

# Run inference with percentage-based output tokens
lmstrix infer "Explain quantum computing" -m llama-3.2-3b-instruct --out_ctx "25%"

# Use file-based prompts with templates
lmstrix infer summary -m llama-3.2-3b-instruct --file_prompt adam.toml --text_file document.txt

# Direct text input for prompts
lmstrix infer "Summarize: {{text}}" -m llama-3.2-3b-instruct --text "Your content here"
```

### Enhanced Verbose Output

When using `--verbose`, LMStrix provides comprehensive statistics:

```
════════════════════════════════════════════════════════════
🤖 MODEL: llama-3.2-3b-instruct
🔧 CONFIG: maxTokens=26214, temperature=0.7
📝 PROMPT (1 lines, 18 chars): Capital of Poland?
════════════════════════════════════════════════════════════
⠸ Running inference...
════════════════════════════════════════════════════════════
📊 INFERENCE STATS
⚡ Time to first token: 0.82s
⏱️  Total inference time: 11.66s
🔢 Predicted tokens: 338
📝 Prompt tokens: 5
🎯 Total tokens: 343
🚀 Tokens/second: 32.04
🛑 Stop reason: eosFound
════════════════════════════════════════════════════════════
```

### Python API

```python
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.core.inference_manager import InferenceManager

# Load model registry
registry = load_model_registry()

# List available models
models = registry.list_models()
print(f"Available models: {len(models)}")

# Run inference
manager = InferenceManager(verbose=True)
result = manager.infer(
    model_id="llama-3.2-3b-instruct",
    prompt="What is the meaning of life?",
    out_ctx=100,
    temperature=0.7
)

if result["succeeded"]:
    print(f"Response: {result['response']}")
    print(f"Tokens used: {result['tokens_used']}")
    print(f"Time: {result['inference_time']:.2f}s")
```

## Context Testing & Optimization

LMStrix uses a sophisticated binary search algorithm to discover true model context limits:

### Safety Features
- **Threshold Protection**: Configurable maximum context size to prevent system crashes
- **Progressive Testing**: Starts with small contexts and increases safely
- **Persistent Results**: Saves test results to avoid re-testing

### Testing Commands
```bash
# Test specific model
lmstrix test llama-3.2-3b-instruct

# Test all models with custom threshold
lmstrix test --all --threshold 65536

# Test at specific context size
lmstrix test --all --ctx 32768

# Reset and re-test a model
lmstrix test llama-3.2-3b-instruct --reset

# Test with custom prompt
lmstrix test llama-3.2-3b-instruct --prompt "What is 2+2?"

# Test with file-based prompt
lmstrix test llama-3.2-3b-instruct --file_prompt test_prompt.txt
```

### Compact Output (Default)
When running without `--verbose`, tests display a clean, live-updating table:
```
Model                                   Context      Status
llama-3.2-3b-instruct                   32,768      Testing...
→
Model                                   Context      Status  
llama-3.2-3b-instruct                   32,768      ✓ Success
```

For batch testing with `--all`, a progress column is added to track multiple models.

## Model Management

### Registry Commands
```bash
# Scan for new models
lmstrix scan --verbose

# List models with different sorting
lmstrix list --sort size        # Sort by size
lmstrix list --sort ctx         # Sort by tested context
lmstrix list --show json        # Export as JSON

# Check system health
lmstrix health --verbose
```

### Model Persistence
Models stay loaded between inference calls for improved performance:
- When no explicit context is specified, models remain loaded
- Last-used model is remembered for subsequent calls
- Explicit context changes trigger model reloading

## Prompt Templating

LMStrix supports flexible prompt templating with TOML files:

```toml
# adam.toml
[aps]
prompt = """
You are an AI assistant skilled in Abstractive Proposition Segmentation.
Convert the following text: {{text}}
"""

[summary] 
prompt = "Create a comprehensive summary: {{text}}"
```

Use with CLI:
```bash
lmstrix infer aps --file_prompt adam.toml --text "Your text here"
lmstrix infer summary --file_prompt adam.toml --text_file document.txt
```

## Development

```bash
# Clone repository
git clone https://github.com/twardoch/lmstrix.git
cd lmstrix

# Install for development
pip install -e ".[dev]"

# Run tests
pytest

# Run linting
hatch run lint:all
```

## Project Structure

```
src/lmstrix/
├── cli/main.py              # CLI interface
├── core/
│   ├── inference_manager.py # Unified inference engine
│   ├── models.py            # Model registry
│   └── context_tester.py    # Context limit testing
├── api/client.py            # LM Studio API client
├── loaders/                 # Data loading utilities
└── utils/                   # Helper utilities
```

## Features in Detail

### Adaptive Context Optimizer
- Binary search algorithm for efficient context limit discovery
- Safety thresholds to prevent system crashes
- Automatic persistence of test results
- Resume capability for interrupted tests

### Enhanced Logging
- Beautiful emoji-rich output in verbose mode
- Comprehensive inference statistics
- Progress indicators for long operations
- Clear error messages with context

### Smart Model Management
- Automatic model discovery from LM Studio
- Persistent registry with JSON storage
- Model state tracking (loaded/unloaded)
- Batch operations for multiple models

## Requirements

- Python 3.11+
- LM Studio installed and configured
- Models downloaded in LM Studio

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions welcome! Please read our contributing guidelines and submit pull requests for any improvements.
</document_content>
</document>

<document index="11">
<source>TESTING.md</source>
<document_content>
# LMStrix Test Suite

## Overview

The LMStrix test suite provides comprehensive coverage for all modules using pytest. The tests are organized into unit tests, integration tests, and end-to-end tests.

## Test Structure

```
tests/
├── conftest.py              # Shared fixtures and configuration
├── run_tests.py             # Simple test runner script
├── test_api/                # API layer tests
│   ├── test_client.py       # LMStudioClient tests
│   └── test_exceptions.py   # Custom exception tests
├── test_core/               # Core module tests
│   ├── test_context_tester.py  # Context optimization tests
│   ├── test_inference.py    # Inference engine tests
│   ├── test_models.py       # Model and registry tests
│   ├── test_prompts.py      # Prompt resolution tests
│   └── test_scanner.py      # Model scanner tests
├── test_loaders/            # Loader tests
│   ├── test_context_loader.py   # Context file loading tests
│   ├── test_model_loader.py     # Model loader tests
│   └── test_prompt_loader.py    # Prompt loader tests
├── test_utils/              # Utility tests
│   └── test_paths.py        # Path utility tests
├── test_integration/        # Integration tests
│   └── test_cli_integration.py  # CLI integration tests
└── test_e2e/                # End-to-end tests (to be added)
```

## Running Tests

### Install Test Dependencies
```bash
pip install pytest pytest-asyncio pytest-cov pytest-mock
```

### Run All Tests
```bash
pytest
```

### Run Specific Test File
```bash
pytest tests/test_api/test_exceptions.py
```

### Run with Coverage
```bash
pytest --cov=src/lmstrix --cov-report=html
```

### Run Only Unit Tests
```bash
pytest -m "not integration"
```

## Key Test Features

### 1. Comprehensive Mocking
- All external dependencies (LM Studio API, file system) are mocked
- Tests run independently without requiring LM Studio installation

### 2. Async Test Support
- Uses pytest-asyncio for testing async methods
- Proper async fixtures and test decorators

### 3. Fixture Organization
- Common fixtures in conftest.py
- Mock data for models, prompts, and contexts
- Temporary directories for file operations

### 4. Test Categories

#### Unit Tests
- Test individual classes and functions in isolation
- Mock all external dependencies
- Fast execution
- High code coverage

#### Integration Tests
- Test interaction between modules
- Mock only external services (LM Studio API)
- Verify end-to-end workflows

#### End-to-End Tests (Planned)
- Test complete user scenarios
- May require actual LM Studio instance
- Focus on context optimization workflow

## Coverage Goals

- Target: >80% code coverage
- Critical paths: 100% coverage
  - Context optimization algorithm
  - Model registry operations
  - API client error handling

## Test Examples

### Testing Context Optimization
```python
@pytest.mark.asyncio
async def test_find_optimal_context_simple(mock_client, mock_llm):
    """Test finding optimal context with simple scenario."""
    # Mock responses: succeed up to 4096, fail above
    async def mock_completion(llm, prompt, **kwargs):
        if len(prompt) <= 4096:
            return Mock(content="4")
        else:
            raise InferenceError("test-model", "Too long")
    
    mock_client.acompletion = AsyncMock(side_effect=mock_completion)
    
    tester = ContextTester(mock_client)
    model = Model(id="test-model", path="/path/to/model.gguf", 
                  size_bytes=1000000, ctx_in=8192)
    
    optimal_size, loadable_size, test_log = await tester.find_optimal_context(model)
    
    assert 3000 < optimal_size <= 4096
    assert loadable_size > optimal_size
```

### Testing Model Registry
```python
def test_registry_save_and_load(tmp_path, sample_model_data):
    """Test saving and loading models."""
    registry = ModelRegistry(tmp_path / "models.json")
    
    model = Model(**sample_model_data)
    registry.update_model("test-model", model)
    
    # Load in new registry instance
    new_registry = ModelRegistry(tmp_path / "models.json")
    
    loaded_model = new_registry.get_model("test-model")
    assert loaded_model.id == "test-model"
```

## CI/CD Integration

The test suite is designed to run in CI/CD pipelines:

1. No external dependencies required
2. All tests use mocking
3. Deterministic results
4. Fast execution (<1 minute)

## Future Enhancements

1. Add performance benchmarks
2. Add mutation testing
3. Create test data generators
4. Add property-based tests for complex algorithms
5. Integration with actual LM Studio test instance
</document_content>
</document>

<document index="12">
<source>TODO.md</source>
<document_content>
# LMStrix TODO List

## Current Active Work

### Priority 1: Issue #302 - Inference Output Mismatch (Final Phase)

**Major diagnostic work completed in v1.0.66. Remaining items:**

- [ ] Fix context length handling (GUI uses full 131072, CLI reduces to 65536)
- [ ] Investigate stop token configuration differences between GUI and CLI
- [ ] Compare maxTokens calculation: GUI uses -1 (unlimited), CLI calculates specific values
- [ ] Run side-by-side comparison with identical parameters
- [ ] Create regression test to prevent future recurrence

### Priority 2: Issue #105 - Adam.toml Simplification

- [ ] Update prompt loader to handle flat TOML structure (no nested groups)
- [ ] Add --text and --text_file parameters to infer command for direct text input
- [ ] Add backward compatibility for existing nested TOML files
- [ ] Update examples to use simplified flat format
- [ ] Create migration guide for existing users

## Future Development (Lower Priority)

### Code Quality Improvements

- [ ] Ruff linting improvements (143 errors identified)
  - [ ] Fix BLE001 blind exception catching (46 occurrences)
  - [ ] Fix A002 builtin shadowing (3 occurrences)
  - [ ] Address TRY300 statement placement issues (8 occurrences)
  - [ ] Remove commented-out code (ERA001)
- [ ] Add comprehensive type hints to public APIs
- [ ] Ensure all functions have proper docstrings
- [ ] Standardize error handling patterns

### Configuration and Utilities

- [ ] Create utils/config.py for centralized configuration handling
- [ ] Consolidate path handling functions
- [ ] Improve model loading optimization and reuse detection

### CLI Enhancement

- [ ] Add `reset` command for clearing model test data
- [ ] Add `health` command for system diagnostics
- [ ] Enhance `scan` and `list` commands with better filtering

### Testing and Documentation

- [ ] Ensure >90% test coverage maintained
- [ ] Update README.md with latest features
- [ ] Create comprehensive CLI reference
- [ ] Add integration tests for new features


</document_content>
</document>

<document index="13">
<source>WORK.md</source>
<document_content>
# Current Work Progress

## 1. Recently Completed Work

### 1.1. Issue #307 - Streaming Inference Support ✅

#### 1.1.1. What was done:
1. **Added streaming support to LMStudioClient** (`src/lmstrix/api/client.py`):
   - Implemented `stream_completion()` method using lmstudio SDK's `complete_stream()`
   - Added token-by-token callbacks with `on_prediction_fragment` and `on_first_token`
   - Implemented timeout watchdog (default 120s) to detect stalled generations
   - Added streaming statistics (tokens/second, time to first token)

2. **Extended InferenceEngine and InferenceManager** with streaming:
   - Added `stream_infer()` method to both classes
   - Maintains same model loading/reuse logic as regular inference
   - Supports all existing parameters plus streaming-specific ones

3. **Updated CLI with --stream flag**:
   - Added `--stream` and `--stream-timeout` parameters to `infer` command
   - Tokens are displayed in real-time to stdout as they are generated
   - Maintains backward compatibility - regular inference still works

#### 1.1.2. How to use:
```bash
# Regular inference (blocking)
lmstrix infer "Hello world" -m model-id

# Streaming inference (real-time)
lmstrix infer "Hello world" -m model-id --stream

# With custom timeout
lmstrix infer "Hello world" -m model-id --stream --stream-timeout 180
```

### 1.2. Issue #306 - Batch Processing Tool ✅

#### 1.2.1. What was done:
1. **Created adamall.py batch processing tool** (`_keep_this/adam/adamall.py`):
   - Processes 6 specific prompts: `think,aps`, `think,humanize`, `think,tldr`, `think,tts_optimize`, `translate`, `tldr`
   - Smart model management - reuses loaded models when possible
   - Skips existing outputs for resumable processing
   - Safe filename generation using pathvalidate
   - Error capture to output files on failure

2. **Key features implemented**:
   - Loads models with 50% context, runs inference with 90% of max context
   - Sorts models by size (descending) for optimal processing order
   - Progress tracking with percentage and ETA
   - Comprehensive error handling and logging

#### 1.2.2. How to use:
```bash
cd _keep_this/adam
python adamall.py
```

Output files will be generated in `_keep_this/adam/out/` with names like:
- `think_aps--model_name.txt`
- `translate--model_name.txt`

## 2. ACTIVE: Issue #302 - Fix Inference Output Mismatch

### 2.1. Problem Summary
When running the same translation prompt:
- **LM Studio GUI**: Produces proper Polish translation (639 tokens)
- **lmstrix CLI**: Only outputs `</translate>` (4 tokens)

### 2.2. Root Cause Analysis
Found several configuration differences:
1. Temperature: GUI=0.8, CLI=0.7 → Updated default to 0.8 ✅
2. top_k: GUI=20, CLI=40 → Now configurable via CLI ✅
3. Context: GUI=131072, CLI=65536 (reduced)
4. max_predict: GUI=-1, CLI=117964
5. Stop tokens configuration may differ

### 2.3. Current Work Items

#### 2.3.1. Add Diagnostic Logging ✅
- [x] Log exact prompt with escape sequences visible
- [x] Log all inference parameters in detail
- [x] Add comparison with LM Studio defaults
- [x] Log stop token configuration

#### 2.3.2. Parameter Alignment
- [x] Change default temperature to 0.8
- [x] Add CLI flags for inference parameters
- [ ] Fix maxTokens calculation
- [ ] Add stop token configuration

#### 2.3.3. Context Length Fix
- [ ] Fix context reduction issue
- [ ] Use full model context by default
- [ ] Add warning for context reduction

#### 2.3.4. Testing
- [ ] Compare with LM Studio output
- [ ] Verify token counts match
- [ ] Test translation quality

### 2.4. Implementation Progress
Added diagnostic logging and streaming support. Need to investigate stop token issue next...

### 2.5. Rich Table Width Fix ✅
Updated all Rich tables to take 100% console width:
- Added `expand=True` to all Table constructors (9 tables total)
- Removed all fixed `width` parameters from `add_column()` calls
- Tables now automatically use full terminal width for better readability

### 2.6. Response Preview Enhancement ✅
Fixed response preview display in `lmstrix test` command:
- **Problem**: Response preview was truncated to only 20 characters, showing "||Custom prompt respon||" instead of meaningful content
- **Solution**: Created `_format_response_preview()` helper function that:
  - Increases preview length from 20 to 60 characters (configurable)
  - Cleans up whitespace and newlines for better table display
  - Adds ellipsis ("...") when truncated
  - Maintains "❌" for failed responses
- **Result**: Users now see more meaningful response content like "||Custom prompt response for testing context limits and model inference...||"

### 2.7. Test Suite Fixes (Priority 0) ✅
All critical AttributeError issues have been resolved:
1. **Model.validate_integrity()** ✅
2. **PromptResolver methods** ✅  
3. **ContextTester methods** ✅
4. **ModelScanner.sync_with_registry()** ✅

### 2.7. Issues 201-204 ✅
- Model persistence between calls
- Beautiful enhanced logging
- Fixed model lookup for paths/IDs
- Comprehensive inference statistics

## 3. Next Steps After Issue #302
1. Complete remaining test fixes
2. Issue #105 - Adam.toml simplification
3. Context testing streamlining
4. Model loading optimization
</document_content>
</document>

<document index="14">
<source>build.sh</source>
<document_content>
#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# Define log files
LOG_FILE="build.log.txt"
ERR_FILE="build.err.txt"

# Clean up previous build artifacts and logs
echo "Cleaning up previous build artifacts and logs..."
rm -f $LOG_FILE $ERR_FILE
rm -rf dist/

# Redirect stdout and stderr to log files
exec > >(tee -a "$LOG_FILE") 2> >(tee -a "$ERR_FILE" >&2)

# Lint and format
echo "Running linter and formatter..."
hatch run lint:all

# Run tests
echo "Running tests with coverage..."
hatch run cov

# Build the package
echo "Building the package..."
hatch build

echo "Build process completed successfully!"
echo "Check $LOG_FILE and $ERR_FILE for details."

</document_content>
</document>

<document index="15">
<source>cleanup.txt</source>
<document_content>
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=5,col=1,endLine=5,endColumn=35::test_model_out_ctx.py:5:1: ERA001 Found commented-out code
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=61,col=12,endLine=61,endColumn=21::test_model_out_ctx.py:61:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=63,col=9,endLine=63,endColumn=25::test_model_out_ctx.py:63:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=87,col=12,endLine=87,endColumn=21::test_model_out_ctx.py:87:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=98,col=16,endLine=98,endColumn=25::test_model_out_ctx.py:98:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py,line=107,col=12,endLine=107,endColumn=21::test_model_out_ctx.py:107:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/get_out_ctx.py,line=5,col=1,endLine=5,endColumn=28::get_out_ctx.py:5:1: ERA001 Found commented-out code
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=106,col=9,endLine=106,endColumn=20::src/lmstrix/loaders/model_loader.py:106:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=186,col=9,endLine=186,endColumn=25::src/lmstrix/loaders/model_loader.py:186:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py,line=361,col=9,endLine=361,endColumn=20::src/lmstrix/loaders/model_loader.py:361:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/test_streaming.py,line=47,col=12,endLine=47,endColumn=21::test_streaming.py:47:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=6,col=1,endLine=6,endColumn=33::debug_test_logic.py:6:1: ERA001 Found commented-out code
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=47,col=9,endLine=47,endColumn=15::debug_test_logic.py:47:9: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=47,col=9,endLine=48,endColumn=17::debug_test_logic.py:47:9: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=117,col=12,endLine=117,endColumn=21::debug_test_logic.py:117:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=119,col=9,endLine=119,endColumn=25::debug_test_logic.py:119:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=137,col=9,endLine=137,endColumn=15::debug_test_logic.py:137:9: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=137,col=9,endLine=138,endColumn=17::debug_test_logic.py:137:9: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=180,col=12,endLine=180,endColumn=21::debug_test_logic.py:180:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=208,col=13,endLine=208,endColumn=19::debug_test_logic.py:208:13: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=208,col=13,endLine=209,endColumn=21::debug_test_logic.py:208:13: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_test_logic.py,line=232,col=16,endLine=232,endColumn=25::debug_test_logic.py:232:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=602,col=9,endLine=602,endColumn=13::src/lmstrix/api/main.py:602:9: ARG002 Unused method argument: `sort`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=670,col=20,endLine=670,endColumn=29::src/lmstrix/api/main.py:670:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=834,col=24,endLine=834,endColumn=33::src/lmstrix/api/main.py:834:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (SIM108),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=848,col=17,endLine=854,endColumn=51::src/lmstrix/api/main.py:848:17: SIM108 Use ternary operator `pairs = dict_params.split(",") if "," in dict_params else dict_params.split(",")` instead of `if`-`else`-block
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=849,col=21,endLine=849,endColumn=56::src/lmstrix/api/main.py:849:21: ERA001 Found commented-out code
::error title=Ruff (PLW2901),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=857,col=21,endLine=857,endColumn=25::src/lmstrix/api/main.py:857:21: PLW2901 `for` loop variable `pair` overwritten by assignment target
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=874,col=24,endLine=874,endColumn=33::src/lmstrix/api/main.py:874:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=957,col=20,endLine=957,endColumn=29::src/lmstrix/api/main.py:957:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=1060,col=20,endLine=1060,endColumn=29::src/lmstrix/api/main.py:1060:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py,line=1202,col=16,endLine=1202,endColumn=25::src/lmstrix/api/main.py:1202:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py,line=3,col=1,endLine=3,endColumn=49::src/lmstrix/core/concrete_config.py:3:1: ERA001 Found commented-out code
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py,line=126,col=13,endLine=126,endColumn=24::src/lmstrix/core/concrete_config.py:126:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=3,col=1,endLine=3,endColumn=40::src/lmstrix/utils/state.py:3:1: ERA001 Found commented-out code
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=26,col=20,endLine=26,endColumn=29::src/lmstrix/utils/state.py:26:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py,line=35,col=16,endLine=35,endColumn=25::src/lmstrix/utils/state.py:35:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PTH123),file=/Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py,line=53,col=10,endLine=53,endColumn=14::examples/python/prompt_templates_demo.py:53:10: PTH123 `open()` should be replaced by `Path.open()`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=6,col=1,endLine=6,endColumn=32::debug_inference.py:6:1: ERA001 Found commented-out code
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=75,col=9,endLine=75,endColumn=15::debug_inference.py:75:9: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=75,col=9,endLine=76,endColumn=17::debug_inference.py:75:9: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=134,col=16,endLine=134,endColumn=25::debug_inference.py:134:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=137,col=9,endLine=137,endColumn=20::debug_inference.py:137:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=139,col=12,endLine=139,endColumn=21::debug_inference.py:139:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=142,col=9,endLine=142,endColumn=25::debug_inference.py:142:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=192,col=16,endLine=192,endColumn=25::debug_inference.py:192:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=195,col=9,endLine=195,endColumn=20::debug_inference.py:195:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=197,col=12,endLine=197,endColumn=21::debug_inference.py:197:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=200,col=9,endLine=200,endColumn=25::debug_inference.py:200:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (ANN201),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=206,col=5,endLine=206,endColumn=34::debug_inference.py:206:5: ANN201 Missing return type annotation for public function `test_lmstrix_inference_engine`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=221,col=13,endLine=221,endColumn=58::debug_inference.py:221:13: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=245,col=9,endLine=245,endColumn=32::debug_inference.py:245:9: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=247,col=12,endLine=247,endColumn=21::debug_inference.py:247:12: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (PLC0415),file=/Users/Shared/lmstudio/lmstrix/debug_inference.py,line=250,col=9,endLine=250,endColumn=25::debug_inference.py:250:9: PLC0415 `import` should be at the top-level of a file
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=122,col=13,endLine=122,endColumn=46::src/lmstrix/core/inference.py:122:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=124,col=16,endLine=124,endColumn=25::src/lmstrix/core/inference.py:124:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=132,col=24,endLine=132,endColumn=33::src/lmstrix/core/inference.py:132:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=234,col=20,endLine=234,endColumn=29::src/lmstrix/core/inference.py:234:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=339,col=16,endLine=339,endColumn=25::src/lmstrix/core/inference.py:339:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (E722),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=376,col=29,endLine=376,endColumn=35::src/lmstrix/core/inference.py:376:29: E722 Do not use bare `except`
::error title=Ruff (S110),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=376,col=29,endLine=377,endColumn=37::src/lmstrix/core/inference.py:376:29: S110 `try`-`except`-`pass` detected, consider logging the exception
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=401,col=28,endLine=401,endColumn=37::src/lmstrix/core/inference.py:401:28: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py,line=484,col=20,endLine=484,endColumn=29::src/lmstrix/core/inference.py:484:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=64,col=17,endLine=64,endColumn=35::src/lmstrix/utils/context_parser.py:64:17: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (B904),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py,line=73,col=13,endLine=75,endColumn=14::src/lmstrix/utils/context_parser.py:73:13: B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=7,col=1,endLine=7,endColumn=51::src/lmstrix/core/inference_manager.py:7:1: ERA001 Found commented-out code
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=180,col=24,endLine=180,endColumn=33::src/lmstrix/core/inference_manager.py:180:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=202,col=13,endLine=210,endColumn=14::src/lmstrix/core/inference_manager.py:202:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=212,col=16,endLine=212,endColumn=25::src/lmstrix/core/inference_manager.py:212:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=257,col=24,endLine=257,endColumn=33::src/lmstrix/core/inference_manager.py:257:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=316,col=13,endLine=316,endColumn=46::src/lmstrix/core/inference_manager.py:316:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=318,col=16,endLine=318,endColumn=25::src/lmstrix/core/inference_manager.py:318:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=325,col=24,endLine=325,endColumn=33::src/lmstrix/core/inference_manager.py:325:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py,line=454,col=24,endLine=454,endColumn=33::src/lmstrix/core/inference_manager.py:454:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=127,col=20,endLine=127,endColumn=29::src/lmstrix/api/client.py:127:20: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (TRY300),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=155,col=13,endLine=155,endColumn=31::src/lmstrix/api/client.py:155:13: TRY300 Consider moving this statement to an `else` block
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=198,col=11,endLine=198,endColumn=17::src/lmstrix/api/client.py:198:11: ARG002 Unused method argument: `kwargs`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=383,col=9,endLine=383,endColumn=29::src/lmstrix/api/client.py:383:9: ARG002 Unused method argument: `model_context_length`
::error title=Ruff (ARG002),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py,line=392,col=11,endLine=392,endColumn=17::src/lmstrix/api/client.py:392:11: ARG002 Unused method argument: `kwargs`
::error title=Ruff (ERA001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=3,col=1,endLine=3,endColumn=47::src/lmstrix/core/models.py:3:1: ERA001 Found commented-out code
::error title=Ruff (SIM102),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=142,col=9,endLine=144,endColumn=40::src/lmstrix/core/models.py:142:9: SIM102 Use a single `if` statement instead of nested `if` statements
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=220,col=24,endLine=220,endColumn=33::src/lmstrix/core/models.py:220:24: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py,line=226,col=16,endLine=226,endColumn=25::src/lmstrix/core/models.py:226:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (BLE001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=144,col=16,endLine=144,endColumn=25::src/lmstrix/core/context_tester.py:144:16: BLE001 Do not catch blind exception: `Exception`
::error title=Ruff (ANN001),file=/Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py,line=158,col=35,endLine=158,endColumn=40::src/lmstrix/core/context_tester.py:158:35: ANN001 Missing type annotation for function argument `model`
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged
1 file left unchanged

</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/debug_inference.py
# Language: python

import sys
import time
from pathlib import Path
import lmstudio
from lmstrix.api.client import LMStudioClient
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger
import traceback
import traceback
from lmstrix.core.scanner import ModelScanner
import traceback

def print_separator((title: str)) -> None:
    """Print a nice separator."""

def test_direct_lmstudio(()) -> bool | None:
    """Test direct lmstudio calls without lmstrix wrapper."""

def test_lmstrix_client(()) -> bool | None:
    """Test lmstrix client calls."""

def test_lmstrix_inference_engine(()):
    """Test lmstrix inference engine."""

def main(()) -> None:
    """Run all diagnostic tests."""


# File: /Users/Shared/lmstudio/lmstrix/debug_test_logic.py
# Language: python

import sys
import time
from pathlib import Path
import builtins
import contextlib
import lmstudio
from lmstrix.utils.logging import logger
import traceback

def test_both_prompts(()) -> None:
    """Test both standard prompts and show why tests fail."""

def test_with_more_tokens(()) -> None:
    """Test with more output tokens to see if model just needs more space."""

def test_other_models(()) -> None:
    """Test a few other models to see if they behave differently."""

def main(()) -> None:
    """Run all diagnostic tests."""


<document index="16">
<source>docs/_config.yml</source>
<document_content>
title: LMStrix
description: The Unofficial Toolkit for Mastering LM Studio
remote_theme: just-the-docs/just-the-docs

url: https://twardoch.github.io/lmstrix

color_scheme: dark

search_enabled: false

aux_links:
  "LMStrix on GitHub":
    - "https://github.com/twardoch/lmstrix"

plugins:
  - jekyll-remote-theme
</document_content>
</document>

<document index="17">
<source>docs/api.md</source>
<document_content>
---
title: API Reference
---

## Python API Reference

This section provides a detailed reference for the LMStrix Python API.

### The `LMStrix` Class

The main entry point for interacting with the API.

`LMStrix(base_uri: str = "http://localhost:1234/v1")`

- `base_uri`: The base URI of the LM Studio server.

#### Methods

- `async scan_models()`: Scans for available models in LM Studio and updates the local model registry.
- `async list_models()`: Returns a list of `Model` objects from the local registry.
- `async test_model(model_id: str)`: Tests the context limit of a specific model and updates the registry with the result.
- `async infer(prompt: str, model_id: str, **kwargs)`: Runs inference on a model. Any additional keyword arguments are passed to the `complete()` method of the `lmstudio` client.

### The `Model` Class

Represents a model in LM Studio.

#### Attributes

- `id`: The model ID (e.g., `lmstudio-community/gemma-2b-it-GGUF`).
- `context_limit`: The declared context limit of the model, as reported by LM Studio.
- `tested_max_context`: The empirically tested maximum context limit that the model can handle on your machine. `None` if the model has not been tested.
- `context_test_status`: The status of the context test. Can be one of `"passed"`, `"failed"`, or `"not_tested"`.

### The `InferenceResponse` Class

Represents the response from an inference request.

#### Attributes

- `content`: The text content of the model's response.
- `usage`: A dictionary containing token usage information, e.g., `{'prompt_tokens': 10, 'completion_tokens': 20, 'total_tokens': 30}`.

</document_content>
</document>

<document index="18">
<source>docs/how-it-works.md</source>
<document_content>
---
title: How It Works
---

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **adaptive testing algorithm** (enhanced in v1.1):

1. **Initial Verification**: Tests at 1,024 tokens to ensure the model loads properly
2. **Threshold Test**: Tests at min(threshold, declared_max) where threshold defaults to 102,400 tokens
   - This prevents system crashes from attempting very large context sizes
3. **Adaptive Search**:
   - If the threshold test succeeds and is below the declared max: incrementally increases by 10,240 tokens until failure
   - If the threshold test fails: performs binary search between 1,024 and the failed size
4. **Progress Tracking**: Saves results after each test, allowing resumption if interrupted

**Batch Testing Optimization** (new in v1.1):
When testing multiple models with `--all`, LMStrix now:
- Sorts models by declared context size (ascending)
- Tests in passes to minimize model loading/unloading
- Excludes failed models from subsequent passes
- Provides detailed progress with Rich table output

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

### The `test` Command

The `test` command is the heart of the context optimization process. When you run `lmstrix test <model-id>`, it performs the binary search algorithm described above.

The command saves the results of the test to a local registry, so you only need to test each model once.

</document_content>
</document>

<document index="19">
<source>docs/index.md</source>
<document_content>
---
---

# LMStrix: The Unofficial Toolkit for Mastering LM Studio

LMStrix is a professional, installable Python toolkit designed to supercharge your interaction with [LM Studio](https://lmstudio.ai/). It provides a powerful command-line interface (CLI) and a clean Python API for managing, testing, and running local language models, with a standout feature: the **Adaptive Context Optimizer**.

## Why LMStrix? The Problem it Solves

Working with local LLMs via LM Studio is powerful, but it comes with challenges:

1.  **The Context Window Mystery**: What's the *true* maximum context a model can handle on your machine? Advertised context lengths are often theoretical. The practical limit depends on your hardware, the model's architecture, and LM Studio's own overhead. Finding this limit manually is a tedious, frustrating process of trial and error.
2.  **Repetitive Workflows**: Managing models, crafting prompts, and running inference often involves repetitive boilerplate code or manual steps in the LM Studio UI.
3.  **Lack of Programmatic Control**: The UI is great for exploration, but developers building applications on top of local LLMs need a robust, scriptable interface for automation and integration.

LMStrix solves these problems by providing a seamless, developer-friendly toolkit that automates the tedious parts and lets you focus on building.

## How It Works: The Adaptive Context Optimizer

The core innovation in LMStrix is its ability to **automatically discover the maximum operational context length** for any model loaded in LM Studio.

It uses a sophisticated **binary search algorithm**:
1.  It starts with a wide range for the possible context size.
2.  It sends a specially crafted prompt to the model, progressively increasing the amount of "filler" text.
3.  It analyzes the model's response (or lack thereof) to see if it successfully processed the context.
4.  By repeatedly narrowing the search range, it quickly pinpoints the precise token count where the model's performance degrades or fails.

This gives you a reliable, empirical measurement of the model's true capabilities on your specific hardware, eliminating guesswork and ensuring your applications run with optimal performance.

## Key Features

- **Automatic Context Optimization**: Discover the true context limit of any model with the `optimize` command.
- **Full Model Management**: Programmatically `list` available models and `scan` for newly downloaded ones.
- **Flexible Inference Engine**: Run inference with a powerful two-phase prompt templating system that separates prompt structure from its content.
- **Rich CLI**: A beautiful and intuitive command-line interface built with `rich` and `fire`, providing formatted tables, progress indicators, and clear feedback.
- **Modern Python API**: An `async`-first API designed for high-performance, concurrent applications.
- **Robust and Resilient**: Features automatic retries with exponential backoff for network requests and a comprehensive exception hierarchy.
- **Lightweight and Focused**: Built with a minimal set of modern, high-quality dependencies.

</document_content>
</document>

<document index="20">
<source>docs/installation.md</source>
<document_content>
---
title: Installation
---

## Installation

### Requirements

- Python 3.10 or higher
- [LM Studio](https://lmstudio.ai/) installed and running locally
- At least one model downloaded in LM Studio

### Using pip

```bash
pip install lmstrix
```

### Using uv (recommended)

```bash
uv pip install lmstrix
```

### For Development

If you want to contribute to LMStrix or install it in an editable mode, you can clone the repository:

```bash
git clone https://github.com/twardoch/lmstrix
cd lmstrix
pip install -e .[dev]
```

</document_content>
</document>

<document index="21">
<source>docs/usage.md</source>
<document_content>
---
title: Usage
---

## Command-Line Interface (CLI)

LMStrix provides a powerful and intuitive CLI for interacting with your local models.

### Scanning for Models

Before you can use LMStrix, you need to scan for available models in LM Studio. This command discovers all models that you have downloaded.

```bash
lmstrix scan
```

### Listing Models

To see a list of all discovered models, their context length, and test status, use the `list` command.

```bash
lmstrix list
```

### Testing Context Limits

This is the core feature of LMStrix. The `test` command automatically determines the maximum context window a model can handle on your machine.

```bash
# Test a specific model by its ID
lmstrix test "model-id-here"

# Test all models that haven't been tested yet
lmstrix test --all

# Test with a custom threshold (default: 102,400 tokens)
# This prevents system crashes by limiting the maximum initial test size
lmstrix test "model-id-here" --threshold 51200

# Test all models with a lower threshold for safety
lmstrix test --all --threshold 32768
```

**New in v1.1**: The `--threshold` parameter (default: 102,400 tokens) prevents system crashes when testing models with very large declared context sizes. The testing algorithm now:
1. Tests at 1,024 tokens to verify the model loads
2. Tests at min(threshold, declared_max)
3. If successful and below declared max, increments by 10,240 tokens
4. If failed, performs binary search to find the exact limit

For more details on how this works, see the [How It Works](./how-it-works.md) page.

### Running Inference

You can run inference directly from the command line.

```bash
# Run a simple prompt
lmstrix infer "Your prompt here" --model "model-id" --max-tokens 150

# Use a prompt from a file
lmstrix infer "@prompts.toml:greeting" --model "model-id"
```

### Verbose Mode

For more detailed output and debugging, you can use the `--verbose` flag with any command.

```bash
lmstrix scan --verbose
```

## Python API

The Python API provides a clean, `async`-first interface for programmatic access to LMStrix features.

```python
import asyncio
from lmstrix import LMStrix

async def main():
    # Initialize the client
    lms = LMStrix()
    
    # Scan for available models
    await lms.scan_models()
    
    # List all models
    models = await lms.list_models()
    for model in models:
        print(f"Model: {model.id}")
        print(f"  Context limit: {model.context_limit:,} tokens")
        print(f"  Tested limit: {model.tested_max_context or 'Not tested'}")
        print(f"  Status: {model.context_test_status}")
    
    # Test a specific model's context limits
    model_id = models[0].id if models else None
    if model_id:
        print(f"\nTesting context limits for {model_id}...")
        result = await lms.test_model(model_id)
        print(f"Optimal context: {result.tested_max_context} tokens")
        print(f"Test status: {result.context_test_status}")
    
    # Run inference
    if model_id:
        response = await lms.infer(
            prompt="What is the meaning of life?",
            model_id=model_id,
            max_tokens=100
        )
        print(f"\nInference result:\n{response.content}")

if __name__ == "__main__":
    asyncio.run(main())
```

</document_content>
</document>

<document index="22">
<source>examples/README.md</source>
<document_content>
# LMStrix Usage Examples

This directory contains comprehensive examples demonstrating all features of the LMStrix CLI and Python API, including the latest context control, prompt templates, and model state management features.

## Prerequisites

1. **LMStrix Installed**: Ensure you have installed LMStrix (`pip install lmstrix`)
2. **LM Studio Running**: Most examples require LM Studio running with at least one model downloaded
3. **Model Downloaded**: Download models in LM Studio (e.g., Llama, Mistral, Phi, Qwen)

**Note**: Examples use "llama" as a placeholder model ID. Update this to match your downloaded models. Run `lmstrix list` to see available models.

## Quick Start

Run all examples at once:
```bash
bash run_all_examples.sh
```

Or run individual examples as shown below.

---

## CLI Examples (`cli/`)

### Core Workflow Examples

#### `basic_workflow.sh`
Complete end-to-end workflow demonstrating:
- Model scanning with verbose output
- Listing models with `--sort` and `--show` options
- Context testing with new fixed strategy
- Inference with `--in_ctx` and `--out_ctx` parameters
- Model state reuse demonstration

```bash
bash cli/basic_workflow.sh
```

#### `model_testing.sh`
Advanced context testing features:
- Fixed context testing strategy (30k, 40k, 60k, etc.)
- `--threshold` parameter for safety limits
- `--fast` mode for quick validation
- `--ctx` for specific context testing
- Batch testing with `--all`
- Test resumption capabilities

```bash
bash cli/model_testing.sh
```

#### `inference_examples.sh`
Comprehensive inference scenarios:
- Context control with `--in_ctx` and `--out_ctx`
- Load without context specification (`--in_ctx 0`)
- Model state detection and reuse
- `--force-reload` demonstration
- TOML prompt templates with `--file_prompt` and `--dict`
- Temperature control for creativity

```bash
bash cli/inference_examples.sh
```

### New Feature Examples

#### `context_control_examples.sh` *(NEW)*
Deep dive into context management:
- Understanding `--in_ctx` vs `--out_ctx`
- Memory-conscious loading strategies
- Context size performance impact
- Long document processing
- Optimal context selection

```bash
bash cli/context_control_examples.sh
```

#### `model_state_demo.sh` *(NEW)*
Model state detection and management:
- How model reuse works
- Performance comparison (load vs reuse)
- Force reload scenarios
- Model switching strategies
- Context change behavior

```bash
bash cli/model_state_demo.sh
```

---

## Python API Examples (`python/`)

### Core API Usage

#### `basic_usage.py`
Fundamentals of the LMStrix Python API:
- Initializing `LMStrix` client
- Scanning and listing models
- Basic inference with `out_ctx`
- Advanced inference with `in_ctx`
- Model state detection
- Error handling

```bash
python3 python/basic_usage.py
```

#### `advanced_testing.py`
Context testing programmatically:
- Fixed context testing strategy
- Fast mode testing
- Custom threshold limits
- Specific context testing
- Batch testing multiple models
- Test result analysis

```bash
python3 python/advanced_testing.py
```

#### `custom_inference.py`
Advanced inference techniques:
- Context control (`in_ctx` and `out_ctx`)
- Temperature adjustment
- TOML prompt template loading
- Structured JSON output
- Model state reuse
- Force reload scenarios

```bash
python3 python/custom_inference.py
```

#### `batch_processing.py`
Working with multiple models:
- Batch testing strategies
- Model response comparison
- Performance benchmarking
- Efficiency analysis
- Smart testing order

```bash
python3 python/batch_processing.py
```

### New Python Examples

#### `prompt_templates_demo.py` *(NEW)*
Advanced prompt template features:
- Creating prompts programmatically
- Nested placeholder resolution
- Loading from TOML files
- Batch prompt resolution
- Missing placeholder handling
- Context injection with truncation

```bash
python3 python/prompt_templates_demo.py
```

---

## Prompt Templates (`prompts/`)

### Main Template File

#### `prompts.toml`
Comprehensive prompt template examples:
- Greetings (formal, casual, professional)
- Templates with internal references
- Code review and explanation prompts
- Research summarization templates
- Math and science prompts
- Creative writing templates
- Q&A formats
- System prompts

### Domain-Specific Templates

- `prompts/analysis.toml` - Data analysis prompts
- `prompts/coding.toml` - Programming assistance
- `prompts/creative.toml` - Creative writing
- `prompts/qa.toml` - Question answering

---

## Data Files (`data/`)

- `sample_context.txt` - Large text for context testing
- `test_questions.json` - Sample Q&A scenarios

---

## Key Features Demonstrated

### Context Management
- **`--in_ctx`**: Control model loading context size
- **`--out_ctx`**: Control maximum generation tokens
- **Model reuse**: Automatic reuse of loaded models
- **Force reload**: Refresh models with `--force-reload`

### Testing Enhancements
- **Fixed contexts**: Tests at 30k, 40k, 60k, 80k, 100k, 120k
- **Threshold safety**: Limit initial test size
- **Fast mode**: Skip semantic verification
- **Test resumption**: Continue interrupted tests

### Prompt Templates
- **TOML loading**: Load prompts from `.toml` files
- **Placeholders**: Dynamic value substitution
- **Nested references**: Templates can reference other templates
- **Batch resolution**: Process multiple prompts at once

### Performance Features
- **Smart sorting**: Optimal model testing order
- **Model state tracking**: Know when models are loaded
- **Efficiency metrics**: Track tested vs declared ratios
- **Batch operations**: Test or query multiple models

---

## Running Examples

### Individual Examples
```bash
# CLI examples
bash cli/basic_workflow.sh
bash cli/model_testing.sh
bash cli/inference_examples.sh
bash cli/context_control_examples.sh
bash cli/model_state_demo.sh

# Python examples
python3 python/basic_usage.py
python3 python/advanced_testing.py
python3 python/custom_inference.py
python3 python/batch_processing.py
python3 python/prompt_templates_demo.py
```

### All Examples
```bash
bash run_all_examples.sh
```

---

## Tips for Success

1. **Update Model IDs**: Change "llama" to match your downloaded models
2. **Check Model List**: Run `lmstrix list` to see available models
3. **Start Small**: Begin with `basic_workflow.sh` or `basic_usage.py`
4. **Test First**: Run `lmstrix test` to find optimal context for your models
5. **Use Verbose Mode**: Add `--verbose` for detailed output
6. **Monitor Memory**: Larger contexts use more GPU/RAM

---

## Troubleshooting

If examples fail:
1. Ensure LM Studio is running
2. Check you have models downloaded
3. Update model IDs in scripts
4. Verify LMStrix installation: `lmstrix --version`
5. Check available models: `lmstrix list`

For more help, see the main [LMStrix documentation](https://github.com/AdamAdli/lmstrix).
</document_content>
</document>

<document index="23">
<source>examples/cli/basic_workflow.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates the complete, basic workflow of LMStrix.
# 1. Scan for downloaded models.
# 2. List the models found with various display options.
# 3. Test the context length of a specific model.
# 4. Run inference with the tested model using new features.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Basic Workflow Demo ###"

# Step 1: Scan for models
# This command discovers all models downloaded in your LM Studio installation
# and updates the local registry file (lmstrix.json).
echo -e "\n--- Step 1: Scanning for models ---"
lmstrix scan --verbose
echo "Scan complete. Model registry updated."

# Step 2: List models with different display options
# This demonstrates the new --show and --sort options
echo -e "\n--- Step 2a: Listing models (default view) ---"
lmstrix list
echo -e "\n--- Step 2b: Listing models sorted by context size ---"
lmstrix list --sort ctx
echo -e "\n--- Step 2c: Showing just model IDs ---"
lmstrix list --show id
echo "Model listing demonstrations complete."

# Step 3: Test a model's context length
# We'll use a common model identifier that users might have
echo -e "\n--- Step 3: Testing a model's context length ---"
echo "Note: This may take several minutes depending on the model and your hardware."
# Common model patterns users might have:
# "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded
echo "Looking for models matching: $MODEL_ID"
# Show models matching the pattern
lmstrix list --show id | grep -i "$MODEL_ID" || echo "No models found matching '$MODEL_ID'"
echo -e "\nTesting model: $MODEL_ID"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Context test complete."

# Step 4: Run inference with new context control features
# Demonstrates --out_ctx instead of deprecated --max_tokens
echo -e "\n--- Step 4a: Running basic inference ---"
lmstrix infer "What is the capital of France?" "$MODEL_ID" --out_ctx 50

echo -e "\n--- Step 4b: Running inference with specific loading context ---"
lmstrix infer "Explain quantum computing in simple terms." "$MODEL_ID" --in_ctx 4096 --out_ctx 200

echo -e "\n--- Step 4c: Checking if model is already loaded (reuse demo) ---"
lmstrix infer "What is 2+2?" "$MODEL_ID" --out_ctx 10

echo -e "\nInference demonstrations complete."

echo -e "\n### Workflow Demo Finished ###"
</document_content>
</document>

<document index="24">
<source>examples/cli/context_control_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates advanced context control features in LMStrix.
# Shows --in_ctx usage, model state detection, and context optimization.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Context Control Examples ###"

# Replace with a model identifier that matches your downloaded models
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Understanding Context Parameters
echo -e "\n--- Example 1: Understanding Context Parameters ---"
echo "LMStrix has two context parameters:"
echo "  --in_ctx: Controls the context size when LOADING the model"
echo "  --out_ctx: Controls the maximum tokens to GENERATE"
echo -e "\nLet's see them in action..."

# Example 2: Default Context Loading
echo -e "\n--- Example 2: Default Context Loading ---"
echo "When --in_ctx is not specified, LMStrix uses the optimal context:"
lmstrix infer "What is machine learning?" "$MODEL_ID" --out_ctx 50 --verbose
echo "(Model loaded with optimal context based on testing or declared limit)"

# Example 3: Specific Context Loading
echo -e "\n--- Example 3: Loading with Specific Context ---"
echo "Load model with exactly 4096 tokens of context:"
lmstrix infer "Explain neural networks briefly." "$MODEL_ID" --in_ctx 4096 --out_ctx 100 --verbose
echo "(Model loaded with 4096 token context)"

# Example 4: Model State Detection
echo -e "\n--- Example 4: Model State Detection ---"
echo "First inference - model loads:"
lmstrix infer "What is 5 + 5?" "$MODEL_ID" --out_ctx 10
echo -e "\nSecond inference - model already loaded (should be faster):"
lmstrix infer "What is 10 + 10?" "$MODEL_ID" --out_ctx 10
echo "(Notice: Model was reused, not reloaded)"

# Example 5: Context Size Impact
echo -e "\n--- Example 5: Context Size Impact ---"
echo "Smaller context (faster loading, less memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 2048 --out_ctx 10

echo -e "\nLarger context (slower loading, more memory):"
time lmstrix infer "Hello!" "$MODEL_ID" --in_ctx 16384 --out_ctx 10

# Example 6: Zero Context Loading
echo -e "\n--- Example 6: Zero Context Loading (--in_ctx 0) ---"
echo "Load model without specifying context (uses model's default):"
lmstrix infer "What is Python?" "$MODEL_ID" --in_ctx 0 --out_ctx 50
echo "(Model loaded with its default context configuration)"

# Example 7: Context Optimization Strategy
echo -e "\n--- Example 7: Context Optimization Strategy ---"
echo "For best performance:"
echo "1. Test your model first to find optimal context:"
echo "   lmstrix test --model_id $MODEL_ID"
echo -e "\n2. Use the tested context for inference:"
echo "   lmstrix infer 'prompt' $MODEL_ID --in_ctx <tested_value>"
echo -e "\n3. Or let LMStrix choose automatically (omit --in_ctx)"

# Example 8: Long Context Use Case
echo -e "\n--- Example 8: Long Context Use Case ---"
echo "Processing a long document (simulated):"
LONG_PROMPT="Summarize this text: The history of artificial intelligence began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by classical philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols."
echo "Prompt length: ${#LONG_PROMPT} characters"
lmstrix infer "$LONG_PROMPT" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 9: Memory-Conscious Loading
echo -e "\n--- Example 9: Memory-Conscious Loading ---"
echo "For limited memory systems, use smaller context:"
lmstrix infer "What is RAM?" "$MODEL_ID" --in_ctx 2048 --out_ctx 50
echo "(Smaller context = less GPU/RAM usage)"

# Example 10: Comparing Context Sizes
echo -e "\n--- Example 10: Context Size Comparison ---"
echo "Let's compare the same prompt with different contexts:"

echo -e "\nWith 2K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 2048 --out_ctx 100

echo -e "\nWith 8K context:"
lmstrix infer "Explain quantum computing" "$MODEL_ID" --in_ctx 8192 --out_ctx 100

echo -e "\nNote: Larger context doesn't always mean better output for simple prompts"

echo -e "\n### Context Control Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- --in_ctx controls model loading context (memory usage)"
echo "- --out_ctx controls generation length"
echo "- Models are reused when possible for efficiency"
echo "- Optimal context depends on your use case"
echo "- Test models first to find their true limits"
</document_content>
</document>

<document index="25">
<source>examples/cli/inference_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various inference scenarios using LMStrix.
# Shows new features: --in_ctx, --out_ctx, --file_prompt, --dict, --force-reload
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Inference Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Simple Question with new --out_ctx parameter
echo -e "\n--- Example 1: Simple Question with Output Context Control ---"
lmstrix infer "Explain the theory of relativity in simple terms." "$MODEL_ID" --out_ctx 200

# Example 2: Context Control - Load model with specific context size
echo -e "\n--- Example 2: Load Model with Specific Context Size ---"
lmstrix infer "What are the benefits of renewable energy?" "$MODEL_ID" --in_ctx 8192 --out_ctx 150

# Example 3: Load model without context specification (--in_ctx 0)
echo -e "\n--- Example 3: Load Model with Default Context ---"
lmstrix infer "What is machine learning?" "$MODEL_ID" --in_ctx 0 --out_ctx 100

# Example 4: Model State Detection - Second call reuses loaded model
echo -e "\n--- Example 4: Model State Detection (Reuse) ---"
echo "First call - loads the model:"
lmstrix infer "What is Python?" "$MODEL_ID" --out_ctx 50
echo -e "\nSecond call - reuses loaded model:"
lmstrix infer "What is JavaScript?" "$MODEL_ID" --out_ctx 50

# Example 5: Force Reload - Reload model even if already loaded
echo -e "\n--- Example 5: Force Model Reload ---"
lmstrix infer "What is artificial intelligence?" "$MODEL_ID" --force-reload --out_ctx 100

# Example 6: Using TOML Prompt Files with Parameters
echo -e "\n--- Example 6: Using Prompt Templates from TOML ---"
# First, let's check if prompts.toml exists
if [ -f "examples/prompts.toml" ]; then
    PROMPT_FILE="examples/prompts.toml"
elif [ -f "prompts.toml" ]; then
    PROMPT_FILE="prompts.toml"
else
    # Create a simple prompts.toml for demonstration
    cat > temp_prompts.toml <<EOL
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"

[code]
review = "Please review this {{language}} code and suggest improvements: {{code}}"
explain = "Explain this {{language}} code in simple terms: {{code}}"
EOL
    PROMPT_FILE="temp_prompts.toml"
fi

echo "Using prompt file: $PROMPT_FILE"
lmstrix infer greetings.formal "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "name=Alice,topic=Python" --out_ctx 100

# Example 7: Multiple Parameters with Different Format
echo -e "\n--- Example 7: Code Review with Template ---"
lmstrix infer code.review "$MODEL_ID" --file_prompt "$PROMPT_FILE" --dict "language=Python,code=def factorial(n): return 1 if n <= 1 else n * factorial(n-1)" --out_ctx 300

# Example 8: Adjusting Temperature for Creative Output
echo -e "\n--- Example 8: Creative Writing with High Temperature ---"
lmstrix infer "Write a short poem about the sea." "$MODEL_ID" --temperature 1.5 --out_ctx 150

# Example 9: Low Temperature for Deterministic Output
echo -e "\n--- Example 9: Math Problem with Low Temperature ---"
lmstrix infer "Calculate: 25 * 4 + 10 - 5" "$MODEL_ID" --temperature 0.1 --out_ctx 20

# Example 10: Combining Features - Context Control + Template + Temperature
echo -e "\n--- Example 10: Combined Features Demo ---"
lmstrix infer greetings.casual "$MODEL_ID" \
    --file_prompt "$PROMPT_FILE" \
    --dict "name=Bob,topic=quantum computing" \
    --in_ctx 4096 \
    --out_ctx 150 \
    --temperature 0.8

# Cleanup temporary file if created
if [ "$PROMPT_FILE" = "temp_prompts.toml" ]; then
    rm temp_prompts.toml
fi

echo -e "\n### Inference Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- Use --out_ctx instead of deprecated --max_tokens"
echo "- Use --in_ctx to control model loading context size"
echo "- Use --file_prompt with --dict for template-based prompts"
echo "- Models are reused when already loaded (unless --force-reload)"
echo "- Combine features for advanced use cases"
</document_content>
</document>

<document index="26">
<source>examples/cli/model_state_demo.sh</source>
<document_content>
#!/bin/bash
# this_file: examples/cli/model_state_demo.sh
#
# Demonstration of model state persistence in LMStrix
# Shows how models remain loaded across multiple infer calls
#

set -e

echo "### LMStrix Model State Persistence Demo ###"
echo ""
echo "This demo shows how LMStrix now keeps models loaded between calls"
echo "for better performance and resource efficiency."
echo ""

MODEL_ID="llama-3.2-3b-instruct"  # Change this to match your model

# Step 1: First inference with explicit model and context
echo "=== Step 1: First inference with explicit model and context ==="
echo "Command: lmstrix infer \"What is 2+2?\" -m \"$MODEL_ID\" --out_ctx \"25%\" --verbose"
echo "(This will load the model with specified context)"
echo ""
lmstrix infer "What is 2+2?" -m "$MODEL_ID" --out_ctx "25%" --verbose || echo "Failed"

echo ""
echo "=== Step 2: Second inference without in_ctx ==="
echo "Command: lmstrix infer \"What is 3+3?\" -m \"$MODEL_ID\" --verbose"
echo "(This should reuse the already loaded model)"
echo ""
sleep 2
lmstrix infer "What is 3+3?" -m "$MODEL_ID" --verbose || echo "Failed"

echo ""
echo "=== Step 3: Third inference without model_id ==="
echo "Command: lmstrix infer \"What is 4+4?\" --verbose"
echo "(This should use the last-used model: $MODEL_ID)"
echo ""
sleep 2
lmstrix infer "What is 4+4?" --verbose || echo "Failed"

echo ""
echo "=== Demo Complete ==="
echo ""
echo "Key features demonstrated:"
echo "1. Models stay loaded between infer calls when in_ctx is not specified"
echo "2. Last-used model is remembered when -m is not specified"
echo "3. Better performance by avoiding repeated model loading/unloading"
echo ""
echo "Check the verbose output above to see:"
echo "- First call: 'Loading with optimal context...'"
echo "- Second call: 'Model already loaded... reusing...'"
echo "- Third call: 'Using last-used model...'"
</document_content>
</document>

<document index="27">
<source>examples/cli/model_testing.sh</source>
<document_content>
#!/bin/bash
#
# This script demonstrates various context testing scenarios with LMStrix.
# Shows new features: --threshold, --all --ctx, --fast, and test resumption.
#

# Exit immediately if a command exits with a non-zero status.
set -e

echo "### LMStrix Model Testing Examples ###"

# Replace with a model identifier that matches your downloaded models
# Common patterns: "llama", "mistral", "qwen", "phi", "gemma", "codellama"
MODEL_ID="llama" # <--- CHANGE THIS to match a model you have downloaded

# Example 1: Standard Test with New Fixed Context Strategy
# Tests at fixed context sizes: 30k, 40k, 60k, 80k, 100k, 120k, and declared_max-1
echo -e "\n--- Example 1: Standard Context Test ---"
echo "Testing model '$MODEL_ID' with fixed context strategy"
lmstrix test --model_id "$MODEL_ID" --verbose
echo "Standard test complete."

# Example 2: Test with Threshold Limit
# Prevents initial test from exceeding specified threshold (default: 102,400)
echo -e "\n--- Example 2: Test with Custom Threshold ---"
echo "Testing with maximum initial context of 50,000 tokens"
lmstrix test --model_id "$MODEL_ID" --threshold 50000
echo "Threshold test complete."

# Example 3: Fast Mode Testing
# Only checks if model can load and generate, skips semantic verification
echo -e "\n--- Example 3: Fast Mode Test ---"
echo "Running fast test (skip semantic verification)"
lmstrix test --model_id "$MODEL_ID" --fast
echo "Fast test complete."

# Example 4: Test at Specific Context Size
# Tests a model at a specific context size only
echo -e "\n--- Example 4: Test at Specific Context ---"
echo "Testing model at exactly 8192 tokens"
lmstrix test --model_id "$MODEL_ID" --ctx 8192
echo "Specific context test complete."

# Example 5: Batch Test All Models
# Tests all untested models in the registry
echo -e "\n--- Example 5: Batch Test All Models ---"
echo "Testing all models (this may take a while)..."
echo "Note: This will test ALL models. Press Ctrl+C to cancel."
read -p "Press Enter to continue..." -t 5 || true
lmstrix test --all --verbose
echo "Batch test complete."

# Example 6: Batch Test with Context Limit
# Tests all models at a specific context size
echo -e "\n--- Example 6: Batch Test at Specific Context ---"
echo "Testing all untested models at 4096 tokens"
lmstrix test --all --ctx 4096
echo "Batch context test complete."

# Example 7: Fast Batch Testing
# Quick test of all models without semantic verification
echo -e "\n--- Example 7: Fast Batch Test ---"
echo "Running fast test on all models"
lmstrix test --all --fast
echo "Fast batch test complete."

# Example 8: Show Test Results
# Display models sorted by their tested context size
echo -e "\n--- Example 8: View Test Results ---"
echo "Models sorted by tested context size:"
lmstrix list --sort ctx
echo -e "\nModels sorted by efficiency (tested/declared ratio):"
lmstrix list --sort eff

# Example 9: Test with Smart Sorting
# When testing all models, they're sorted for efficiency
echo -e "\n--- Example 9: Smart Sorted Batch Test ---"
echo "Testing all models with smart sorting (smaller models first)"
lmstrix test --all --threshold 30000 --fast
echo "Smart sorted test complete."

# Example 10: Resume Interrupted Test
# If a test is interrupted, it can resume from where it left off
echo -e "\n--- Example 10: Test Resumption Demo ---"
echo "LMStrix automatically saves progress during testing."
echo "If a test is interrupted, simply run the same command again."
echo "The test will resume from the last successful context size."

echo -e "\n### Model Testing Examples Finished ###"
echo -e "\nKey takeaways:"
echo "- New fixed context testing strategy (30k, 40k, 60k, etc.)"
echo "- Use --threshold to limit maximum initial test size"
echo "- Use --fast for quick testing without semantic checks"
echo "- Use --ctx to test at specific context sizes"
echo "- Batch testing with --all supports all options"
echo "- Tests automatically resume if interrupted"
</document_content>
</document>

<document index="28">
<source>examples/data/sample_context.txt</source>
<document_content>
This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

This is a sample text file designed for context length testing in LMStrix. The purpose of this file is to provide a sufficiently large and repetitive block of text that can be used to fill the context window of a large language model. By feeding a model a long prompt composed of this text, we can determine the maximum number of tokens it can process before it fails to load or produce a coherent response. This process is crucial for understanding the true operational limits of a model, which may differ from the limits advertised by its developers. The text itself is not intended to be meaningful, but rather to serve as a consistent and measurable filler for the model's context buffer. The repetition ensures that the content is uniform and does not introduce unexpected variables into the testing process.

</document_content>
</document>

<document index="29">
<source>examples/data/test_questions.json</source>
<document_content>
[
  {
    "id": "qa_1",
    "question": "What is the primary purpose of the sample_context.txt file?",
    "context_source": "sample_context.txt"
... (file content truncated to first 5 lines)
</document_content>
</document>

<document index="30">
<source>examples/prompts/analysis.toml</source>
<document_content>
# examples/prompts/analysis.toml

[summarize_text]
prompt = "Summarize the key points of the following text in three bullet points:

{text}"
system_prompt = "You are a helpful assistant that specializes in text summarization."
description = "Summarizes a long piece of text."

[sentiment_analysis]
prompt = "Analyze the sentiment of the following review. Respond with 'Positive', 'Negative', or 'Neutral'.

Review: "{review_text}""
system_prompt = "You are a sentiment analysis AI."
description = "Analyzes the sentiment of a given text."
</document_content>
</document>

<document index="31">
<source>examples/prompts/coding.toml</source>
<document_content>
# examples/prompts/coding.toml

[python_function]
prompt = "Write a Python function that does the following: {request}. Include a docstring and type hints."
system_prompt = "You are an expert Python programmer who writes clean, efficient, and well-documented code."
description = "Generates a Python function based on a request."

[explain_code]
prompt = "Explain what this code does in simple terms. Identify potential bugs if any.

Code:
```
{code_snippet}
```"
system_prompt = "You are a senior software engineer who is excellent at code reviews and explaining complex concepts."
description = "Explains a snippet of code."
</document_content>
</document>

<document index="32">
<source>examples/prompts/creative.toml</source>
<document_content>
# examples/prompts/creative.toml

[write_poem]
prompt = "Write a short, four-line poem about {topic}."
system_prompt = "You are a world-class poet."
description = "Writes a short poem on a given topic."

[generate_story_idea]
prompt = "Generate a compelling story idea based on the following genre and character: Genre: {genre}, Character: {character_description}"
system_prompt = "You are a creative writing assistant, skilled in generating unique plot ideas."
description = "Generates a story idea from a genre and character."
</document_content>
</document>

<document index="33">
<source>examples/prompts/qa.toml</source>
<document_content>
# examples/prompts/qa.toml

[simple_question]
prompt = "Answer the following question: {question}"
system_prompt = "You are a helpful and factual question-answering AI."
description = "Answers a direct question."

[contextual_question]
prompt = "Based on the text below, answer the question.

Text: {context}

Question: {question}"
system_prompt = "You are an AI assistant that answers questions based *only* on the provided context."
description = "Answers a question based on a provided context."
</document_content>
</document>

<document index="34">
<source>examples/prompts.toml</source>
<document_content>
# Example prompt templates for LMStrix
# Demonstrates placeholder resolution and template reuse

# Simple greeting prompts
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"
professional = "Hello {{name}}, I'm here to help you with {{topic}}. Please describe your requirements."

# Templates for different use cases
[templates]
base = "You are an expert assistant specializing in {{domain}}."
instruction = """{{templates.base}}

Please provide a {{style}} explanation of {{concept}}.
Your response should be appropriate for someone with {{level}} knowledge."""

# Code-related prompts
[code]
review = """You are a code reviewer. Please review the following {{language}} code:

{{code}}

Focus on:
1. Code quality and best practices
2. Potential bugs or issues
3. Performance considerations
4. Security concerns"""

explain = "Explain this {{language}} code in simple terms: {{code}}"

# Research prompts
[research]
summarize = """Please summarize the following text about {{topic}}:

{{text}}

Provide a {{length}} summary focusing on the key points."""

analyze = """Analyze the following information about {{subject}}:

{{content}}

Consider the following aspects:
- {{aspect1}}
- {{aspect2}}
- {{aspect3}}"""

# Math and science prompts
[math]
solve = "Solve this {{difficulty}} math problem step by step: {{problem}}"
explain_concept = "Explain the concept of {{concept}} in {{field}} using {{approach}} approach."

# Creative writing prompts
[creative]
story = "Write a {{genre}} story about {{character}} who {{situation}}. The story should be {{length}} and include {{elements}}."
poem = "Create a {{style}} poem about {{theme}} that incorporates {{imagery}}."

# Question answering
[qa]
simple = "{{question}}"
detailed = """Question: {{question}}

Please provide a comprehensive answer that includes:
- Direct answer
- Explanation
- Examples where relevant
- Any important caveats or exceptions"""

# System prompts
[system]
assistant = """You are a helpful AI assistant. Your traits:
- {{personality}}
- Knowledge level: {{expertise}}
- Communication style: {{style}}
- Primary language: {{language}}"""

chatbot = """You are {{bot_name}}, a {{bot_type}} chatbot.
Your purpose: {{purpose}}
Your tone: {{tone}}
Special instructions: {{instructions}}"""
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/python/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/examples/python/advanced_testing.py
# Language: python

from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates advanced context testing with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/basic_usage.py
# Language: python

from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError, ModelNotFoundError
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates basic usage of the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/batch_processing.py
# Language: python

import time
from lmstrix import LMStrix
from lmstrix.core.models import ContextTestStatus
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates batch processing of multiple models for testing or inference."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/custom_inference.py
# Language: python

from pathlib import Path
from lmstrix import LMStrix
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates custom inference workflows with the LMStrix Python API."""


# File: /Users/Shared/lmstudio/lmstrix/examples/python/prompt_templates_demo.py
# Language: python

from pathlib import Path
import toml
from lmstrix import LMStrix
from lmstrix.core.prompts import PromptResolver
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt
from lmstrix.utils.logging import logger

def main(()) -> None:
    """Demonstrates prompt template features."""


<document index="35">
<source>examples/run_all_examples.sh</source>
<document_content>
#!/bin/bash
#
# This script runs all the examples in the `cli` and `python` directories.
# It's a way to functionally test that all example code is working as expected.
#
# NOTE: This script assumes you have a model downloaded in LM Studio
# and that the identifier 'ultron-summarizer-1b' will match it. If not, please edit the
# example files to use an identifier for a model you have.
#

set -e # Exit on any error

echo "===== Running All LMStrix Examples ====="

# --- Running CLI Examples ---
echo -e "

--- Testing CLI Examples ---"
echo "NOTE: The CLI scripts have a placeholder model identifier ('ultron-summarizer-1b')."
echo "Please edit them if you don't have a model matching that ID."

echo -e "
>>> Running basic_workflow.sh"
bash "$(dirname "$0")/cli/basic_workflow.sh"

echo -e "
>>> Running model_testing.sh"
bash "$(dirname "$0")/cli/model_testing.sh"

echo -e "
>>> Running inference_examples.sh"
bash "$(dirname "$0")/cli/inference_examples.sh"

echo -e "
--- CLI Examples Complete ---"


# --- Running Python Examples ---
echo -e "

--- Testing Python Examples ---"
echo "NOTE: The Python scripts will use the first model they find."

echo -e "
>>> Running basic_usage.py"
python3 "$(dirname "$0")/python/basic_usage.py"

echo -e "
>>> Running advanced_testing.py"
python3 "$(dirname "$0")/python/advanced_testing.py"

echo -e "
>>> Running custom_inference.py"
python3 "$(dirname "$0")/python/custom_inference.py"

echo -e "
>>> Running batch_processing.py"
python3 "$(dirname "$0")/python/batch_processing.py"

echo -e "
--- Python Examples Complete ---"


echo -e "

===== All Examples Finished Successfully ====="
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/examples/specialized/elo_liczby.py
# Language: python

import sys
from pathlib import Path
import fire
from rich.console import Console
from slugify import slugify
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.loaders.model_loader import load_model_registry
from lmstrix.utils.logging import logger

def process_text_with_model((
    input_file: str,
    model_id: str | None = None,
    verbose: bool = False,
)) -> str:
    """ Process a text file by sending paragraphs to a model for number-to-words conversion...."""

def main(()) -> None:
    """Main entry point using Fire CLI."""


# File: /Users/Shared/lmstudio/lmstrix/get_out_ctx.py
# Language: python

import sys
from pathlib import Path
from lmstrix.core.models import ModelRegistry

def get_out_ctx_for_model((model_id: str)) -> int | None:
    """Get the out_ctx value for a specific model."""

def get_all_out_ctx_values(()) -> dict[str, int]:
    """Get out_ctx values for all models in the registry."""

def main(()) -> None:
    """Main function to demonstrate getting out_ctx values."""


<document index="36">
<source>issues/401.txt</source>
<document_content>
I’m struggling to understand why `lmstrix test -c 2047 -m MODEL_ID` fails if MODEL_ID is one of these: 

```
slim-extract-tool            1.59    4,096      ✗ Failed
slim-extract-qwen-1.5b       1.04    131,072    ✗ Failed
slim-extract-qwen-0.5b       0.46    131,072    ✗ Failed
slim-summary-tool            1.59    4,096      ✗ Failed
dragon-mistral-answer-tool   4.07    32,768     ✗ Failed
dragon-mistral-7b-v0         4.07    32,768     ✗ Failed
dragon-llama-3.1             4.58    131,072    ✗ Failed
bling-qwen-mini-tool         1.04    131,072    ✗ Failed
mistral-7b-claim-extractor   7.17    32,768     ✗ Failed
llama3.2-entity-1b           2.31    131,072    ✗ Failed
llama3.2-entity              3.19    131,072    ✗ Failed
```

Basically the inference from these models fails when called via the code that `lmstrix test` uses. 

But in the LM Studio GUI the inference works. 

Write some code that uses the `lmstrix infer` code but also more direct `lmstudio` code, with just the model `slim-extract-qwen-0.5b` so we can pinpoint the issue. 

</document_content>
</document>

<document index="37">
<source>publish.sh</source>
<document_content>
#!/usr/bin/env bash
cd "$(dirname "$0")" || exit

fd -e py -x uvx autoflake -i {} &>cleanup.txt
fd -e py -x uvx pyupgrade --py312-plus {} &>>cleanup.txt
fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {} &>>cleanup.txt
fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {} &>>cleanup.txt
llms . "llms.txt"
uvx hatch clean
gitnextvers .
uvx hatch build
uvx hatch publish

</document_content>
</document>

<document index="38">
<source>pyproject.toml</source>
<document_content>
[build-system]
requires = ["hatchling>=1.24.2", "hatch-vcs>=0.3.0"]
build-backend = "hatchling.build"

[project]
name = "lmstrix"
description = "A toolkit for managing and testing LM Studio models with automatic context limit discovery"
readme = "README.md"
requires-python = ">=3.11"
license = {file = "LICENSE"}
license-files = ["LICENSE"]
authors = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
maintainers = [
  { name = "Adam Twardoch", email = "adam+github@twardoch.com" },
]
keywords = [
  "ai",
  "cli",
  "context",
  "developer-tools",
  "llm",
  "lmstudio",
  "optimization",
  "testing",
]
classifiers = [
  "Development Status :: 4 - Beta",
  "Intended Audience :: Developers",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
  "Programming Language :: Python :: 3.12",
  "Programming Language :: Python :: 3.13",
  "Topic :: Scientific/Engineering :: Artificial Intelligence",
  "Topic :: Software Development :: Libraries :: Python Modules",
  "Topic :: Software Development :: Quality Assurance",
  "Topic :: System :: Benchmark",
  "Typing :: Typed",
]
dependencies = [
  "fire>=0.7.0",
  "httpx>=0.24",
  "lmstudio>=1.4.1",
  "loguru>=0.7",
  "pydantic>=2.0",
  "rich>=13.9.4",
  "tenacity>=8.5.0",
  "tiktoken>=0.5",
  "toml-topl>=1.0.5",
  "tomli>=2.0.1; python_version < '3.11'",
  "hydra-core",
  "omegaconf",
  "pathvalidate",
]
dynamic = ["version"]

[project.urls]
Homepage = "https://github.com/twardoch/lmstrix"
Documentation = "https://github.com/twardoch/lmstrix#readme"
Repository = "https://github.com/twardoch/lmstrix.git"
Issues = "https://github.com/twardoch/lmstrix/issues"
Changelog = "https://github.com/twardoch/lmstrix/blob/main/CHANGELOG.md"

[project.scripts]
lmstrix = "lmstrix.__main__:main"

[project.optional-dependencies]
dev = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "mypy>=1.0",
  "black>=23.0",
  "ruff>=0.1.0",
  "pre-commit>=3.0",
  "setuptools>=68.0.0",
]
docs = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

# Hatch configuration
[tool.hatch.version]
source = "vcs"

[tool.hatch.build.hooks.vcs]
version-file = "src/lmstrix/_version.py"

[tool.hatch.build.targets.sdist]
include = [
  "/src",
  "/tests",
  "/LICENSE",
  "/README.md",
  "/CHANGELOG.md",
  "/pyproject.toml",
]

[tool.hatch.build.targets.wheel]
packages = ["src/lmstrix"]

[tool.hatch.build]
ignore-vcs = false

# Development environments
[tool.hatch.envs.default]
dependencies = [
  "pytest>=7.0",
  "pytest-asyncio>=0.21",
  "pytest-cov>=4.0",
  "pytest-mock>=3.10",
  "pytest-timeout>=2.1",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov=src/lmstrix --cov-report=term-missing --cov-report=html {args:tests}"
cov-report = "python -m http.server --directory htmlcov"

[tool.hatch.envs.lint]
detached = true
dependencies = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.0",
]

[tool.hatch.envs.lint.scripts]
style = [
  "ruff check {args:.}",
  "black --check --diff {args:.}",
]
fmt = [
  "black {args:.}",
  "ruff check --fix {args:.}",
  "style",
]
all = [
  "style",
  "typing",
]
typing = "mypy --install-types --non-interactive {args:src/lmstrix tests}"

[tool.hatch.envs.docs]
dependencies = [
  "sphinx>=7.0",
  "sphinx-rtd-theme>=2.0",
  "sphinx-autodoc-typehints>=1.25",
  "myst-parser>=2.0",
]

[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs docs/_build/html"
serve = "python -m http.server --directory docs/_build/html"

# Tool configurations
[tool.ruff]
target-version = "py310"
line-length = 100
src = ["src", "tests"]

[tool.ruff.lint]
select = [
  "E",   # pycodestyle errors
  "W",   # pycodestyle warnings
  "F",   # pyflakes
  "I",   # isort
  "B",   # flake8-bugbear
  "C4",  # flake8-comprehensions
  "UP",  # pyupgrade
  "N",   # pep8-naming
  "YTT", # flake8-2020
  "ANN", # flake8-annotations
  "S",   # flake8-bandit
  "BLE", # flake8-blind-except
  "A",   # flake8-builtins
  "COM", # flake8-commas
  "C90", # mccabe complexity
  "ISC", # flake8-implicit-str-concat
  "ICN", # flake8-import-conventions
  "G",   # flake8-logging-format
  "INP", # flake8-no-pep420
  "PIE", # flake8-pie
  "PT",  # flake8-pytest-style
  "Q",   # flake8-quotes
  "RSE", # flake8-raise
  "RET", # flake8-return
  "SIM", # flake8-simplify
  "TID", # flake8-tidy-imports
  "TCH", # flake8-type-checking
  "ARG", # flake8-unused-arguments
  "PTH", # flake8-use-pathlib
  "ERA", # eradicate
  "PGH", # pygrep-hooks
  "PL",  # pylint
  "TRY", # tryceratops
  "RUF", # ruff-specific rules
]
ignore = [
  "E501",    # line too long (handled by formatter)
  "B008",    # do not perform function calls in argument defaults
  "C901",    # too complex
  "ANN401",  # dynamically typed expressions (Any)
  "S101",    # use of assert detected
  "PLR0913", # too many arguments
  "PLR2004", # magic value comparison
  "COM812",  # missing trailing comma (handled by formatter)
  # Complexity rules - these are often legitimate in complex business logic
  "PLR0911", # too many return statements
  "PLR0912", # too many branches
  "PLR0915", # too many statements
  # Exception handling patterns that may be intentional
  "TRY003",  # long messages outside exception class
  "TRY301",  # abstract raise to inner function
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101", "ARG", "PLR2004", "ARG002", "ANN001"]

# Example scripts may have different standards
"examples/**/*.py" = ["ERA001", "BLE001", "ANN001"]

# CLI main file needs some builtin shadows for Fire compatibility
"src/lmstrix/__main__.py" = ["A002"]

# Context tester has legitimate unused args for interface consistency
"src/lmstrix/core/context_tester.py" = ["ARG002"]

# Client has some necessary bare excepts for connection handling
"src/lmstrix/api/client.py" = ["E722"]

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "lf"

[tool.black]
line-length = 100
target-version = ["py310", "py311", "py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
pretty = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[[tool.mypy.overrides]]
module = "tests.*"
disallow_untyped_defs = false
disallow_incomplete_defs = false

[[tool.mypy.overrides]]
module = [
  "fire",
  "tiktoken",
  "toml",
  "lmstudio",
]
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["tests"]
pythonpath = ["src"]
addopts = [
  "-ra",
  "-v",
  "--tb=short",
  "--strict-markers",
  "--strict-config",
  "--cov-branch",
]
filterwarnings = [
  "error",
  "ignore::DeprecationWarning",
  "ignore::PendingDeprecationWarning",
  "ignore:Benchmarks are automatically disabled",
]
asyncio_mode = "auto"
markers = [
  "slow: marks tests as slow (deselect with '-m \"not slow\"')",
  "integration: marks tests as integration tests",
  "unit: marks tests as unit tests",
]


[tool.coverage.run]
branch = true
source = ["src/lmstrix"]
omit = [
  "*/tests/*",
  "*/__init__.py",
  "*/conftest.py",
]

[tool.coverage.report]
precision = 2
exclude_lines = [
  "pragma: no cover",
  "def __repr__",
  "if self.debug:",
  "if settings.DEBUG",
  "raise AssertionError",
  "raise NotImplementedError",
  "if 0:",
  "if __name__ == .__main__.:",
  "class .*\\bProtocol\\):",
  "@(abc\\.)?abstractmethod",
]

[tool.coverage.html]
directory = "htmlcov"
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__init__.py
# Language: python

from importlib.metadata import PackageNotFoundError, version
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix._version import __version__

class LMStrix:
    """High-level interface to LMStrix's core features."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initializes the LMStrix API wrapper."""
    def scan((self)) -> list[Model]:
        """Scan for LM Studio models and update the registry."""
    def list_models((self)) -> list[Model]:
        """List all models currently in the registry."""
    def test_model((self, model_id: str)) -> Model:
        """Test a model's true operational context limits."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
        """Runs inference on a specified model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initializes the LMStrix API wrapper."""

def scan((self)) -> list[Model]:
    """Scan for LM Studio models and update the registry."""

def list_models((self)) -> list[Model]:
    """List all models currently in the registry."""

def test_model((self, model_id: str)) -> Model:
    """Test a model's true operational context limits."""

def infer((
        self,
        model_id: str,
        prompt: str,
        out_ctx: int | str = -1,
        temperature: float = 0.7,
    )) -> dict:
    """Runs inference on a specified model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/__main__.py
# Language: python

import sys
import fire
from lmstrix.api.main import LMStrixService

class LMStrixCLI:
    """A thin CLI wrapper for LMStrix commands."""
    def __init__((self)) -> None:
        """Initialize the CLI with the service layer."""
    def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
        """List all models from the registry with their test status."""
    def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
        """Test the context limits for models."""
    def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def health((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""

def __init__((self)) -> None:
    """Initialize the CLI with the service layer."""

def scan((self, failed: bool = False, reset: bool = False, verbose: bool = False)) -> None:
    """Scan for LM Studio models and update the local registry."""

def list((self, sort: str = "id", show: str | None = None, verbose: bool = False)) -> None:
    """List all models from the registry with their test status."""

def test((
        self,
        model_id: str | None = None,
        all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
    """Test the context limits for models."""

def infer((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def health((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def main(()) -> None:
    """Main entry point for the CLI."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/__init__.py
# Language: python

from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import (
    APIConnectionError,
    ConfigurationError,
    ContextLimitExceededError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
    ModelNotFoundError,
)
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/client.py
# Language: python

import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from lmstudio import LMStudioServerError
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError
from lmstrix.utils.logging import logger

class CompletionResponse(B, a, s, e, M, o, d, e, l):
    """Response from a completion request."""

class LMStudioClient:
    """Client for interacting with LM Studio."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the LM Studio client."""
    def list_models((self)) -> list[dict[str, Any]]:
        """List all downloaded models."""
    def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
        """Load a model with a specific context length using model path."""
    def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
        """Load a model with a specific context length using model ID."""
    def get_loaded_models((self)) -> list[dict[str, Any]]:
        """Get information about currently loaded models."""
    def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
        """Check if a specific model is currently loaded."""
    def unload_all_models((self)) -> None:
        """Unload all currently loaded models to free up resources."""
    def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
        """Make a completion request to a loaded LM Studio model."""
    def stream_completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds for no progress
        **kwargs: Any,  # Accept additional parameters
    )) -> Iterator[str]:
        """Stream a completion request to a loaded LM Studio model."""

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the LM Studio client."""

def list_models((self)) -> list[dict[str, Any]]:
    """List all downloaded models."""

def load_model((self, model_path: str, context_len: int, unload_all: bool = True)) -> Any:
    """Load a model with a specific context length using model path."""

def load_model_by_id((self, model_id: str, context_len: int)) -> Any:
    """Load a model with a specific context length using model ID."""

def get_loaded_models((self)) -> list[dict[str, Any]]:
    """Get information about currently loaded models."""

def is_model_loaded((self, model_id: str)) -> tuple[bool, int | None]:
    """Check if a specific model is currently loaded."""

def unload_all_models((self)) -> None:
    """Unload all currently loaded models to free up resources."""

def completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        **kwargs: Any,  # Accept additional parameters
    )) -> CompletionResponse:
    """Make a completion request to a loaded LM Studio model."""

def stream_completion((
        self,
        llm: Any,  # The loaded model object from lmstudio.llm
        prompt: str,
        out_ctx: int = -1,  # -1 for unlimited
        temperature: float = 0.8,  # Temperature for generation (LM Studio default)
        model_id: str | None = None,  # Pass model_id separately
        model_context_length: (
            int | None
        ) = None,  # Model's total context length for percentage calculation
        top_k: int = 40,  # topKSampling - controls token sampling diversity
        top_p: float = 0.95,  # topPSampling - nucleus sampling
        repeat_penalty: float = 1.1,  # repeatPenalty - penalty for repeated tokens
        min_p: float = 0.05,  # minPSampling - minimum token probability threshold
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds for no progress
        **kwargs: Any,  # Accept additional parameters
    )) -> Iterator[str]:
    """Stream a completion request to a loaded LM Studio model."""

def handle_token((token: str)) -> None:
    """Handle incoming token from stream."""

def handle_first_token((elapsed_seconds: float)) -> None:
    """Handle first token event."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/exceptions.py
# Language: python

class LMStrixError(E, x, c, e, p, t, i, o, n):
    """Base exception for all LMStrix errors."""
    def __init__((self, message: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ModelLoadError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails to load."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class InferenceError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when inference fails."""
    def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class APIConnectionError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when connection to LM Studio API fails."""
    def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class ContextLimitExceededError(I, n, f, e, r, e, n, c, e, E, r, r, o, r):
    """Raised when the input exceeds the model's context limit."""
    def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
        """Initialize the exception."""

class ModelNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a requested model is not found."""
    def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
        """Initialize the exception."""

class ConfigurationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's a configuration issue."""
    def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
        """Initialize the exception."""

class LMStudioInstallationNotFoundError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when the LM Studio installation path cannot be found."""
    def __init__((self)) -> None:
        """Initialize the exception."""

class ValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when data validation fails."""
    def __init__((self, field: str, value: any, reason: str)) -> None:
        """Initialize the exception."""

class InvalidContextLimitError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when context limit is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class InvalidModelSizeError(V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when model size is invalid."""
    def __init__((self, value: int)) -> None:
        """Initialize the exception."""

class RegistryValidationError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when registry validation fails."""
    def __init__((self, reason: str)) -> None:
        """Initialize the exception."""

class InvalidModelError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when a model fails integrity check."""
    def __init__((self, model_id: str)) -> None:
        """Initialize the exception."""

class InvalidModelCountError(R, e, g, i, s, t, r, y, V, a, l, i, d, a, t, i, o, n, E, r, r, o, r):
    """Raised when registry contains invalid models."""
    def __init__((self, count: int)) -> None:
        """Initialize the exception."""

class ModelRegistryError(L, M, S, t, r, i, x, E, r, r, o, r):
    """Raised when there's an error with the model registry."""

def __init__((self, message: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, endpoint: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((
        self,
        model_id: str,
        input_tokens: int,
        context_limit: int,
        details: dict | None = None,
    )) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str, available_models: list[str] | None = None)) -> None:
    """Initialize the exception."""

def __init__((self, config_name: str, reason: str, details: dict | None = None)) -> None:
    """Initialize the exception."""

def __init__((self)) -> None:
    """Initialize the exception."""

def __init__((self, field: str, value: any, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, value: int)) -> None:
    """Initialize the exception."""

def __init__((self, reason: str)) -> None:
    """Initialize the exception."""

def __init__((self, model_id: str)) -> None:
    """Initialize the exception."""

def __init__((self, count: int)) -> None:
    """Initialize the exception."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/api/main.py
# Language: python

import json
import sys
import time
from datetime import datetime
from pathlib import Path
from rich.console import Console
from rich.live import Live
from rich.table import Table
from lmstrix.api.exceptions import APIConnectionError, ModelRegistryError
from lmstrix.core.concrete_config import ConcreteConfigManager
from lmstrix.core.context_tester import ContextTester
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    scan_and_update_registry,
)
from lmstrix.loaders.prompt_loader import load_single_prompt
from lmstrix.utils import get_context_test_log_path, setup_logging
from lmstrix.utils.context_parser import get_model_max_context, parse_out_ctx
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file, get_lmstudio_path
from lmstrix.utils.state import StateManager

class LMStrixService:
    """Service layer for LMStrix operations."""
    def scan_models((
        self,
        failed: bool = False,
        reset: bool = False,
        verbose: bool = False,
    )) -> None:
        """Scan for LM Studio models and update the local registry."""
    def list_models((
        self,
        sort: str = "id",
        show: str | None = None,
        verbose: bool = False,
    )) -> None:
        """List all models from the registry with their test status."""
    def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
        """Test the context limits for models."""
    def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
        """Run inference on a specified model."""
    def check_health((self, verbose: bool = False)) -> None:
        """Check database health and backup status."""
    def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
        """Save tested context limits to LM Studio concrete config files."""
    def show_help((self)) -> None:
        """Show comprehensive help text."""

def _format_response_preview((response: str | None, max_length: int = 60)) -> str:
    """Format response text for table display."""

def _get_models_to_test((
    registry: ModelRegistry,
    test_all: bool,
    ctx: int | None,
    model_id: str | None,
    reset: bool = False,
    fast_mode: bool = False,
)) -> list[Model]:
    """Filter and return a list of models to be tested."""

def _sort_models((models: list[Model], sort_by: str)) -> list[Model]:
    """Sort a list of models based on a given key."""

def _test_single_model((
    tester: ContextTester,
    model: Model,
    ctx: int,
    registry: ModelRegistry,
    force: bool = False,
    verbose: bool = False,
)) -> None:
    """Test a single model at a specific context size."""

def _test_all_models_at_ctx((
    tester: ContextTester,
    models_to_test: list[Model],
    ctx: int,
    registry: ModelRegistry,
    verbose: bool = False,
)) -> list[Model]:
    """Test all models at a specific context size."""

def _test_all_models_optimized((
    tester: ContextTester,
    models_to_test: list[Model],
    threshold: int,
    registry: ModelRegistry,
)) -> list[Model]:
    """Run optimized batch testing for multiple models."""

def _print_final_results((updated_models: list[Model])) -> None:
    """Print the final results table."""

def scan_models((
        self,
        failed: bool = False,
        reset: bool = False,
        verbose: bool = False,
    )) -> None:
    """Scan for LM Studio models and update the local registry."""

def list_models((
        self,
        sort: str = "id",
        show: str | None = None,
        verbose: bool = False,
    )) -> None:
    """List all models from the registry with their test status."""

def test_models((
        self,
        model_id: str | None = None,
        test_all: bool = False,
        reset: bool = False,
        threshold: int = 31744,
        ctx: int | None = None,
        sort: str = "id",
        fast: bool = False,
        verbose: bool = False,
        force: bool = False,
        prompt: str | None = None,
        file_prompt: str | None = None,
    )) -> None:
    """Test the context limits for models."""

def run_inference((
        self,
        prompt: str,
        model_id: str | None = None,
        out_ctx: int | str = -1,
        in_ctx: int | str | None = None,
        reload: bool = False,
        file_prompt: str | None = None,
        dict_params: str | None = None,
        text: str | None = None,
        text_file: str | None = None,
        param_temp: float = 0.8,
        param_top_k: int = 40,
        param_top_p: float = 0.95,
        param_repeat: float = 1.1,
        param_min_p: float = 0.05,
        stream: bool = False,
        stream_timeout: int = 120,
        verbose: bool = False,
    )) -> None:
    """Run inference on a specified model."""

def print_token((token: str)) -> None:

def check_health((self, verbose: bool = False)) -> None:
    """Check database health and backup status."""

def save_configs((self, flash: bool = False, verbose: bool = False)) -> None:
    """Save tested context limits to LM Studio concrete config files."""

def show_help((self)) -> None:
    """Show comprehensive help text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/__init__.py
# Language: python

from lmstrix.core.context import ContextOptimizer, OptimizationResult
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/concrete_config.py
# Language: python

import json
from pathlib import Path
from typing import Any
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class ConcreteConfigManager:
    """Manages LM Studio concrete model configurations."""
    def __init__((self, lms_path: Path)) -> None:
        """Initialize the concrete config manager."""
    def _get_config_path((self, model: Model)) -> Path:
        """Get the path for a model's concrete config file."""
    def _create_skeleton_config((self)) -> dict[str, Any]:
        """Create the skeleton structure for a concrete config."""
    def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
        """Update or add a field in the fields list."""
    def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
        """Save a model's tested context limit to its concrete config."""
    def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
        """Save concrete configs for all models with tested contexts."""

def __init__((self, lms_path: Path)) -> None:
    """Initialize the concrete config manager."""

def _get_config_path((self, model: Model)) -> Path:
    """Get the path for a model's concrete config file."""

def _create_skeleton_config((self)) -> dict[str, Any]:
    """Create the skeleton structure for a concrete config."""

def _update_field((self, fields: list[dict[str, Any]], key: str, value: Any)) -> None:
    """Update or add a field in the fields list."""

def save_model_config((self, model: Model, enable_flash: bool = False)) -> bool:
    """Save a model's tested context limit to its concrete config."""

def save_all_configs((self, models: list[Model], enable_flash: bool = False)) -> tuple[int, int]:
    """Save concrete configs for all models with tested contexts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context.py
# Language: python

import json
from pathlib import Path
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import InferenceError, ModelLoadError
from lmstrix.core.models import Model
from lmstrix.utils.logging import logger

class OptimizationResult(B, a, s, e, M, o, d, e, l):
    """Result from context optimization."""

class ContextOptimizer:
    """Finds the maximum effective context window for models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the context optimizer."""
    def _load_cache((self)) -> dict[str, int]:
        """Load cached optimization results."""
    def _save_cache((self)) -> None:
        """Save optimization results to cache."""
    def _generate_test_prompt((self, size: int)) -> str:
        """Generate a test prompt of approximately the given token size."""
    def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
        """Test if a model can handle a specific context size."""
    def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
        """Find the optimal context size for a model using binary search."""

def succeeded((self)) -> bool:
    """Check if optimization was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        cache_file: Path | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the context optimizer."""

def _load_cache((self)) -> dict[str, int]:
    """Load cached optimization results."""

def _save_cache((self)) -> None:
    """Save optimization results to cache."""

def _generate_test_prompt((self, size: int)) -> str:
    """Generate a test prompt of approximately the given token size."""

def _test_context_size((
        self,
        model_id: str,
        context_size: int,
        test_prompt: str | None = None,
    )) -> tuple[bool, str]:
    """Test if a model can handle a specific context size."""

def find_optimal_context((
        self,
        model: Model,
        initial_size: int | None = None,
        min_size: int = 2048,
        max_attempts: int = 20,
    )) -> OptimizationResult:
    """Find the optimal context size for a model using binary search."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/context_tester.py
# Language: python

from datetime import datetime
from typing import TYPE_CHECKING, ClassVar
from lmstrix.api import LMStudioClient
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.core.models import ContextTestStatus

class ContextTestResult:
    """Result of a context test attempt."""
    def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
        """Initialize test result."""
    def to_dict((self)) -> dict:
        """Convert to dictionary for logging."""
    def is_valid_response((self)) -> bool:
        """Check if we got any response at all (not validating content)."""

class ContextTester:
    """Tests models to find their true operational context limits."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
        """Initialize context tester."""
    def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: str | None = None,
        model: "Model" = None,
        registry: "ModelRegistry" = None,
    )) -> ContextTestResult:
        """Test a model at a specific context size using InferenceEngine."""
    def _is_embedding_model((self, model)) -> bool:
        """Check if a model is an embedding model."""
    def _filter_models_for_testing((self, models: list)) -> list:
        """Filter out embedding models from the list of models to test."""
    def test_all_models((
        self,
        models_to_test: list["Model"],
        threshold: int = 4096,
        registry: "ModelRegistry" = None,
    )) -> list["Model"]:
        """Test all models to find their maximum working context."""
    def test_model_by_id((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
        """Test a model to find its maximum working context."""
    def test_model((
        self,
        model: "Model",
        max_context: int = 131072,
        registry: "ModelRegistry" = None,
    )) -> "Model":
        """Test a model to find its maximum working context."""

def __init__((
        self,
        context_size: int,
        load_success: bool,
        inference_success: bool = False,
        prompt: str = "",
        response: str = "",
        error: str = "",
    )) -> None:
    """Initialize test result."""

def to_dict((self)) -> dict:
    """Convert to dictionary for logging."""

def is_valid_response((self)) -> bool:
    """Check if we got any response at all (not validating content)."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        verbose: bool = False,
        fast_mode: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
    """Initialize context tester."""

def _test_at_context((
        self,
        model_id: str,
        context_size: int,
        log_path: str | None = None,
        model: "Model" = None,
        registry: "ModelRegistry" = None,
    )) -> ContextTestResult:
    """Test a model at a specific context size using InferenceEngine."""

def _is_embedding_model((self, model)) -> bool:
    """Check if a model is an embedding model."""

def _filter_models_for_testing((self, models: list)) -> list:
    """Filter out embedding models from the list of models to test."""

def test_all_models((
        self,
        models_to_test: list["Model"],
        threshold: int = 4096,
        registry: "ModelRegistry" = None,
    )) -> list["Model"]:
    """Test all models to find their maximum working context."""

def test_model_by_id((
        self,
        model_id: str,
        min_context: int = 512,
        max_context: int = 131072,
    )) -> dict:
    """Test a model to find its maximum working context."""

def test_model((
        self,
        model: "Model",
        max_context: int = 131072,
        registry: "ModelRegistry" = None,
    )) -> "Model":
    """Test a model to find its maximum working context."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference.py
# Language: python

import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from pydantic import BaseModel, Field
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger

class InferenceResult(B, a, s, e, M, o, d, e, l):
    """Result from an inference run."""

class InferenceEngine:
    """Engine for running inference on models."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
        """Initialize the inference engine."""
    def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""
    def _find_working_context((self, model_id: str, initial_context: int)) -> int:
        """Find the maximum working context length for a model."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
        """Run inference on a model with automatic context optimization."""
    def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds
        **kwargs: Any,
    )) -> Iterator[str]:
        """Stream inference on a model with automatic context optimization."""

def succeeded((self)) -> bool:
    """Check if the inference was successful."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        model_registry: ModelRegistry | None = None,
        verbose: bool = False,
        custom_prompt: str | None = None,
    )) -> None:
    """Initialize the inference engine."""

def _test_inference_capability((self, model_id: str, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""

def _find_working_context((self, model_id: str, initial_context: int)) -> int:
    """Find the maximum working context length for a model."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        **kwargs: Any,
    )) -> InferenceResult:
    """Run inference on a model with automatic context optimization."""

def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,  # Use -1 for unlimited as per new client
        temperature: float = 0.7,
        on_token: Callable[[str], None] | None = None,  # Callback for each token
        stream_timeout: int = 120,  # Timeout in seconds
        **kwargs: Any,
    )) -> Iterator[str]:
    """Stream inference on a model with automatic context optimization."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/inference_manager.py
# Language: python

import builtins
import contextlib
import time
from collections.abc import Callable, Iterator
from typing import Any
import lmstudio
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils.logging import logger

class InferenceManager:
    """Unified manager for model inference operations."""
    def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
        """Initialize the inference manager."""
    def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
        """Run inference on a model."""
    def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
        """Test if model can do basic inference at given context length."""
    def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        on_token: Callable[[str], None] | None = None,
        stream_timeout: int = 120,
        **kwargs: Any,
    )) -> Iterator[str]:
        """Stream inference on a model."""

def __init__((
        self,
        client: LMStudioClient | None = None,
        registry: ModelRegistry | None = None,
        verbose: bool = False,
    )) -> None:
    """Initialize the inference manager."""

def infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        **kwargs: Any,
    )) -> dict[str, Any]:
    """Run inference on a model."""

def test_inference_capability((self, model: Model, context_len: int)) -> tuple[bool, str]:
    """Test if model can do basic inference at given context length."""

def stream_infer((
        self,
        model_id: str,
        prompt: str,
        in_ctx: int | None = None,
        out_ctx: int = -1,
        temperature: float = 0.8,
        top_k: int = 40,
        top_p: float = 0.95,
        repeat_penalty: float = 1.1,
        min_p: float = 0.05,
        on_token: Callable[[str], None] | None = None,
        stream_timeout: int = 120,
        **kwargs: Any,
    )) -> Iterator[str]:
    """Stream inference on a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/models.py
# Language: python

import json
import re
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file

class ContextTestStatus(s, t, r, ,,  , E, n, u, m):
    """Status of context testing for a model."""

class Model:
    """Represents a model in the registry."""
    def __init__((
        self,
        model_id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
        """Initialize a model with essential fields only."""
    def to_dict((self)) -> dict[str, Any]:
        """Convert model to dictionary for JSON storage."""
    def to_registry_dict((self)) -> dict[str, Any]:
        """Alias for to_dict() for backward compatibility."""
    def reset_test_data((self)) -> None:
        """Reset all context testing data."""
    def validate_integrity((self)) -> bool:
        """Validate the model's integrity."""
    def sanitized_id((self)) -> str:
        """Return a sanitized version of the model ID for filenames."""

class ModelRegistryError(E, x, c, e, p, t, i, o, n):
    """Exception raised for model registry errors."""

class ModelRegistry:
    """Simplified model registry without complex validation."""
    def __init__((self, models_file: Path | None = None)) -> None:
        """Initialize the model registry."""
    def _get_default_models_file((self)) -> Path:
        """Get the default models file path."""
    def load((self)) -> None:
        """Load models from JSON file."""
    def save((self)) -> None:
        """Save models to JSON file."""
    def add_model((self, model: Model)) -> None:
        """Add a model to the registry."""
    def update_model((self, model_id: str, model: Model)) -> None:
        """Update a model in the registry (adds if not exists for compatibility)."""
    def update_model_by_id((self, model: Model)) -> None:
        """Update a model using its own ID, finding existing entry by path or ID."""
    def get_model((self, model_id: str)) -> Model | None:
        """Get a model by ID."""
    def find_model((self, model_identifier: str)) -> Model | None:
        """Find a model by ID or path."""
    def list_models((self)) -> list[Model]:
        """List all models in the registry."""
    def remove_model((self, model_id: str)) -> bool:
        """Remove a model from the registry."""
    def clear((self)) -> None:
        """Clear all models from the registry."""
    def __len__((self)) -> int:
        """Return the number of models."""
    def __contains__((self, model_id: str)) -> bool:
        """Check if a model exists."""

def __init__((
        self,
        model_id: str,
        path: str,
        size_bytes: int,
        ctx_in: int,
        ctx_out: int = 4096,
        has_tools: bool = False,
        has_vision: bool = False,
        tested_max_context: int | None = None,
        context_test_status: str = "untested",
        context_test_date: str | None = None,
        **kwargs: Any,  # Ignore extra fields
    )) -> None:
    """Initialize a model with essential fields only."""

def to_dict((self)) -> dict[str, Any]:
    """Convert model to dictionary for JSON storage."""

def to_registry_dict((self)) -> dict[str, Any]:
    """Alias for to_dict() for backward compatibility."""

def reset_test_data((self)) -> None:
    """Reset all context testing data."""

def validate_integrity((self)) -> bool:
    """Validate the model's integrity."""

def sanitized_id((self)) -> str:
    """Return a sanitized version of the model ID for filenames."""

def __init__((self, models_file: Path | None = None)) -> None:
    """Initialize the model registry."""

def _get_default_models_file((self)) -> Path:
    """Get the default models file path."""

def load((self)) -> None:
    """Load models from JSON file."""

def save((self)) -> None:
    """Save models to JSON file."""

def add_model((self, model: Model)) -> None:
    """Add a model to the registry."""

def update_model((self, model_id: str, model: Model)) -> None:
    """Update a model in the registry (adds if not exists for compatibility)."""

def update_model_by_id((self, model: Model)) -> None:
    """Update a model using its own ID, finding existing entry by path or ID."""

def get_model((self, model_id: str)) -> Model | None:
    """Get a model by ID."""

def find_model((self, model_identifier: str)) -> Model | None:
    """Find a model by ID or path."""

def list_models((self)) -> list[Model]:
    """List all models in the registry."""

def remove_model((self, model_id: str)) -> bool:
    """Remove a model from the registry."""

def clear((self)) -> None:
    """Clear all models from the registry."""

def __len__((self)) -> int:
    """Return the number of models."""

def __contains__((self, model_id: str)) -> bool:
    """Check if a model exists."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/prompts.py
# Language: python

import re
from collections.abc import Mapping
from types import MappingProxyType
from typing import Any
import tiktoken
from pydantic import BaseModel, Field
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

class ResolvedPrompt(B, a, s, e, M, o, d, e, l):
    """A prompt after placeholder resolution."""

class PromptResolver:
    """Handles two-phase placeholder resolution for prompt templates."""
    def __init__((self, verbose: bool = False)) -> None:
        """Initialize the prompt resolver."""
    def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
        """Get value at dotted path from nested dict."""
    def _find_placeholders((self, text: str)) -> list[str]:
        """Find all placeholders in text."""
    def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
        """Replace one pass of internal placeholders."""
    def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
        """Replace external placeholders using provided parameters."""
    def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
        """Resolve a single prompt template."""
    def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
        """Resolve all prompts in the data."""
    def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
        """Truncate text to fit within token limit."""
    def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
        """Inject context into a prompt, with optional truncation."""
    def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
        """Resolve placeholders in a single phase."""
    def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
        """Resolve a template with both prompt and runtime contexts."""
    def _count_tokens((self, text: str)) -> int:
        """Count tokens in text."""

class _SafeDict(d, i, c, t):
    """Dict that leaves unknown placeholders unchanged."""
    def __missing__((self, key: str)) -> str:

def __init__((self, verbose: bool = False)) -> None:
    """Initialize the prompt resolver."""

def _get_by_path((self, data: dict[str, Any], dotted_path: str)) -> Any:
    """Get value at dotted path from nested dict."""

def _find_placeholders((self, text: str)) -> list[str]:
    """Find all placeholders in text."""

def _resolve_internal_once((self, text: str, root: dict[str, Any])) -> tuple[str, list[str]]:
    """Replace one pass of internal placeholders."""

def repl((match: re.Match[str])) -> str:

def _resolve_external((self, text: str, params: Mapping[str, str])) -> tuple[str, list[str]]:
    """Replace external placeholders using provided parameters."""

def __missing__((self, key: str)) -> str:

def resolve_prompt((
        self,
        prompt_name: str,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> ResolvedPrompt:
    """Resolve a single prompt template."""

def resolve_all_prompts((
        self,
        prompts_data: dict[str, Any],
        **params: str,
    )) -> dict[str, ResolvedPrompt]:
    """Resolve all prompts in the data."""

def process_prompts((data: dict[str, Any], prefix: str = "")) -> None:

def truncate_to_limit((
        self,
        text: str,
        limit: int,
        strategy: str = "end",
    )) -> str:
    """Truncate text to fit within token limit."""

def inject_context((
        self,
        prompt: str,
        context: str,
        context_placeholder: str = "{{text}}",
        out_ctx: int | None = None,
    )) -> str:
    """Inject context into a prompt, with optional truncation."""

def _resolve_phase((
        self,
        template: str,
        context: dict[str, Any],
    )) -> tuple[str, list[str], list[str], list[str]]:
    """Resolve placeholders in a single phase."""

def resolve_template((
        self,
        name: str,
        template: str,
        prompt_context: dict[str, Any],
        runtime_context: dict[str, Any],
    )) -> ResolvedPrompt:
    """Resolve a template with both prompt and runtime contexts."""

def _count_tokens((self, text: str)) -> int:
    """Count tokens in text."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/core/scanner.py
# Language: python

from pathlib import Path
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.utils import get_lmstudio_path
from lmstrix.utils.logging import logger

class ModelScanner:
    """Scans LM Studio for available models."""
    def __init__((self)) -> None:
        """Initialize scanner."""
    def _get_model_size((self, model_path: Path)) -> int:
        """Get size of model file(s)."""
    def _extract_model_info((self, model_path: Path)) -> dict | None:
        """Extract model information from path."""
    def scan_models((self)) -> dict[str, dict]:
        """Scan for all models in LM Studio."""
    def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
        """Update model registry with scanned models."""
    def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
        """Sync scanned models with registry."""

def __init__((self)) -> None:
    """Initialize scanner."""

def _get_model_size((self, model_path: Path)) -> int:
    """Get size of model file(s)."""

def _extract_model_info((self, model_path: Path)) -> dict | None:
    """Extract model information from path."""

def scan_models((self)) -> dict[str, dict]:
    """Scan for all models in LM Studio."""

def update_registry((self, registry: ModelRegistry | None = None)) -> ModelRegistry:
    """Update model registry with scanned models."""

def sync_with_registry((self, registry: ModelRegistry)) -> tuple[list[str], list[str]]:
    """Sync scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/__init__.py
# Language: python

from lmstrix.loaders.context_loader import (
    estimate_tokens,
    load_context,
    load_context_with_limit,
    load_multiple_contexts,
    save_context,
)
from lmstrix.loaders.model_loader import load_model_registry, save_model_registry
from lmstrix.loaders.prompt_loader import load_prompts, load_single_prompt, save_prompts
from lmstrix.utils.logging import logger


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/context_loader.py
# Language: python

from pathlib import Path
import tiktoken
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.utils.logging import logger

def load_context((
    file_path: str | Path,
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load context text from a file."""

def load_multiple_contexts((
    file_paths: list[str | Path],
    separator: str = "\n\n",
    encoding: str = "utf-8",
    verbose: bool = False,
)) -> str:
    """Load and concatenate multiple context files."""

def estimate_tokens((
    text: str,
    model_encoding: str = "cl100k_base",
)) -> int:
    """Estimate the number of tokens in a text."""

def load_context_with_limit((
    file_path: str | Path,
    out_ctx: int,
    encoding: str = "utf-8",
    model_encoding: str = "cl100k_base",
    verbose: bool = False,
)) -> tuple[str, int, bool]:
    """Load context with a token limit."""

def save_context((
    content: str,
    file_path: str | Path,
    encoding: str = "utf-8",
)) -> None:
    """Save context to a file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/model_loader.py
# Language: python

from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.api.exceptions import APIConnectionError
from lmstrix.core.models import (
    ContextTestStatus,
    Model,
    ModelRegistry,
    ModelRegistryError,
)
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_default_models_file

def load_model_registry((
    json_path: Path | None = None,
    verbose: bool = False,
)) -> ModelRegistry:
    """Load the model registry from a JSON file."""

def save_model_registry((
    registry: ModelRegistry,
    json_path: Path | None = None,
)) -> Path:
    """Save a ModelRegistry to a JSON file."""

def _validate_discovered_model((model_data: dict)) -> bool:
    """Validate that discovered model data is reasonable before processing."""

def _update_existing_model((
    existing_model: Model,
    model_data: dict,
    rescan_all: bool,
    rescan_failed: bool,
)) -> Model:
    """Update an existing model's data and handle rescan options."""

def _add_new_model((model_data: dict)) -> Model | None:
    """Create a new model entry from discovered data."""

def _remove_deleted_models((registry: ModelRegistry, discovered_models: list[dict])) -> None:
    """Remove models from the registry that are no longer discovered."""

def scan_and_update_registry((
    rescan_failed: bool = False,
    rescan_all: bool = False,
    verbose: bool = False,
)) -> ModelRegistry:
    """Scan for downloaded LM Studio models and update the local registry."""

def reset_test_data((
    model_identifier: str,
    verbose: bool = False,
)) -> bool:
    """Reset test data for a specific model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/loaders/prompt_loader.py
# Language: python

from collections.abc import Iterable
from pathlib import Path
from typing import Any
import tomllib
import tomli as tomllib
import tomli_w
import tomlkit
import toml
from topl.core import resolve_placeholders
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.utils.logging import logger

def load_prompts((
    toml_path: Path,
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> dict[str, ResolvedPrompt]:
    """Load and resolve prompts from a TOML file."""

def load_single_prompt((
    toml_path: Path,
    prompt_name: str | Iterable[str],
    resolver: PromptResolver | None = None,
    verbose: bool = False,
    **params: str,
)) -> ResolvedPrompt:
    """Load and resolve a single prompt from a TOML file."""

def save_prompts((
    prompts: dict[str, Any],
    toml_path: Path,
)) -> None:
    """Save prompts to a TOML file."""


<document index="39">
<source>src/lmstrix/py.typed</source>
<document_content>
# This file indicates that the package supports type hints
# as specified in PEP 561.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/__init__.py
# Language: python

from lmstrix.utils.logging import logger, setup_logging
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstrix_log_path,
    get_lmstudio_path,
    get_prompts_dir,
)


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/context_parser.py
# Language: python

from typing import Any
from lmstrix.utils.logging import logger

def parse_out_ctx((
    out_ctx: int | str,
    max_context: int,
    fallback_context: int | None = None,
)) -> int:
    """Parse out_ctx parameter which can be an integer or percentage string."""

def get_model_max_context((model: Any, use_tested: bool = True)) -> int | None:
    """Get the maximum context for a model."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/logging.py
# Language: python

import sys
from typing import Any
from loguru import logger

def setup_logging((verbose: bool = False)) -> None:
    """Configure loguru logging based on verbose flag."""

def format_log((record: Any)) -> str:
    """Custom formatter for colored output with level shortcuts."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/paths.py
# Language: python

from pathlib import Path
from lmstrix.api.exceptions import LMStudioInstallationNotFoundError
from lmstrix.utils.logging import logger

def get_lmstudio_path(()) -> Path:
    """Get the LM Studio installation path."""

def get_lmstrix_data_dir(()) -> Path:
    """Get the LMStrix data directory within LM Studio."""

def get_default_models_file(()) -> Path:
    """Get the path to the lmstrix.json registry file."""

def get_context_tests_dir(()) -> Path:
    """Get the directory for context test logs."""

def get_context_test_log_path((model_id: str)) -> Path:
    """Get the path for a specific model's context test log."""

def get_prompts_dir(()) -> Path:
    """Get the directory for prompts."""

def get_contexts_dir(()) -> Path:
    """Get the directory for contexts."""

def get_lmstrix_log_path(()) -> Path:
    """Get the path to the lmstrix.log.txt file."""


# File: /Users/Shared/lmstudio/lmstrix/src/lmstrix/utils/state.py
# Language: python

import json
from pathlib import Path
from lmstrix.utils.logging import logger
from lmstrix.utils.paths import get_lmstudio_path

class StateManager:
    """Manages persistent state for LMStrix."""
    def __init__((self)) -> None:
        """Initialize the state manager."""
    def _load_state((self)) -> dict:
        """Load state from file."""
    def _save_state((self)) -> None:
        """Save state to file."""
    def get_last_used_model((self)) -> str | None:
        """Get the last used model ID."""
    def set_last_used_model((self, model_id: str)) -> None:
        """Set the last used model ID."""
    def clear_last_used_model((self)) -> None:
        """Clear the last used model ID."""

def __init__((self)) -> None:
    """Initialize the state manager."""

def _load_state((self)) -> dict:
    """Load state from file."""

def _save_state((self)) -> None:
    """Save state to file."""

def get_last_used_model((self)) -> str | None:
    """Get the last used model ID."""

def set_last_used_model((self, model_id: str)) -> None:
    """Set the last used model ID."""

def clear_last_used_model((self)) -> None:
    """Clear the last used model ID."""


# File: /Users/Shared/lmstudio/lmstrix/test_model_out_ctx.py
# Language: python

import sys
from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.core.inference import InferenceEngine
from lmstrix.core.models import ModelRegistry
from lmstrix.utils.logging import logger
import traceback

def test_context_out_usage(()) -> None:
    """Test that model-specific context_out values are used in testing."""

def compare_inference_modes(()) -> None:
    """Compare inference with different out_ctx values."""


<document index="40">
<source>test_prompt_example.txt</source>
<document_content>
Write a haiku about artificial intelligence.
</document_content>
</document>

# File: /Users/Shared/lmstudio/lmstrix/test_streaming.py
# Language: python

import sys
from pathlib import Path
from lmstrix.api.client import LMStudioClient
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import ModelRegistry

def test_streaming(()) -> None:
    """Test streaming inference."""


# File: /Users/Shared/lmstudio/lmstrix/tests/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/conftest.py
# Language: python

import asyncio
import sys
from collections.abc import Generator
from pathlib import Path
from typing import Any
from unittest.mock import Mock
import pytest

def mock_lmstudio_client(()) -> Mock:
    """Mock LMStudioClient for testing."""

def mock_llm(()) -> Mock:
    """Mock LLM object returned by lmstudio.llm()."""

def sample_model_data(()) -> dict[str, Any]:
    """Sample model data for testing."""

def tmp_models_dir((tmp_path: Path)) -> Path:
    """Create a temporary models directory."""

def tmp_registry_file((tmp_path: Path)) -> Path:
    """Create a temporary registry file path."""

def event_loop(()) -> Generator[asyncio.AbstractEventLoop, None, None]:
    """Create an instance of the default event loop for the test session."""

def mock_completion_response(()) -> dict[str, Any]:
    """Mock completion response from LM Studio."""

def mock_prompt_template(()) -> dict[str, Any]:
    """Sample prompt template for testing."""

def mock_context_data(()) -> dict[str, str]:
    """Sample context data for testing."""


# File: /Users/Shared/lmstudio/lmstrix/tests/run_tests.py
# Language: python

import subprocess
import sys
from lmstrix.utils.logging import logger

def run_tests(()) -> int:
    """Run the test suite."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_client.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.client import CompletionResponse, LMStudioClient
from lmstrix.api.exceptions import APIConnectionError, InferenceError, ModelLoadError

class TestCompletionResponse:
    """Test CompletionResponse model."""
    def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse."""
    def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
        """Test creating a CompletionResponse with minimal fields."""

class TestLMStudioClient:
    """Test LMStudioClient class."""
    def test_client_initialization((self: "TestLMStudioClient")) -> None:
        """Test client initialization with different verbose settings."""
    def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test successful completion."""
    def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
        """Test completion failure."""
    def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
        """Test completion with default parameters."""

def test_completion_response_creation((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse."""

def test_completion_response_minimal((self: "TestCompletionResponse")) -> None:
    """Test creating a CompletionResponse with minimal fields."""

def test_client_initialization((self: "TestLMStudioClient")) -> None:
    """Test client initialization with different verbose settings."""

def test_list_models_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful list_models call."""

def test_list_models_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test list_models with connection error."""

def test_load_model_success((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test successful model loading."""

def test_load_model_failure((self: "TestLMStudioClient", mock_lmstudio: Mock)) -> None:
    """Test model loading failure."""

def test_completion_success((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test successful completion."""

def test_completion_failure((
        self: "TestLMStudioClient",
        mock_llm: Mock,
    )) -> None:
    """Test completion failure."""

def test_completion_with_defaults((
        self: "TestLMStudioClient",
        mock_llm: Mock,
        mock_completion_response: CompletionResponse,
    )) -> None:
    """Test completion with default parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_api/test_exceptions.py
# Language: python

from lmstrix.api.exceptions import (
    APIConnectionError,
    InferenceError,
    LMStrixError,
    ModelLoadError,
)

class TestAPIExceptions:
    """Test API exception classes."""
    def test_api_error_base((self)) -> None:
        """Test base LMStrixError class."""
    def test_api_connection_error((self)) -> None:
        """Test APIConnectionError creation and attributes."""
    def test_model_load_error((self)) -> None:
        """Test ModelLoadError creation and attributes."""
    def test_inference_error((self)) -> None:
        """Test InferenceError creation and attributes."""
    def test_exception_inheritance((self)) -> None:
        """Test that all exceptions inherit from lmstrixError."""

def test_api_error_base((self)) -> None:
    """Test base LMStrixError class."""

def test_api_connection_error((self)) -> None:
    """Test APIConnectionError creation and attributes."""

def test_model_load_error((self)) -> None:
    """Test ModelLoadError creation and attributes."""

def test_inference_error((self)) -> None:
    """Test InferenceError creation and attributes."""

def test_exception_inheritance((self)) -> None:
    """Test that all exceptions inherit from lmstrixError."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_context_tester.py
# Language: python

from datetime import datetime
from unittest.mock import Mock, patch
from lmstrix.api.exceptions import ModelLoadError
from lmstrix.core.context_tester import ContextTester, ContextTestResult
from lmstrix.core.models import Model

class TestContextTestResult:
    """Test ContextTestResult data class."""
    def test_result_creation((self)) -> None:
        """Test creating a context test result."""
    def test_result_to_dict((self)) -> None:
        """Test converting result to dictionary."""

class TestContextTester:
    """Test ContextTester PUBLIC API."""
    def test_initialization((self)) -> None:
        """Test context tester initialization."""
    def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test initialization with custom client."""
    def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
        """Test testing a model that doesn't exist."""
    def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
        """Test batch testing of models."""

def test_result_creation((self)) -> None:
    """Test creating a context test result."""

def test_result_to_dict((self)) -> None:
    """Test converting result to dictionary."""

def test_initialization((self)) -> None:
    """Test context tester initialization."""

def test_initialization_with_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test initialization with custom client."""

def test_test_model_not_found((self, mock_lmstudio_client: Mock)) -> None:
    """Test testing a model that doesn't exist."""

def test_test_all_models((self, mock_lmstudio_client: Mock)) -> None:
    """Test batch testing of models."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_inference.py
# Language: python

from unittest.mock import Mock, patch
import pytest
from lmstrix.api.exceptions import ModelNotFoundError
from lmstrix.core.inference_manager import InferenceManager
from lmstrix.core.models import Model

class TestInferenceDict:
    """Test inference result dictionary structure."""
    def test_inference_dict_success((self)) -> None:
        """Test successful inference result dict."""
    def test_inference_dict_failure((self)) -> None:
        """Test failed inference result dict."""
    def test_inference_dict_empty_response((self)) -> None:
        """Test inference result dict with empty response."""

class TestInferenceManager:
    """Test InferenceManager PUBLIC API."""
    def test_manager_initialization((self)) -> None:
        """Test inference manager initialization."""
    def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
        """Test manager with custom client."""
    def test_infer_model_not_found((self)) -> None:
        """Test inference with non-existent model."""
    def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
        """Test basic successful inference."""

def test_inference_dict_success((self)) -> None:
    """Test successful inference result dict."""

def test_inference_dict_failure((self)) -> None:
    """Test failed inference result dict."""

def test_inference_dict_empty_response((self)) -> None:
    """Test inference result dict with empty response."""

def test_manager_initialization((self)) -> None:
    """Test inference manager initialization."""

def test_manager_with_custom_client((self, mock_lmstudio_client: Mock)) -> None:
    """Test manager with custom client."""

def test_infer_model_not_found((self)) -> None:
    """Test inference with non-existent model."""

def test_infer_basic_success((self, mock_lmstudio_client: Mock)) -> None:
    """Test basic successful inference."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_models.py
# Language: python

import json
from datetime import datetime
from pathlib import Path
from typing import Any
import pytest
from lmstrix.core.models import ContextTestStatus, Model, ModelRegistry

class TestContextTestStatus:
    """Test ContextTestStatus enum."""
    def test_enum_values((self: "TestContextTestStatus")) -> None:
        """Test that all expected enum values exist."""

class TestModel:
    """Test Model class."""
    def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test creating a model with minimal required fields."""
    def test_model_creation_with_aliases((self: "TestModel")) -> None:
        """Test model creation using field aliases."""
    def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test model with context testing information."""
    def test_model_path_validation((self: "TestModel")) -> None:
        """Test that path field accepts both string and Path objects."""
    def test_model_sanitized_id((self: "TestModel")) -> None:
        """Test sanitized_id method."""
    def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
        """Test converting model to registry dictionary format."""
    def test_model_validation_error((self: "TestModel")) -> None:
        """Test that model validation raises appropriate errors."""

class TestModelRegistry:
    """Test ModelRegistry class."""
    def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
        """Test registry initialization with custom file path."""
    def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving and loading models."""
    def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test getting a model by ID."""
    def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test listing all models."""
    def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test removing a model."""
    def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test saving/loading models with context test information."""
    def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
        """Test that the saved JSON has the expected format."""

def test_enum_values((self: "TestContextTestStatus")) -> None:
    """Test that all expected enum values exist."""

def test_model_creation_minimal((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test creating a model with minimal required fields."""

def test_model_creation_with_aliases((self: "TestModel")) -> None:
    """Test model creation using field aliases."""

def test_model_with_context_testing((
        self: "TestModel",
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test model with context testing information."""

def test_model_path_validation((self: "TestModel")) -> None:
    """Test that path field accepts both string and Path objects."""

def test_model_sanitized_id((self: "TestModel")) -> None:
    """Test sanitized_id method."""

def test_model_to_registry_dict((self: "TestModel", sample_model_data: dict[str, Any])) -> None:
    """Test converting model to registry dictionary format."""

def test_model_validation_error((self: "TestModel")) -> None:
    """Test that model validation raises appropriate errors."""

def test_registry_initialization((self: "TestModelRegistry", tmp_registry_file: Path)) -> None:
    """Test registry initialization with custom file path."""

def test_registry_save_and_load((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving and loading models."""

def test_registry_get_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test getting a model by ID."""

def test_registry_list_models((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test listing all models."""

def test_registry_remove_model((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test removing a model."""

def test_registry_with_context_test_data((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test saving/loading models with context test information."""

def test_registry_json_format((
        self: "TestModelRegistry",
        tmp_registry_file: Path,
        sample_model_data: dict[str, Any],
    )) -> None:
    """Test that the saved JSON has the expected format."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_prompts.py
# Language: python

from lmstrix.core.prompts import PromptResolver, ResolvedPrompt

class TestResolvedPrompt:
    """Test ResolvedPrompt model."""
    def test_resolved_prompt_creation((self)) -> None:
        """Test creating a resolved prompt."""
    def test_resolved_prompt_minimal((self)) -> None:
        """Test creating resolved prompt with minimal fields."""

class TestPromptResolver:
    """Test PromptResolver class."""
    def test_resolver_initialization((self)) -> None:
        """Test resolver initialization."""
    def test_find_placeholders((self)) -> None:
        """Test finding placeholders in templates."""
    def test_resolve_phase_simple((self)) -> None:
        """Test simple single-phase resolution."""
    def test_resolve_phase_missing_placeholder((self)) -> None:
        """Test resolution with missing placeholders."""
    def test_resolve_phase_extra_context((self)) -> None:
        """Test resolution with extra context values."""
    def test_resolve_template_two_phase((self)) -> None:
        """Test two-phase template resolution."""
    def test_resolve_template_recursive((self)) -> None:
        """Test recursive placeholder resolution."""
    def test_resolve_template_circular_reference((self)) -> None:
        """Test handling of circular references."""
    def test_resolve_template_no_placeholders((self)) -> None:
        """Test template with no placeholders."""
    def test_resolve_template_numeric_values((self)) -> None:
        """Test resolution with numeric values."""
    def test_resolve_template_empty_value((self)) -> None:
        """Test resolution with empty string values."""
    def test_count_tokens((self)) -> None:
        """Test token counting."""
    def test_resolve_with_special_characters((self)) -> None:
        """Test resolution with special characters in values."""

def test_resolved_prompt_creation((self)) -> None:
    """Test creating a resolved prompt."""

def test_resolved_prompt_minimal((self)) -> None:
    """Test creating resolved prompt with minimal fields."""

def test_resolver_initialization((self)) -> None:
    """Test resolver initialization."""

def test_find_placeholders((self)) -> None:
    """Test finding placeholders in templates."""

def test_resolve_phase_simple((self)) -> None:
    """Test simple single-phase resolution."""

def test_resolve_phase_missing_placeholder((self)) -> None:
    """Test resolution with missing placeholders."""

def test_resolve_phase_extra_context((self)) -> None:
    """Test resolution with extra context values."""

def test_resolve_template_two_phase((self)) -> None:
    """Test two-phase template resolution."""

def test_resolve_template_recursive((self)) -> None:
    """Test recursive placeholder resolution."""

def test_resolve_template_circular_reference((self)) -> None:
    """Test handling of circular references."""

def test_resolve_template_no_placeholders((self)) -> None:
    """Test template with no placeholders."""

def test_resolve_template_numeric_values((self)) -> None:
    """Test resolution with numeric values."""

def test_resolve_template_empty_value((self)) -> None:
    """Test resolution with empty string values."""

def test_count_tokens((self)) -> None:
    """Test token counting."""

def test_resolve_with_special_characters((self)) -> None:
    """Test resolution with special characters in values."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_core/test_scanner.py
# Language: python

from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.scanner import ModelScanner

class TestModelScanner:
    """Test ModelScanner class."""
    def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a single model file."""
    def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of a model directory."""
    def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test getting size of non-existent path."""
    def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from GGUF model file."""
    def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test extracting info from MLX model directory."""
    def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that hidden files are skipped."""
    def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
        """Test that non-model files are skipped."""

def test_scanner_initialization((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanner initialization."""

def test_get_model_size_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a single model file."""

def test_get_model_size_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of a model directory."""

def test_get_model_size_nonexistent((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test getting size of non-existent path."""

def test_extract_model_info_gguf_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from GGUF model file."""

def test_extract_model_info_mlx_directory((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test extracting info from MLX model directory."""

def test_extract_model_info_hidden_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that hidden files are skipped."""

def test_extract_model_info_non_model_file((self: "TestModelScanner", tmp_path: Path)) -> None:
    """Test that non-model files are skipped."""

def test_scan_models((self: "TestModelScanner", mock_get_path: Mock, tmp_path: Path)) -> None:
    """Test scanning for models."""

def test_sync_with_registry((
        self: "TestModelScanner",
        mock_get_path: Mock,
        tmp_path: Path,
    )) -> None:
    """Test syncing scanned models with registry."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_integration/test_cli_integration.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
import pytest
from lmstrix.__main__ import LMStrixCLI

class TestCLIIntegration:
    """Test CLI integration - basic functionality only."""
    def test_cli_initialization((self)) -> None:
        """Test CLI can be initialized."""
    def test_infer_requires_parameters((self)) -> None:
        """Test infer command validates required parameters."""

def mock_registry((self, tmp_path: Path)) -> Path:
    """Create a mock model registry."""

def test_cli_initialization((self)) -> None:
    """Test CLI can be initialized."""

def test_list_command((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command shows models."""

def test_list_json_format((self, mock_get_file: Mock, mock_registry: Path, capsys)) -> None:
    """Test list command with JSON output."""

def test_infer_requires_parameters((self)) -> None:
    """Test infer command validates required parameters."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_context_loader.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.loaders.context_loader import load_context

class TestContextLoader:
    """Test context loading functions."""
    def test_load_context_simple((self, tmp_path: Path)) -> None:
        """Test loading simple text context."""
    def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
        """Test loading context with specific encoding."""
    def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
        """Test loading from non-existent file."""
    def test_load_context_read_error((self, tmp_path: Path)) -> None:
        """Test handling read errors."""
    def test_load_context_string_path((self, tmp_path: Path)) -> None:
        """Test loading context with string path."""
    def test_load_context_large_file((self, tmp_path: Path)) -> None:
        """Test loading large context file."""

def test_load_context_simple((self, tmp_path: Path)) -> None:
    """Test loading simple text context."""

def test_load_context_with_encoding((self, tmp_path: Path)) -> None:
    """Test loading context with specific encoding."""

def test_load_context_nonexistent_file((self, tmp_path: Path)) -> None:
    """Test loading from non-existent file."""

def test_load_context_read_error((self, tmp_path: Path)) -> None:
    """Test handling read errors."""

def test_load_context_string_path((self, tmp_path: Path)) -> None:
    """Test loading context with string path."""

def test_load_context_large_file((self, tmp_path: Path)) -> None:
    """Test loading large context file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_model_loader.py
# Language: python

import json
from pathlib import Path
from unittest.mock import Mock, patch
from lmstrix.core.models import Model, ModelRegistry
from lmstrix.loaders.model_loader import (
    load_model_registry,
    save_model_registry,
    scan_and_update_registry,
)

class TestModelLoader:
    """Test model loading functions."""
    def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with default path."""
    def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry with custom path."""
    def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test loading registry when file doesn't exist."""
    def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry with default path."""
    def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
        """Test saving registry to custom path."""

def test_load_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with default path."""

def test_load_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry with custom path."""

def test_load_model_registry_nonexistent_file((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test loading registry when file doesn't exist."""

def test_save_model_registry_default_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry with default path."""

def test_save_model_registry_custom_path((self: "TestModelLoader", tmp_path: Path)) -> None:
    """Test saving registry to custom path."""

def test_scan_and_update_models((
        self: "TestModelLoader",
        mock_save_registry: Mock,
        mock_load_registry: Mock,
        mock_client_class: Mock,
        tmp_path: Path,
    )) -> None:
    """Test scanning and updating models."""

def test_scan_and_update_models_default_client((
        self: "TestModelLoader",
        mock_load_registry: Mock,
        mock_client_class: Mock,
    )) -> None:
    """Test scan_and_update_models creates default client if none provided."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_loaders/test_prompt_loader.py
# Language: python

from pathlib import Path
import pytest
import toml
from lmstrix.api.exceptions import ConfigurationError
from lmstrix.core.prompts import PromptResolver, ResolvedPrompt
from lmstrix.loaders.prompt_loader import load_prompts

class TestPromptLoader:
    """Test prompt loading functions."""
    def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading simple prompts from TOML file."""
    def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from non-existent file."""
    def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from invalid TOML file."""
    def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
        """Test loading prompts with nested placeholders."""
    def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with missing parameters."""
    def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts with custom resolver."""
    def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
        """Test loading prompts from empty TOML file."""

def test_load_prompts_simple((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading simple prompts from TOML file."""

def test_load_prompts_nonexistent_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from non-existent file."""

def test_load_prompts_invalid_toml((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from invalid TOML file."""

def test_load_prompts_with_nested_placeholders((
        self: "TestPromptLoader",
        tmp_path: Path,
    )) -> None:
    """Test loading prompts with nested placeholders."""

def test_load_prompts_with_missing_params((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with missing parameters."""

def test_load_prompts_with_custom_resolver((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts with custom resolver."""

def test_load_prompts_empty_file((self: "TestPromptLoader", tmp_path: Path)) -> None:
    """Test loading prompts from empty TOML file."""


# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/__init__.py
# Language: python



# File: /Users/Shared/lmstudio/lmstrix/tests/test_utils/test_paths.py
# Language: python

from pathlib import Path
from unittest.mock import patch
import pytest
from lmstrix.utils.paths import (
    get_context_test_log_path,
    get_context_tests_dir,
    get_contexts_dir,
    get_default_models_file,
    get_lmstrix_data_dir,
    get_lmstudio_path,
    get_prompts_dir,
)

class TestPathUtilities:
    """Test path utility functions."""
    def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LM Studio path from home pointer file."""
    def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test fallback to common LM Studio locations."""
    def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test finding LM Studio in shared location."""
    def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test error when LM Studio is not found."""
    def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting LMStrix data directory."""
    def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting existing LMStrix data directory."""
    def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting default models file path."""
    def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context tests directory."""
    def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting context test log path."""
    def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting prompts directory."""
    def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
        """Test getting contexts directory."""
    def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
        """Test handling permission errors when creating directories."""

def test_get_lmstudio_path_from_pointer((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LM Studio path from home pointer file."""

def test_get_lmstudio_path_fallback_locations((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test fallback to common LM Studio locations."""

def test_get_lmstudio_path_shared_location((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test finding LM Studio in shared location."""

def exists_side_effect((self: Path)) -> bool:

def test_get_lmstudio_path_not_found((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test error when LM Studio is not found."""

def test_get_lmstrix_data_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting LMStrix data directory."""

def test_get_lmstrix_data_dir_exists((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting existing LMStrix data directory."""

def test_get_default_models_file((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting default models file path."""

def test_get_context_tests_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context tests directory."""

def test_get_context_test_log_path((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting context test log path."""

def test_get_prompts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting prompts directory."""

def test_get_contexts_dir((self: "TestPathUtilities", tmp_path: Path)) -> None:
    """Test getting contexts directory."""

def test_directory_creation_permissions_error((
        self: "TestPathUtilities",
        tmp_path: Path,
    )) -> None:
    """Test handling permission errors when creating directories."""


</documents>