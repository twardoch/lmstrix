# WORK.md - Issue #103 Implementation

## Current Tasks

### Implementing Enhanced Infer Context Control

1. **Add CLI parameters** - Add --in_ctx and --out_ctx to infer command
2. **Update InferenceEngine** - Modify to support new context control
3. **Implement smart loading** - Check if model already loaded
4. **Handle edge cases** - in_ctx=0, backward compatibility
5. **Test implementation** - Verify all scenarios work correctly

## Progress

- [x] Modify cli/main.py to add new parameters
- [x] Update InferenceEngine.infer() method
- [x] Add model loading state detection
- [x] Implement conditional loading logic
- [x] Test and validate changes

## Implementation Complete! âœ…

### Changes Made:

1. **CLI Enhancement (cli/main.py)**:
   - Added `--in_ctx` parameter for controlling model loading context
   - Added `--out_ctx` parameter to replace deprecated `--max_tokens`
   - Maintained backward compatibility with deprecation warning
   - Parameter validation and precedence handling

2. **InferenceEngine Updates (core/inference.py)**:
   - Added `in_ctx` parameter to `infer()` method
   - Implemented conditional loading logic:
     - When `in_ctx` specified: Always unload existing and reload
     - When `in_ctx=0`: Load without context specification
     - When `in_ctx` not specified: Load with optimal context
   - Smart unloading: Only unload if explicitly loaded with `in_ctx`

3. **Key Features Implemented**:
   - âœ… `--in_ctx INT`: Controls model loading context
   - âœ… `--in_ctx 0`: Loads model without specified context
   - âœ… No `--in_ctx`: Uses optimal context (tested or declared)
   - âœ… `--out_ctx INT`: Replaces `--max_tokens` for generation
   - âœ… Backward compatibility with deprecation warnings

### Usage Examples:

```bash
# Load model with specific context
lmstrix infer "Hello" model-id --in_ctx 8192 --out_ctx 100

# Load model with default context (no specification)
lmstrix infer "Hello" model-id --in_ctx 0 --out_ctx 100

# Reuse existing loaded model
lmstrix infer "Hello" model-id --out_ctx 100

# Backward compatibility (deprecated)
lmstrix infer "Hello" model-id --max_tokens 100
```

## Previous Work Progress

### ğŸ‰ **LATEST IMPROVEMENT - GRACEFUL MODEL LOAD ERROR HANDLING** ğŸ‰

#### **CRITICAL ISSUE RESOLVED** âœ…

**Issue**: User encountered `LMStudioServerError: Model get/load error: Model not found: lucy-128k` during model testing, causing the entire `lmstrix test --all` process to crash.

**Root Cause**: Models that exist in the LMStrix registry but are no longer available in LM Studio (deleted, unloaded, or moved) would cause a crash during the model loading phase, preventing the testing process from continuing.

#### **COMPREHENSIVE SOLUTION IMPLEMENTED** âœ…

##### **1. Enhanced Client Model Load Error Handling**
- âœ… Added `LMStudioServerError` handling to `load_model` method in `src/lmstrix/api/client.py`
- âœ… Specific detection for "model not found" and "path not found" errors
- âœ… Converts `LMStudioServerError` to descriptive `ModelLoadError` with context

##### **2. Intelligent Context Tester Model Load Recovery**
- âœ… Enhanced ModelLoadError handler in `src/lmstrix/core/context_tester.py`
- âœ… **Automatic Bad Model Detection**: Identifies models not found in LM Studio
- âœ… **Registry Status Updates**: Marks bad models as FAILED in the database
- âœ… **Detailed Logging**: Logs with "MODEL_NOT_FOUND" category for tracking
- âœ… **Graceful Continuation**: Testing continues with remaining models

##### **3. Robust Model Status Management**
- âœ… **Automatic Model Marking**: Models not found are marked as FAILED status
- âœ… **Database Updates**: Registry automatically updated with failure information
- âœ… **Clear Error Classification**: Distinguishes between load errors and not-found errors
- âœ… **Comprehensive Error Patterns**: Handles multiple error message variations

### **Error Handling Flow** ğŸ”„

```mermaid
graph TD
    A[Start Model Test] --> B[Load Model]
    B --> C{Model Load Success?}
    C -->|No| D{LMStudioServerError?}
    D -->|Yes| E{Model Not Found?}
    E -->|Yes| F[Mark Model as BAD]
    F --> G[Update Registry Status]
    G --> H[Log MODEL_NOT_FOUND]
    H --> I[Continue to Next Model]
    E -->|No| J[Log LOAD_FAILED]
    J --> I
    D -->|No| K[Handle Other Errors]
    K --> I
    C -->|Yes| L[Continue with Testing]
```

### **User Benefits** ğŸ¯

1. **âœ… Crash-Proof Testing**: `lmstrix test --all` no longer crashes on missing models
2. **âœ… Automatic Cleanup**: Bad models are automatically marked as failed
3. **âœ… Clear Error Messages**: Users understand when models are missing from LM Studio
4. **âœ… Continuous Operation**: Testing continues with remaining valid models
5. **âœ… Database Integrity**: Registry kept up-to-date with model availability
6. **âœ… Better Diagnostics**: Clear distinction between load errors and missing models

### **Example Error Messages** ğŸ“

**Before:**
```
Testing lucy-128k at 4,096 tokens...
lmstudio.LMStudioServerError: Model get/load error: Model not found: lucy-128k
Traceback (most recent call last):
  ... [ENTIRE PROCESS CRASHES]
```

**After:**
```
Testing lucy-128k at 4,096 tokens...
  â³ Waiting 3 seconds before next model (resource cleanup)...
  â†’ Testing context size: 4,096 tokens...
  âœ— Model failed to load at 4,096 tokens (load failed)
  ğŸš« Model 'lucy-128k' not found in LM Studio
  ğŸš« Model may have been deleted or unloaded
  âœ… Marked model lucy-128k as bad in registry (not found in LM Studio)
Testing next-model at 4,096 tokens...
  â³ Waiting 3 seconds before next model (resource cleanup)...
```

## ğŸ‰ **PREVIOUS IMPROVEMENT - GRACEFUL MEMORY/CACHE ERROR HANDLING** ğŸ‰

### **CRITICAL ISSUE RESOLVED** âœ…

**Issue**: User encountered `LMStudioServerError: Completion error: Unable to reuse from cache: llama_memory is null` during model testing, causing the entire `lmstrix test --all` process to crash.

**Follow-up Issue**: When trying to unload corrupted models, `LMStudioModelNotFoundError` was thrown because the model was already unloaded or not found.

**Root Cause**: The LM Studio server was throwing `LMStudioServerError` exceptions for models with corrupted memory/cache state, but our client was only catching `TypeError`, `AttributeError`, and `ValueError` exceptions. Additionally, our cleanup code didn't handle the case where the model was already unloaded.

### **COMPREHENSIVE SOLUTION IMPLEMENTED** âœ…

#### **1. Enhanced Client Exception Handling**
- âœ… Added `LMStudioServerError` import to `src/lmstrix/api/client.py`
- âœ… Added specific handling for memory/cache errors: "llama_memory is null", "unable to reuse from cache"
- âœ… Added handling for model not found errors from LM Studio server
- âœ… Converts `LMStudioServerError` to appropriate `InferenceError` or `ModelLoadError` with descriptive messages

#### **2. Intelligent Context Tester Error Recovery** 
- âœ… Added memory/cache error detection in `src/lmstrix/core/context_tester.py`
- âœ… **Automatic Model Cleanup**: Attempts to unload corrupted models to free resources
- âœ… **No Retry Logic**: Memory/cache errors are marked as permanent failures (don't retry)
- âœ… **Clear Logging**: Logs memory/cache errors with "MEMORY_CACHE_ERROR" category
- âœ… **User Guidance**: Warns users that model needs to be reloaded in LM Studio

#### **3. Robust Model Unload Error Handling** ğŸ†•
- âœ… Added `LMStudioModelNotFoundError` import to context tester
- âœ… Enhanced `finally` block to catch `LMStudioModelNotFoundError` during unload
- âœ… **Prevents Cleanup Crashes**: Model unload failures no longer crash the testing process
- âœ… **Graceful Degradation**: Testing continues even if model unload fails

#### **4. Graceful Degradation**
- âœ… **Continue Testing**: When a model has memory/cache errors, testing continues with next model
- âœ… **Resource Cleanup**: Attempts to unload problematic model to prevent resource leaks
- âœ… **Descriptive Error Messages**: Clear indication of what went wrong and what user should do
- âœ… **Robust Cleanup**: Handles cases where models are already unloaded or not found

### **Error Handling Flow** ğŸ”„

```mermaid
graph TD
    A[Model Test Starts] --> B[Load Model]
    B --> C[First Completion Call]
    C --> D{LMStudioServerError?}
    D -->|Yes| E{Memory/Cache Error?}
    E -->|Yes| F[Log Memory Error]
    F --> G[Try Model Unload]
    G --> H{Unload Success?}
    H -->|Yes| I[Continue to Next]
    H -->|No - ModelNotFound| J[Log Unload Failure]
    J --> I[Continue to Next]
    G --> K[Mark as Failed - Don't Retry]
    K --> I
    E -->|No| L[Check Other Error Types]
    L --> M[Handle Appropriately]
    D -->|No| N[Continue Normal Testing]
```

### **User Benefits** ğŸ¯

1. **âœ… Robust Testing**: `lmstrix test --all` no longer crashes on corrupted models
2. **âœ… Clear Error Messages**: Users understand when models need reloading in LM Studio
3. **âœ… Resource Management**: Automatic cleanup of corrupted model state
4. **âœ… Continuous Operation**: Testing continues with remaining models instead of aborting
5. **âœ… Better Logging**: Memory/cache errors are clearly categorized in logs
6. **âœ… Unload Safety**: Model unload failures don't crash the testing process

### **Example Error Messages** ğŸ“

**Before:**
```
lmstudio.LMStudioServerError: Completion error: Unable to reuse from cache: llama_memory is null
Traceback (most recent call last):
  ... [CRASH]

lmstudio.LMStudioModelNotFoundError: RPC error: No model found that fits the query
Traceback (most recent call last):
  ... [ANOTHER CRASH]
```

**After:**
```
[25/57] Testing dream-org_dream-v0-instruct-7b...
  â³ Waiting 3 seconds before next model (resource cleanup)...
  â†’ Testing context size: 2,048 tokens...
  âœ— Memory/cache error for dream-org_dream-v0-instruct-7b: Model memory/cache error - model may need to be reloaded
  ğŸ§  Model memory/cache corrupted, skipping to avoid crashes
  âš ï¸ Failed to unload model: RPC error: No model found that fits the query
[26/57] Testing next-model...
  â³ Waiting 3 seconds before next model (resource cleanup)...
```

## ğŸ‰ **PREVIOUS MISSION ACCOMPLISHED - DATABASE PROTECTION COMPLETE** ğŸ‰

### **CRITICAL BUG RESOLVED** âœ…

**Issue**: User reported "something wrecks the LMSTRIX.JSON database" during model testing, with embedding models causing errors and test results not being saved.

**Root Cause Found**: Two critical bugs were causing database corruption:

1. **ğŸš¨ Registry Update Bug**: All context testing methods were calling `registry.update_model(model.id, model)` instead of `registry.update_model_by_id(model)`, causing updates to fail silently.

2. **ğŸš¨ Datetime Parsing Bug**: Models with test data failed to load due to `datetime.fromisoformat()` receiving datetime objects instead of strings during deserialization.

### **COMPREHENSIVE SOLUTION IMPLEMENTED** âœ…

#### **1. Fixed All Registry Update Calls**
- âœ… Fixed 12+ incorrect calls in `src/lmstrix/core/context_tester.py`
- âœ… Fixed 4 incorrect calls in `src/lmstrix/cli/main.py`  
- âœ… Removed unused `save_model_registry` imports
- âœ… All registry updates now use `update_model_by_id()` method

#### **2. Fixed Datetime Serialization Bug**
- âœ… Added type checking in `_validate_registry_data()` method
- âœ… Only calls `datetime.fromisoformat()` on string objects
- âœ… Preserves existing datetime objects during validation

#### **3. Database Safety & Backup System**
- âœ… **Automatic Backup System**: Creates timestamped backups before every save
- âœ… **Keeps 10 most recent backups** with automatic cleanup
- âœ… **Data Validation**: Comprehensive integrity checks before saving  
- âœ… **Embedding Model Filtering**: Automatically skips embedding models
- âœ… **Range Validation**: Prevents unreasonable context values (>10M tokens)
- âœ… **Recovery System**: Can restore from corrupted databases
- âœ… **Health Check Command**: `lmstrix health` to verify database status

### **VERIFICATION COMPLETE** âœ…

**Test Results Now Work Perfectly:**
```
ğŸ‰ FINAL VERIFICATION - ultron-summarizer-8b test results:
  Model ID: ultron-summarizer-8b
  Status: ContextTestStatus.COMPLETED
  Tested Max Context: 60,000 tokens
  Last Known Good: 45,000 tokens
  Test Date: 2025-07-27 04:08:31.123012

âœ… ALL TEST RESULTS PROPERLY SAVED AND PERSISTENT!
âœ… DATABASE CORRUPTION ISSUE RESOLVED!
âœ… EMBEDDING MODEL FILTERING WORKING!
âœ… BACKUP SYSTEM OPERATIONAL!
```

**Before vs After:**
- **Before**: Test results showed `null` values, models disappeared, embedding models caused crashes
- **After**: Test results properly saved, models persist, embedding models filtered safely

### **User Benefits** ğŸ¯

1. **âœ… Reliable Test Results**: All context testing data is now saved and persistent
2. **âœ… Database Protection**: Automatic backups prevent data loss
3. **âœ… Embedding Model Safety**: No more crashes from embedding models  
4. **âœ… Data Integrity**: Comprehensive validation prevents corruption
5. **âœ… Recovery Options**: Health check and backup restoration available
6. **âœ… Performance**: Faster testing with proper registry updates

### **Commands Now Working Perfectly** ğŸ› ï¸

```bash
# Test models and save results (now handles memory/cache errors gracefully!)
lmstrix test --all

# Test specific model with error handling
lmstrix test ultron-summarizer-8b --ctx 45000

# Check database health and backups  
lmstrix health --verbose

# List models with test results
lmstrix list --sort ctx

# Scan models with embedding filtering
lmstrix scan --verbose
```

---

## **LATEST UPDATE - SMART SORTING IMPLEMENTATION** ğŸ‰

### **Feature Enhancement Completed** âœ…

**Request**: `lmstrix list --sort smart` should sort models the same way as `lmstrix test --all` does.

**Solution**: Implemented the "smart" sorting algorithm for the `list` command that matches the exact sorting used by `test --all`.

### **Implementation Details** âœ…

#### **Smart Sorting Algorithm**
- Formula: `size_bytes + (context_limit * 100,000)`
- Prioritizes smaller models first
- Within similar sizes, prioritizes lower context limits
- Available in both ascending (`--sort smart`) and descending (`--sort smartd`) order

#### **Code Changes**
- âœ… Added smart sorting case to `list` command in `src/lmstrix/cli/main.py`
- âœ… Updated help text to include smart sorting option
- âœ… Updated docstring documentation

### **Usage Examples** ğŸ“

```bash
# List models with smart sorting (ascending - smaller models first)
lmstrix list --sort smart

# List models with smart sorting (descending - larger models first)  
lmstrix list --sort smartd

# Test all models (uses smart sorting automatically)
lmstrix test --all
```

### **Benefits** ğŸ¯

1. **âœ… Consistent Sorting**: List and test commands now use the same sorting algorithm
2. **âœ… Optimal Testing Order**: Models are sorted for efficient testing (smaller models first)
3. **âœ… User Control**: Can choose ascending or descending order

---

## ğŸ‰ **LATEST UPDATE - FAST MODE TESTING** ğŸ‰

### **Feature Request Completed** âœ…

**Request**: `lms test --fast` (with --all or with a model id) should test only loading and whether the inference is possible, but should not perform the semantic verification of 96 or 5. In other words, if the inference has technically completed then we treat it as pass.

### **Implementation Details** âœ…

#### **1. CLI Enhancement**
- âœ… Added `--fast` flag to the `test` command in `src/lmstrix/cli/main.py`
- âœ… Updated help documentation to explain fast mode functionality
- âœ… Fast mode parameter properly propagated through all test functions

#### **2. Context Tester Fast Mode**
- âœ… Added `fast_mode` parameter to `ContextTester.__init__()` in `src/lmstrix/core/context_tester.py`
- âœ… Modified `_test_at_context()` to skip semantic verification when fast mode is enabled
- âœ… Fast mode uses simple "Say hello" prompt instead of dual arithmetic/number tests
- âœ… Any non-empty response is considered success in fast mode

#### **3. Fast Mode Logic**
```python
if self.fast_mode:
    # Fast mode: Just test if inference completes technically
    response = self.client.completion(
        llm=llm,
        prompt=self.test_prompt,  # Use simple prompt
        temperature=0.9,
        model_id=model_path,
    )
    # Any response means technical success in fast mode
    inference_success = bool(response.content.strip())
    combined_response = f"Fast mode - Response: '{response.content.strip()}'"
```

### **Usage Examples** ğŸ“

```bash
# Fast test a specific model (skip semantic checks)
lmstrix test my-model --fast

# Fast test all models (check only if inference works)
lmstrix test --all --fast

# Fast test at specific context size
lmstrix test my-model --ctx 8192 --fast

# Combine with verbose for detailed output
lmstrix test --all --fast --verbose
```

### **Benefits** ğŸ¯

1. **âœ… Faster Testing**: Skip semantic verification for quicker results
2. **âœ… Technical Validation**: Confirms model can load and generate responses
3. **âœ… Flexible Testing**: Choose between thorough (default) or fast testing
4. **âœ… Better for Initial Scans**: Quickly identify which models work at all

## ğŸ‰ **LATEST UPDATE - NEW CONTEXT TESTING STRATEGY** ğŸ‰

### **Feature Request Completed** âœ… (2025-07-29)

**Request**: Change 'lmstrix test' approach to use fixed context values instead of binary search.

### **Implementation Details** âœ…

#### **New Testing Strategy**
- âœ… **Fixed Context Values**: Tests at 30k, 40k, 60k, 80k, 100k, 120k (if < declared max)
- âœ… **Always Test Max-1**: Always tests declared max context - 1
- âœ… **10% Reduction Retry**: If failure at context > 4095 and < declared max, retries with 10% reduction
- âœ… **Stop on Failure**: Stops testing further contexts after first failure (with retry attempts)
- âœ… **Failure Declaration**: Declares failure if all tests fail

#### **Code Changes**
- âœ… Completely rewrote `test_model()` method in `src/lmstrix/core/context_tester.py`
- âœ… Removed binary search logic in favor of fixed context testing
- âœ… Added retry logic with 10% reduction for failed contexts
- âœ… Updated logging and progress messages for new approach

### **Testing Flow** ğŸ”„

```mermaid
graph TD
    A[Start Testing] --> B[Test Fixed Contexts]
    B --> C[30k, 40k, 60k, 80k, 100k, 120k]
    C --> D[Filter < Declared Max]
    D --> E[Add Declared Max - 1]
    E --> F[Test Each Context]
    F --> G{Success?}
    G -->|Yes| H{Max-1?}
    H -->|Yes| I[Complete Success]
    H -->|No| F
    G -->|No| J{Context > 4095?}
    J -->|Yes| K[Reduce by 10%]
    K --> L{Still > 4095?}
    L -->|Yes| M[Retry Test]
    L -->|No| N[Stop Testing]
    M --> O{Success?}
    O -->|Yes| P[Record Success]
    O -->|No| K
    J -->|No| N
```

### **Benefits** ğŸ¯

1. **âœ… Predictable Testing**: Fixed context values make testing more consistent
2. **âœ… Faster Results**: No binary search overhead for finding optimal context
3. **âœ… Better Coverage**: Tests common context sizes that users actually use
4. **âœ… Smart Retry**: 10% reduction helps find working context near failure point
5. **âœ… Clear Success Criteria**: Success at declared max - 1 means full support

## **NEXT TASKS** ğŸ“‹

- [ ] Update test suite to match new testing approach
- [ ] Monitor user feedback on the new testing strategy
- [ ] Consider adding retry logic for temporary LM Studio server issues (non-memory errors)
- [ ] Document the new testing strategy in README.md
- [x] Implement new fixed context testing strategy
- [x] Implement smart sorting for `lmstrix list` command
- [x] Implement --fast mode for test command

**Status**: âœ… **COMPLETE - New context testing strategy, database protection, memory/cache error handling, smart sorting, and fast mode testing fully operational**

---

## ğŸ‰ **LATEST UPDATE - PROMPT FILE SUPPORT WITH TOML** ğŸ‰

### **Issue #104 Implementation** âœ… (2025-07-29)

**Feature**: Added support for loading prompts from TOML files with parameter resolution.

### **Implementation Details** âœ…

#### **1. New CLI Parameters**
- âœ… Added `--file_prompt PATH` to specify TOML file containing prompts
- âœ… Added `--dict PARAMS` for key=value parameter pairs
- âœ… Modified `--prompt` behavior: when used with `--file_prompt`, refers to prompt name in TOML

#### **2. Parameter Resolution**
- âœ… Supports comma-separated format: `--dict "name=Alice,topic=AI"`
- âœ… Integrates with existing PromptResolver for placeholder resolution
- âœ… Handles nested placeholders and internal references
- âœ… Reports unresolved placeholders with warnings

#### **3. File Loading**
- âœ… Path expansion for ~ and relative paths
- âœ… Validates TOML file existence
- âœ… Clear error messages for missing files or prompts
- âœ… Verbose mode shows resolution details

#### **4. Example TOML Structure**
```toml
[greetings]
formal = "Good day, {{name}}. How may I assist you with {{topic}}?"
casual = "Hey {{name}}! What's up with {{topic}}?"

[templates]
base = "You are an expert in {{domain}}."
instruction = "{{templates.base}} Please explain {{concept}} in {{style}} terms."
```

### **Usage Examples** ğŸ“

```bash
# Simple greeting with parameters
lmstrix infer greetings.casual model-id \
  --file_prompt prompts.toml \
  --dict "name=Alice,topic=AI"

# Nested template resolution
lmstrix infer templates.instruction model-id \
  --file_prompt prompts.toml \
  --dict "domain=Python,concept=decorators,style=simple"

# Multiple parameters
lmstrix infer code.review model-id \
  --file_prompt examples/prompts.toml \
  --dict "language=Python,code=def hello(): pass"
```

### **Benefits** ğŸ¯

1. **âœ… Reusable Prompts**: Store prompts in version-controlled files
2. **âœ… Dynamic Templates**: Use placeholders for flexible prompts
3. **âœ… Better Organization**: Separate prompt logic from code
4. **âœ… Team Collaboration**: Share prompt libraries
5. **âœ… Complex Prompts**: Support nested templates and references

---

## ğŸ‰ **PREVIOUS UPDATE - ENHANCED INFERENCE IMPROVEMENTS** ğŸ‰

### **Features Implemented** âœ… (2025-07-29)

**Enhancement**: Improved `lmstrix infer` command with smart model loading and better user feedback.

### **Implementation Details** âœ…

#### **1. Smart Model State Detection**
- âœ… Added `get_loaded_models()` and `is_model_loaded()` methods to LMStudioClient
- âœ… Checks if model is already loaded before reloading
- âœ… Reuses existing loaded model when no explicit context specified
- âœ… Shows clear status messages about model reuse vs reload

#### **2. Force Reload Option**
- âœ… Added `--force-reload` flag to force model reload
- âœ… Automatically uses optimal context when force reloading
- âœ… Useful for refreshing model state or changing context

#### **3. Context Validation**
- âœ… Validates requested context against model's declared limit
- âœ… Warns when context exceeds tested maximum
- âœ… Suggests using tested maximum for best results
- âœ… Prevents common errors before attempting to load

#### **4. Enhanced Status Messages**
- âœ… Shows when model is being reused vs reloaded
- âœ… Displays current loaded context information
- âœ… Clear warnings for context size issues
- âœ… Better error messages with actionable hints

### **Usage Examples** ğŸ“

```bash
# First run - loads model
lmstrix infer "Hello" model-id --out_ctx 100
# Output: Loading model with optimal context 32,768...

# Second run - reuses loaded model
lmstrix infer "Hello again" model-id --out_ctx 100
# Output: Model already loaded with context 32,768, reusing...

# Force reload with different context
lmstrix infer "Hello" model-id --in_ctx 16384 --out_ctx 100
# Output: Model currently loaded with context 32,768, will reload with new context

# Force reload without specific context
lmstrix infer "Hello" model-id --force-reload --out_ctx 100
# Output: Force reload requested, loading with context 32,768
```

### **Benefits** ğŸ¯

1. **âœ… Better Performance**: Avoids unnecessary model reloads
2. **âœ… Clear Feedback**: Users know exactly what's happening
3. **âœ… Safer Loading**: Context validation prevents common errors
4. **âœ… More Control**: Force reload when needed
5. **âœ… Smarter Defaults**: Optimal behavior without flags